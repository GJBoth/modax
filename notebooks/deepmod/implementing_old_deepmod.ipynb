{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "from functools import partial\n",
    "from jax import lax\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from flax import linen as nn\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modax.data.burgers import burgers\n",
    "from modax.feature_generators import library_backward\n",
    "from modax.networks import MLP\n",
    "from flax import optim\n",
    "from modax.losses import neg_LL, mse\n",
    "from modax.logging import Logger\n",
    "from typing import Sequence\n",
    "\n",
    "from jax import jit, value_and_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_update(loss_fn, *args, **kwargs):\n",
    "    def step(opt, state, loss_fn, *args, **kwargs):\n",
    "        grad_fn = value_and_grad(loss_fn, argnums=0, has_aux=True)\n",
    "        (loss, (updated_state, metrics)), grad = grad_fn(\n",
    "            opt.target, state, *args, **kwargs\n",
    "        )\n",
    "        opt = opt.apply_gradient(grad)  # Return the updated optimizer with parameters.\n",
    "\n",
    "        return (opt, updated_state), metrics\n",
    "\n",
    "    return jit(lambda opt, state: step(opt, state, loss_fn, *args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_pinn(params, state, model, x, y):\n",
    "    variables = {'params': params, **state}\n",
    "    (prediction, dt, theta, coeffs), updated_state = model.apply(variables, x, mutable=list(state.keys()))\n",
    "\n",
    "    MSE = mse(prediction, y)\n",
    "    Reg = mse(dt.squeeze(), (theta @ coeffs).squeeze())\n",
    "    loss = MSE + Reg\n",
    "    metrics = {\"loss\": loss, \"mse\": MSE, \"reg\": Reg, \"coeff\": coeffs}\n",
    "\n",
    "    return loss, (updated_state, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing masked least squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jnp.load('../test_data.npy', allow_pickle=True).item()\n",
    "y, X = data['y'], data['X']\n",
    "\n",
    "X_normed = X / jnp.linalg.norm(X, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = jnp.zeros((X.shape[1]), dtype=bool)\n",
    "mask = jax.ops.index_update(mask, jnp.array([2, 5]), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_masked = X * (~mask * 1e-6 + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 8.7450782e-04],\n",
       "             [-4.1661084e-02],\n",
       "             [ 9.8933630e-02],\n",
       "             [ 1.6169542e-03],\n",
       "             [-1.4724124e-02],\n",
       "             [-7.6219821e-01],\n",
       "             [ 9.2846295e-03],\n",
       "             [-2.8839568e-03],\n",
       "             [ 2.4262253e-02],\n",
       "             [-2.6267979e-01],\n",
       "             [-8.8574179e-03],\n",
       "             [-2.2183033e-04]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.lstsq(X, y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-2.6406812e-09],\n",
       "             [-1.5912620e-06],\n",
       "             [ 9.5455840e-02],\n",
       "             [ 3.5970850e-05],\n",
       "             [-1.3432731e-07],\n",
       "             [-9.9294835e-01],\n",
       "             [-6.8076105e-07],\n",
       "             [ 2.7888411e-05],\n",
       "             [-1.2501999e-07],\n",
       "             [-7.0616380e-07],\n",
       "             [-7.7968679e-07],\n",
       "             [ 2.3472699e-05]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.lstsq(X_masked, y)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so the idea works, great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastSquares(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        y, X = inputs\n",
    "        mask = self.variable(\n",
    "            \"vars\",\n",
    "            \"mask\",\n",
    "            lambda n_terms: jnp.ones((n_terms, ), dtype=bool), X.shape[1])\n",
    "        \n",
    "        X_masked = X * (~mask.value * 1e-6 + mask.value)\n",
    "        coeffs =  jnp.linalg.lstsq(X_masked, y)[0]\n",
    "\n",
    "        return coeffs * mask.value[:, None] # extra multiplication to compensate numerical errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "key_data, key_network = random.split(key)\n",
    "\n",
    "# Making dataset\n",
    "x = jnp.linspace(-3, 4, 100)\n",
    "t = jnp.linspace(0.5, 5.0, 20)\n",
    "\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "u = burgers(x_grid, t_grid, 0.1, 1.0)\n",
    "\n",
    "X_train = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y_train = u.reshape(-1, 1)\n",
    "y_train += 0.01 * jnp.std(y_train) * jax.random.normal(key_data, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepmod(nn.Module):\n",
    "    features: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        prediction, dt, theta = library_backward(MLP(self.features), inputs)\n",
    "        coeffs = LeastSquares()((dt, theta))\n",
    "        return prediction, dt, theta, coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deepmod([50, 50, 1])\n",
    "variables = model.init(key_network, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop('params')\n",
    "optimizer = optimizer.create(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    vars: {\n",
      "        LeastSquares_0: {\n",
      "            mask: DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                          True,  True,  True,  True], dtype=bool),\n",
      "        },\n",
      "    },\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling train step\n",
    "update = create_update(loss_fn_pinn, model=model, x=X_train, y=y_train)\n",
    "_ = update(optimizer, state)  # triggering compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import numpy as np\n",
    "from flax.core import freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mask(X, y, reg, threshold=0.1):\n",
    "    X_normed = X / jnp.linalg.norm(X, axis=0, keepdims=True)\n",
    "    y_normed = y / jnp.linalg.norm(y, axis=0, keepdims=True)\n",
    "    coeffs = reg.fit(np.array(X_normed), np.array(y_normed).squeeze()).coef_\n",
    "    mask = np.abs(coeffs) > threshold \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: 0.19655461609363556\n",
      "Loss step 1000: 0.0001199067773995921\n",
      "Loss step 2000: 3.684171133500058e-06\n",
      "Loss step 3000: 2.1924374777881894e-06\n",
      "Loss step 4000: 1.9715557755262125e-06\n",
      "Loss step 5000: 1.9775927739829058e-06\n",
      "Loss step 6000: 1.885058736661449e-06\n",
      "Loss step 7000: 1.8930719534182572e-06\n",
      "Loss step 8000: 1.9562123725336278e-06\n",
      "Loss step 9000: 1.916803057611105e-06\n"
     ]
    }
   ],
   "source": [
    "# Running to convergence\n",
    "max_epochs = 10000\n",
    "logger = Logger(comment='baseline')\n",
    "for epoch in jnp.arange(max_epochs):\n",
    "    (optimizer, state), metrics = update(optimizer, state)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Loss step {epoch}: {metrics['loss']}\")\n",
    "    if epoch % 25 == 0:\n",
    "        logger.write(metrics, epoch)\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt, theta, coeffs = model.apply({\"params\": optimizer.target, **state}, X_train, mutable=list(state.keys()))[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-1.6329670e-04],\n",
       "             [-2.3596138e-03],\n",
       "             [ 1.0054986e-01],\n",
       "             [-1.7797307e-04],\n",
       "             [ 4.3829046e-03],\n",
       "             [-9.8531437e-01],\n",
       "             [-7.2126538e-03],\n",
       "             [ 8.3148736e-04],\n",
       "             [-1.5166596e-02],\n",
       "             [-3.0938089e-03],\n",
       "             [ 3.8494654e-03],\n",
       "             [-2.9145367e-04]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.linalg.lstsq(theta, dt)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False, False,  True, False, False, False,\n",
       "       False, False, False])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LassoCV(fit_intercept=False)\n",
    "update_mask(theta, dt, reg, threshold=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So perfect this works. Now to add in the update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deepmod([50, 50, 1])\n",
    "variables = model.init(key_network, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop('params')\n",
    "optimizer = optimizer.create(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: 0.19655461609363556\n",
      "Loss step 1000: 0.0001199067773995921\n",
      "Loss step 2000: 3.684171133500058e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 3000: 2.292486215083045e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 4000: 2.0859679352724925e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 5000: 2.0271486391720828e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 6000: 1.8840654547602753e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 7000: 1.8166015252063517e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 8000: 1.8975762259287876e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 9000: 1.894703132165887e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# Running to convergence\n",
    "max_epochs = 10000\n",
    "reg = LassoCV(fit_intercept=False)\n",
    "logger = Logger(comment='updating_mask')\n",
    "for epoch in jnp.arange(max_epochs):\n",
    "    (optimizer, state), metrics = update(optimizer, state)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Loss step {epoch}: {metrics['loss']}\")\n",
    "    if (epoch % 100 == 0) and (epoch > 2000):\n",
    "        dt, theta = model.apply({\"params\": optimizer.target, **state}, X_train, mutable=list(state.keys()))[0][1:3]\n",
    "        mask = update_mask(theta, dt, reg)\n",
    "        print(mask)\n",
    "        state = freeze({'vars': {'LeastSquares_0': {'mask': mask}}})\n",
    "    if epoch % 25 == 0:\n",
    "        logger.write(metrics, epoch)\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class mask_scheduler:\n",
    "    patience: int = 500\n",
    "    delta: float = 1e-5\n",
    "    periodicity: int = 200\n",
    "        \n",
    "    periodic: bool = False\n",
    "    best_loss = None\n",
    "    best_iteration = None\n",
    "    \n",
    "    def __call__(self, loss, iteration, optimizer):\n",
    "        if self.periodic is True:\n",
    "            if (iteration - self.best_iteration) % self.periodicity == 0:\n",
    "                update_mask, optimizer = True, optimizer\n",
    "            else:\n",
    "                 update_mask, optimizer = False, optimizer\n",
    "\n",
    "        elif self.best_loss is None:\n",
    "            self.best_loss = loss\n",
    "            self.best_iteration = iteration\n",
    "            self.best_optim_state = optimizer\n",
    "            update_mask, optimizer = False, optimizer\n",
    "\n",
    "        # If it didnt improve, check if we're past patience\n",
    "        elif (self.best_loss - loss) < self.delta:\n",
    "            if (iteration - self.best_iteration) >= self.patience:\n",
    "                self.periodic = True  # switch to periodic regime\n",
    "                self.best_iteration = iteration  # because the iterator doesnt reset\n",
    "                update_mask, optimizer = True, self.best_optim_state\n",
    "            else:\n",
    "                update_mask, optimizer = False, optimizer\n",
    "\n",
    "        # If not, keep going\n",
    "        else:\n",
    "            self.best_loss = loss\n",
    "            self.best_iteration = iteration\n",
    "            self.best_optim_state = optimizer\n",
    "            update_mask, optimizer = False, optimizer\n",
    "\n",
    "        return update_mask, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "key_data, key_network = random.split(key)\n",
    "\n",
    "# Making dataset\n",
    "x = jnp.linspace(-3, 4, 100)\n",
    "t = jnp.linspace(0.5, 5.0, 20)\n",
    "\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "u = burgers(x_grid, t_grid, 0.1, 1.0)\n",
    "\n",
    "X = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y = u.reshape(-1, 1)\n",
    "y += 0.10 * jnp.std(y_train) * jax.random.normal(key_data, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = random.permutation(key, X.shape[0])\n",
    "X = X[rand_idx, :]\n",
    "y = y[rand_idx, :]\n",
    "\n",
    "split = int(0.8 * X.shape[0])\n",
    "X_train, X_test = X[:split, :], X[split:, :]\n",
    "y_train, y_test = y[:split, :], y[split:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deepmod([50, 50, 1])\n",
    "variables = model.init(key_network, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop('params')\n",
    "optimizer = optimizer.create(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_metric = jit(lambda opt, state: loss_fn_pinn(opt.target, state, model, X_test, y_test)[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: 0.19655461609363556\n",
      "Loss step 1000: 0.0001199067773995921\n",
      "[False False  True False False  True  True False  True False False False]\n",
      "Loss step 2000: 2.886177389882505e-05\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 3000: 2.844449682015693e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 4000: 2.1348250811570324e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 5000: 2.049187287411769e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 6000: 2.0477320958889322e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 7000: 1.9848930605803616e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 8000: 1.8894813820224954e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "Loss step 9000: 1.9287519990029978e-06\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n",
      "[False False  True False False  True False False False False False False]\n"
     ]
    }
   ],
   "source": [
    "# Running to convergence\n",
    "max_epochs = 10000\n",
    "reg = LassoCV(fit_intercept=False, )\n",
    "logger = Logger(comment='validation')\n",
    "scheduler = mask_scheduler()\n",
    "\n",
    "for epoch in jnp.arange(max_epochs):\n",
    "    (optimizer, state), train_metrics = update(optimizer, state)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Loss step {epoch}: {train_metrics['loss']}\")\n",
    "        \n",
    "    if epoch % 25 == 0:\n",
    "        logger.write(train_metrics, epoch)\n",
    "        \n",
    "        val_metrics = validation_metric(optimizer, state)\n",
    "        apply_sparsity, optimizer = scheduler(val_metrics['mse'], epoch, optimizer)\n",
    "\n",
    "        if apply_sparsity:\n",
    "            dt, theta = model.apply({\"params\": optimizer.target, **state}, X_train, mutable=list(state.keys()))[0][1:3]\n",
    "            mask = update_mask(theta, dt, reg)\n",
    "            state = freeze({'vars': {'LeastSquares_0': {'mask': mask}}})\n",
    "            print(mask)\n",
    "        \n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import numpy as np\n",
    "from flax.core import freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mask(X, y, reg, threshold=0.1):\n",
    "    X_normed = X / jnp.linalg.norm(X, axis=0, keepdims=True)\n",
    "    y_normed = y / jnp.linalg.norm(y, axis=0, keepdims=True)\n",
    "    coeffs = reg.fit(np.array(X_normed), np.array(y_normed).squeeze()).coef_\n",
    "    mask = np.abs(coeffs) > threshold \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deepmod([50, 50, 1])\n",
    "variables = model.init(key_network, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop('params')\n",
    "optimizer = optimizer.create(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_metric = jit(lambda opt, state: loss_fn_pinn(opt.target, state, model, X_test, y_test)[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling train step\n",
    "update = create_update(loss_fn_pinn, model=model, x=X_train, y=y_train)\n",
    "_ = update(optimizer, state)  # triggering compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: 0.1946893334388733\n",
      "Loss step 1000: 0.04699103534221649\n",
      "Loss step 2000: 0.04699103534221649\n",
      "Loss step 3000: 0.04699103534221649\n",
      "Loss step 4000: 0.04699103534221649\n",
      "Loss step 5000: 0.04699103534221649\n",
      "Loss step 6000: 0.04699103534221649\n",
      "Loss step 7000: 0.04699103534221649\n",
      "Loss step 8000: 0.04699103534221649\n",
      "Loss step 9000: 0.04699103534221649\n"
     ]
    }
   ],
   "source": [
    "# Running to convergence\n",
    "max_epochs = 10000\n",
    "reg = LassoCV(fit_intercept=False, )\n",
    "logger = Logger(comment='validation')\n",
    "scheduler = jax.jit(lambda metrics, epoch, opt: mask_scheduler()(metrics['mse'], epoch, optimizer))\n",
    "\n",
    "for epoch in jnp.arange(max_epochs):\n",
    "    (optimizer, state), train_metrics = update(optimizer, state)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Loss step {epoch}: {train_metrics['loss']}\")\n",
    "        \n",
    "    if epoch % 25 == 0:\n",
    "        logger.write(train_metrics, epoch)\n",
    "        \n",
    "        val_metrics = validation_metric(optimizer, state)\n",
    "        apply_sparsity, optimizer = scheduler(val_metrics, epoch, optimizer)\n",
    "\n",
    "        if apply_sparsity:\n",
    "            dt, theta = model.apply({\"params\": optimizer.target, **state}, X_train, mutable=list(state.keys()))[0][1:3]\n",
    "            mask = update_mask(theta, dt, reg)\n",
    "            state = freeze({'mask': {'LeastSquares_0': {'active terms': mask}}})\n",
    "            print(mask)\n",
    "        \n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7f5660ea3e52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mask' is not defined"
     ]
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    vars: {\n",
       "        LeastSquares_0: {\n",
       "            mask: DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                          True,  True,  True,  True], dtype=bool),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
