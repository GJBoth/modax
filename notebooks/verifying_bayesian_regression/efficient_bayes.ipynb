{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make some efficiency changes to our bayes, mostly precalculating some arrays. we do that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "from functools import partial\n",
    "from jax import lax\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from flax import linen as nn\n",
    "from typing import Tuple\n",
    "from code import fwd_solver, bayes_ridge_update, evidence, BayesianRegression, fixed_point_solver\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from jax.scipy.stats import gamma\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jnp.load('test_data.npy', allow_pickle=True).item()\n",
    "y, X = data['y'], data['X']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first get our baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = X.shape[0]\n",
    "hyper_prior =  jnp.stack([n_samples / 2, 1 / (n_samples / 2 * 1e-4)],  axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_init = jnp.stack([1., 1. / jnp.var(y)], axis=0)\n",
    "update = jax.jit(lambda prior: bayes_ridge_update(prior_params=prior, y=y, X=X, hyper_prior_params=hyper_prior))\n",
    "_ = update(prior_init) # compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.67 ms ± 275 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "z_star = fwd_solver(update, prior_init, tol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "model = BayesianRegression(hyper_prior, tol=1e-4)\n",
    "inputs = (y, X)\n",
    "variables = model.init(key, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = jax.jit(lambda variables, inputs: model.apply(variables, inputs, mutable=['bayes']))\n",
    "_ = update(variables, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.34 ms ± 94.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "y, updated_state = update(variables, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a more efficient update by precalculating the gram, normalization and eigvals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_ridge_update_efficient(prior_params, y, X, gram, eigvals, hyper_prior_params):\n",
    "    # Unpacking parameters\n",
    "    alpha_prev, beta_prev = prior_params\n",
    "    a, b = hyper_prior_params\n",
    "  \n",
    "    # Calculating intermediate matrices\n",
    "    n_samples, n_terms = X.shape\n",
    "    gamma_ = jnp.sum((beta_prev * eigvals) / (alpha_prev + beta_prev * eigvals))\n",
    "    S = jnp.linalg.inv(beta_prev * gram + alpha_prev * jnp.eye(n_terms))\n",
    "    mn = beta_prev * S @ X.T @ y\n",
    "\n",
    "    # Update estimate\n",
    "    alpha = gamma_ / jnp.sum(mn ** 2)\n",
    "    beta = (n_samples - gamma_ + 2 * (a - 1)) / (\n",
    "        jnp.sum((y - X @ mn) ** 2) + 2 * b\n",
    "    )\n",
    "\n",
    "    return jnp.stack([alpha, beta], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_init = jnp.stack([1., 1. / jnp.var(y)], axis=0)\n",
    "X_normed = X / jnp.linalg.norm(X, axis=0, keepdims=True)\n",
    "gram = X_normed.T @ X_normed\n",
    "eigvals = jnp.linalg.eigvalsh(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = jax.jit(lambda prior: bayes_ridge_update_efficient(prior_params=prior, y=y, X=X_normed, gram=gram, eigvals=eigvals, hyper_prior_params=hyper_prior))\n",
    "_ = update(prior_init) # compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = jax.jit(fwd_solver, static_argnums=(0, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.07 ms ± 232 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "z_star = layer(update, prior_init, tol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we cut it down with a factor of two :-). Now let's put it nicely in a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianRegressionEfficient(nn.Module):\n",
    "    hyper_prior: Tuple\n",
    "    tol: float = 1e-3\n",
    "\n",
    "    def setup(self):\n",
    "        self.update = lambda prior, y, X, gram, eigvals: bayes_ridge_update_efficient(\n",
    "            prior, y, X, gram, eigvals, self.hyper_prior\n",
    "        )\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        is_initialized = self.has_variable(\"bayes\", \"z\")\n",
    "        z_init = self.variable(\n",
    "            \"bayes\", \"z\", lambda y: jnp.stack([1, 1 / jnp.var(y)], axis=0), inputs[0]\n",
    "        )\n",
    "        \n",
    "        y, X = inputs\n",
    "        X_normed = X / jnp.linalg.norm(X, axis=0, keepdims=True)\n",
    "        gram = X_normed.T @ X_normed\n",
    "        eigvals = jnp.linalg.eigvalsh(gram)\n",
    "        \n",
    "        z_star = fixed_point_solver(self.update, (y, X_normed, gram, eigvals), z_init.value, tol=self.tol)\n",
    "        if is_initialized:\n",
    "            z_init.value = z_star\n",
    "        return z_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "model = BayesianRegressionEfficient(hyper_prior, tol=1e-4)\n",
    "inputs = (y, X)\n",
    "variables = model.init(key, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = jax.jit(lambda variables, inputs: model.apply(variables, inputs, mutable=['bayes']))\n",
    "_ = update(variables, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.45 ms ± 6.83 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "y, updated_state = update(variables, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
