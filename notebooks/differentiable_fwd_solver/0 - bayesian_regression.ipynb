{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "northern-rehabilitation",
   "metadata": {},
   "source": [
    "In this notebook we try to get a forward solver for the bayesian regression working and explicitly differentiate, and compare with the implicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "determined-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "from jax import numpy as jnp, random\n",
    "import jax\n",
    "\n",
    "from modax.data.kdv import doublesoliton\n",
    "from modax.models import Deepmod\n",
    "from modax.training.utils import create_update\n",
    "from flax import optim\n",
    "from modax.training import train_max_iter\n",
    "from modax.training.losses.utils import precision, normal_LL\n",
    "\n",
    "\n",
    "from forward_solver import fixed_point_solver_explicit, fixed_point_solver_implicit\n",
    "from bayesian_regression import bayesian_regression\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-relations",
   "metadata": {},
   "source": [
    "Lets first create some fake input the neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-folder",
   "metadata": {},
   "source": [
    "# test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lovely-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "x = jnp.linspace(-10, 10, 100)\n",
    "t = jnp.linspace(0.1, 1.0, 10)\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "u = doublesoliton(x_grid, t_grid, c=[5.0, 2.0], x0=[0.0, -5.0])\n",
    "\n",
    "X = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y = u.reshape(-1, 1)\n",
    "y += 0.10 * jnp.std(y) * random.normal(key, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "simplified-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "prediction, dt, theta, coeffs = model.apply(variables, X)\n",
    "theta_normed = theta / jnp.linalg.norm(theta, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "processed-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = theta.shape\n",
    "prior_params_mse = (0.0, 0.0)\n",
    "tau = precision(y, prediction, *prior_params_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "norwegian-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_prior = (1e-6, 1e-6)\n",
    "beta_prior = (n_samples / 2, n_samples / (jax.lax.stop_gradient(tau)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-emperor",
   "metadata": {},
   "source": [
    "# Bayesian regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-juvenile",
   "metadata": {},
   "source": [
    "Let's first do a forward pass wihtout jit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "necessary-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jax.disable_jit():\n",
    "    loss, coeffs, prior, metrics= bayesian_regression(fixed_point_solver_explicit, \n",
    "                                                        theta_normed, \n",
    "                                                        dt, \n",
    "                                                        prior_init=None, \n",
    "                                                        hyper_prior=(alpha_prior, beta_prior), \n",
    "                                                        tol=1e-5, \n",
    "                                                        max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "olympic-mortgage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1154.081 [[ 4.1776318e-03]\n",
      " [-1.9126892e-06]\n",
      " [ 6.5167336e-04]\n",
      " [-1.0187295e-04]\n",
      " [ 5.8338847e-03]\n",
      " [ 9.4901613e-04]\n",
      " [-2.9233226e-03]\n",
      " [ 1.2994788e-04]\n",
      " [ 4.1545983e-03]\n",
      " [ 1.2195628e-03]\n",
      " [-2.3457576e-03]\n",
      " [-1.8430690e-03]] [529.0887    2.14874] (251, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(loss, coeffs, prior, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-democracy",
   "metadata": {},
   "source": [
    "So it works without jit - with jit it doesnt work (yet). Let's first check if we can calculate the derivative w.r.t the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "arbitrary-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dtheta = jax.grad(lambda X: bayesian_regression(fixed_point_solver_explicit, \n",
    "                                                            X, \n",
    "                                                            dt, \n",
    "                                                            prior_init=None, \n",
    "                                                            hyper_prior=(alpha_prior, beta_prior), \n",
    "                                                            tol=1e-5, \n",
    "                                                            max_iter=500)[0])\n",
    "\n",
    "dL_ddt = jax.grad(lambda y: bayesian_regression(fixed_point_solver_explicit, \n",
    "                                                            theta_normed, \n",
    "                                                            y, \n",
    "                                                            prior_init=None, \n",
    "                                                            hyper_prior=(alpha_prior, beta_prior), \n",
    "                                                            tol=1e-5, \n",
    "                                                            max_iter=500)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "upper-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jax.disable_jit():\n",
    "    grad_theta_exp = dL_dtheta(theta_normed)\n",
    "    grad_dt_exp = dL_ddt(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "incident-surrey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 12)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_theta_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "laughing-system",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_dt_exp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-inquiry",
   "metadata": {},
   "source": [
    "So its not fast but we're getting results. Now let's run a pass using the implicit diff method and compare the results - they should be the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "numerous-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jax.disable_jit():\n",
    "    loss, coeffs, prior, metrics= bayesian_regression(fixed_point_solver_implicit, \n",
    "                                                        theta_normed, \n",
    "                                                        dt, \n",
    "                                                        prior_init=None, \n",
    "                                                        hyper_prior=(alpha_prior, beta_prior), \n",
    "                                                        tol=1e-5, \n",
    "                                                        max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cognitive-shooting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1154.081 [[ 4.1776318e-03]\n",
      " [-1.9126892e-06]\n",
      " [ 6.5167336e-04]\n",
      " [-1.0187295e-04]\n",
      " [ 5.8338847e-03]\n",
      " [ 9.4901613e-04]\n",
      " [-2.9233226e-03]\n",
      " [ 1.2994788e-04]\n",
      " [ 4.1545983e-03]\n",
      " [ 1.2195628e-03]\n",
      " [-2.3457576e-03]\n",
      " [-1.8430690e-03]] [529.0887    2.14874] (251, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(loss, coeffs, prior, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-driving",
   "metadata": {},
   "source": [
    "Forward pass is the same as it should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "oriental-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dtheta_imp = jax.grad(lambda X: bayesian_regression(fixed_point_solver_implicit, \n",
    "                                                            X, \n",
    "                                                            dt, \n",
    "                                                            prior_init=None, \n",
    "                                                            hyper_prior=(alpha_prior, beta_prior), \n",
    "                                                            tol=1e-5, \n",
    "                                                            max_iter=500)[0])\n",
    "\n",
    "dL_ddt_imp = jax.grad(lambda y: bayesian_regression(fixed_point_solver_implicit, \n",
    "                                                            theta_normed, \n",
    "                                                            y, \n",
    "                                                            prior_init=None, \n",
    "                                                            hyper_prior=(alpha_prior, beta_prior), \n",
    "                                                            tol=1e-5, \n",
    "                                                            max_iter=500)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "protecting-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jax.disable_jit():\n",
    "    grad_theta_imp = dL_dtheta_imp(theta_normed)\n",
    "    grad_dt_imp = dL_ddt_imp(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-tutorial",
   "metadata": {},
   "source": [
    "That worked - now let's check if they're similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "returning-comparison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2.1466985e-07, dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.max(jnp.abs(grad_theta_exp - grad_theta_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "balanced-prompt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(5.364418e-07, dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.max(jnp.abs(grad_dt_exp - grad_dt_imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-brook",
   "metadata": {},
   "source": [
    "They are: that's good news. Let's check if we can run the implicit with jit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "chronic-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, coeffs, prior, metrics= bayesian_regression(fixed_point_solver_implicit, \n",
    "                                                        theta_normed, \n",
    "                                                        dt, \n",
    "                                                        prior_init=None, \n",
    "                                                        hyper_prior=(alpha_prior, beta_prior), \n",
    "                                                        tol=1e-5, \n",
    "                                                        max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "threaded-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dtheta_imp = jax.grad(lambda X: bayesian_regression(fixed_point_solver_implicit, \n",
    "                                                        X, \n",
    "                                                        dt, \n",
    "                                                        prior_init=None, \n",
    "                                                        hyper_prior=(alpha_prior, beta_prior), \n",
    "                                                        tol=1e-5, \n",
    "                                                        max_iter=500)[0])\n",
    "\n",
    "dL_ddt_imp = jax.grad(lambda y: bayesian_regression(fixed_point_solver_implicit, \n",
    "                                                        theta_normed, \n",
    "                                                        y, \n",
    "                                                        prior_init=None, \n",
    "                                                        hyper_prior=(alpha_prior, beta_prior), \n",
    "                                                        tol=1e-5, \n",
    "                                                        max_iter=500)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "smooth-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_theta_imp = dL_dtheta_imp(theta_normed)\n",
    "grad_dt_imp = dL_ddt_imp(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-compression",
   "metadata": {},
   "source": [
    "Let's see if its still the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dried-thunder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(5.364418e-07, dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.max(jnp.abs(grad_dt_exp - grad_dt_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "scheduled-broadcasting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(1.671724e-07, dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.max(jnp.abs(grad_theta_exp - grad_theta_imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-synthesis",
   "metadata": {},
   "source": [
    "great! so to recap, we now have an explicitly and implicitly differentiable forward solver which works on bayesian regression. The implicit differentiable solver works when jitted, the explicit not (yet). We now have two paths - we can make the explicit solver jittable, and start working on the SBL. SBL has more priority, so let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-democrat",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
