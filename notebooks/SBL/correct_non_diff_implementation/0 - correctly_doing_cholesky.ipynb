{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "imposed-villa",
   "metadata": {},
   "source": [
    "We know we don't need the gradient, (or at least for now), so let's make the code for the SBL a lot better by correctly doing 1) the cholesky decomp and 2) implementing the evidence convegrence criterium. In this notebook we focus on point 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "behavioral-helena",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "from jax import numpy as jnp, random\n",
    "import jax\n",
    "from modax.data.kdv import doublesoliton\n",
    "from modax.models import Deepmod\n",
    "from modax.training.utils import create_update\n",
    "from flax import optim\n",
    "\n",
    "from modax.training import train_max_iter\n",
    "from modax.training.losses.utils import precision, normal_LL\n",
    "from modax.utils.forward_solver import fixed_point_solver, fwd_solver, fwd_solver_simple\n",
    "\n",
    "from flax.core import unfreeze\n",
    "from flax.traverse_util import flatten_dict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-knitting",
   "metadata": {},
   "source": [
    "# Making data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intense-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "x = jnp.linspace(-10, 10, 100)\n",
    "t = jnp.linspace(0.1, 1.0, 10)\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "u = doublesoliton(x_grid, t_grid, c=[5.0, 2.0], x0=[0.0, -5.0])\n",
    "\n",
    "X = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y = u.reshape(-1, 1)\n",
    "y += 0.10 * jnp.std(y) * random.normal(key, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "naval-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "state, params = variables.pop(\"params\")\n",
    "\n",
    "prediction, dt, theta, coeffs = model.apply({\"params\": params, **state}, X)\n",
    "theta_normed = theta / jnp.linalg.norm(theta, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stone-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = theta.shape\n",
    "prior_params_mse = (0.0, 0.0)\n",
    "tau = precision(y, prediction, *prior_params_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coordinated-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_prior = (1e-6, 1e-6)\n",
    "beta_prior = (n_samples / 2, n_samples / (2 * jax.lax.stop_gradient(tau)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "focal-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = theta_normed.shape\n",
    "norm_weight = jnp.concatenate((jnp.ones((n_features,)), jnp.zeros((1,))), axis=0)\n",
    "prior_init = jnp.concatenate([jnp.ones((n_features,)), (1.0 / (jnp.var(dt) + 1e-7))[jnp.newaxis]], axis=0)\n",
    "gram = jnp.dot(theta_normed.T, theta_normed)\n",
    "XT_y = jnp.dot(theta_normed.T, dt)\n",
    "\n",
    "tol = 1e-3\n",
    "max_iter = 1000 # low to keep it manageable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-stewart",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unlimited-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sigma(gram, alpha, beta):\n",
    "    sigma_inv = jnp.diag(alpha) + beta * gram\n",
    "    L_inv = jnp.linalg.pinv(jnp.linalg.cholesky(sigma_inv))\n",
    "    sigma_ = jnp.dot(L_inv.T, L_inv)\n",
    "    return sigma_\n",
    "\n",
    "\n",
    "def update_coeff(XT_y, beta, sigma_):\n",
    "    coef_ = beta * jnp.linalg.multi_dot([sigma_, XT_y])\n",
    "    return coef_\n",
    "\n",
    "\n",
    "def update(prior, X, y, gram, XT_y, alpha_prior, beta_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    alpha, beta = prior[:-1], prior[-1]\n",
    "    sigma = update_sigma(gram, alpha, beta)\n",
    "    coeffs = update_coeff(XT_y, beta, sigma)\n",
    "\n",
    "    # Update alpha and lambda\n",
    "    rmse_ = jnp.sum((y - jnp.dot(X, coeffs)) ** 2)\n",
    "    gamma_ = 1.0 - alpha * jnp.diag(sigma)\n",
    "\n",
    "    # TODO: Cap alpha with some threshold.\n",
    "    alpha = (gamma_ + 2.0 * alpha_prior[0]) / (\n",
    "        (coeffs.squeeze() ** 2 + 2.0 * alpha_prior[1])\n",
    "    )\n",
    "    beta = (n_samples - gamma_.sum() + 2.0 * beta_prior[0]) / (\n",
    "        rmse_ + 2.0 * beta_prior[1]\n",
    "    )\n",
    "\n",
    "    return jnp.concatenate([alpha, beta[jnp.newaxis]], axis=0)\n",
    "\n",
    "def evidence(X, y, prior, gram, XT_y, alpha_prior, beta_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    alpha, beta = prior[:-1], prior[-1]\n",
    "\n",
    "    sigma = update_sigma(gram, alpha, beta)\n",
    "    coeffs = update_coeff(XT_y, beta, sigma)\n",
    "    rmse_ = jnp.sum((y - jnp.dot(X, coeffs)) ** 2)\n",
    "\n",
    "    score = jnp.sum(alpha_prior[0] * jnp.log(alpha) - alpha_prior[1] * alpha)\n",
    "    score += beta_prior[0] * jnp.log(beta) - beta_prior[1] * beta\n",
    "    score += 0.5 * (\n",
    "        jnp.linalg.slogdet(sigma)[1]\n",
    "        + n_samples * jnp.log(beta)\n",
    "        + jnp.sum(jnp.log(alpha))\n",
    "    )\n",
    "    score -= 0.5 * (beta * rmse_ + jnp.sum(alpha * coeffs.squeeze() ** 2))\n",
    "\n",
    "    return score.squeeze(), coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "blind-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once for jit\n",
    "prior_params_baseline, metrics = fixed_point_solver(\n",
    "    update,\n",
    "    (theta_normed, dt, gram, XT_y, alpha_prior, beta_prior),\n",
    "    prior_init,\n",
    "    norm_weight,\n",
    "    tol=tol,\n",
    "    max_iter=max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "martial-failure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 284 ms, sys: 3.32 ms, total: 288 ms\n",
      "Wall time: 286 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prior_params, metrics = fixed_point_solver(\n",
    "    update,\n",
    "    (theta_normed, dt, gram, XT_y, alpha_prior, beta_prior),\n",
    "    prior_init,\n",
    "    norm_weight,\n",
    "    tol=tol,\n",
    "    max_iter=max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "whole-checklist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([3.8261593e+01, 1.4216833e+03, 1.5917912e+00, 1.4647550e+03,\n",
       "             3.5256863e-01, 1.4051538e+03, 1.0434841e+03, 1.2460419e+03,\n",
       "             8.3008948e+02, 1.3492664e+03, 8.2153131e+02, 1.3355718e+03,\n",
       "             4.2957764e+00], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_params_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-minneapolis",
   "metadata": {},
   "source": [
    "# V1 - switching to cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "corrected-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.numpy.linalg import cholesky\n",
    "from jax.scipy.linalg import solve_triangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "seeing-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sigma(gram, alpha, beta):\n",
    "    sigma_inv = jnp.diag(alpha) + beta * gram\n",
    "    L = cholesky(sigma_inv)\n",
    "    R = solve_triangular(L, jnp.eye(alpha.shape[0]), check_finite=False, lower=True)\n",
    "    \n",
    "    return jnp.dot(R.T, R)\n",
    "\n",
    "\n",
    "def update_coeff(XT_y, beta, sigma_):\n",
    "    coef_ = beta * jnp.linalg.multi_dot([sigma_, XT_y])\n",
    "    return coef_\n",
    "\n",
    "\n",
    "def update(prior, X, y, gram, XT_y, alpha_prior, beta_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    alpha, beta = prior[:-1], prior[-1]\n",
    "    sigma = update_sigma(gram, alpha, beta)\n",
    "    coeffs = update_coeff(XT_y, beta, sigma)\n",
    "\n",
    "    # Update alpha and lambda\n",
    "    rmse_ = jnp.sum((y - jnp.dot(X, coeffs)) ** 2)\n",
    "    gamma_ = 1.0 - alpha * jnp.diag(sigma)\n",
    "\n",
    "    # TODO: Cap alpha with some threshold.\n",
    "    alpha = (gamma_ + 2.0 * alpha_prior[0]) / (\n",
    "        (coeffs.squeeze() ** 2 + 2.0 * alpha_prior[1])\n",
    "    )\n",
    "    beta = (n_samples - gamma_.sum() + 2.0 * beta_prior[0]) / (\n",
    "        rmse_ + 2.0 * beta_prior[1]\n",
    "    )\n",
    "\n",
    "    return jnp.concatenate([alpha, beta[jnp.newaxis]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stretch-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once for jit\n",
    "prior_params, metrics = fixed_point_solver(\n",
    "    update,\n",
    "    (theta_normed, dt, gram, XT_y, alpha_prior, beta_prior),\n",
    "    prior_init,\n",
    "    norm_weight,\n",
    "    tol=tol,\n",
    "    max_iter=max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "reasonable-bonus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.9 ms, sys: 0 ns, total: 58.9 ms\n",
      "Wall time: 58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prior_params, metrics = fixed_point_solver(\n",
    "    update,\n",
    "    (theta_normed, dt, gram, XT_y, alpha_prior, beta_prior),\n",
    "    prior_init,\n",
    "    norm_weight,\n",
    "    tol=tol,\n",
    "    max_iter=max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "limiting-polish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([6.4805281e-06, 3.1433667e-03, 4.4934018e-07, 3.0143492e-04,\n",
       "             1.5215245e-06, 5.6858559e-04, 2.7151845e-04, 1.2637673e-05,\n",
       "             4.5822901e-04, 8.2238701e-05, 2.4153102e-04, 4.4877052e-05,\n",
       "             0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.abs(prior_params_baseline - prior_params) / prior_params_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-elizabeth",
   "metadata": {},
   "source": [
    "Perfect - now in the next step let's merge sigma and mu and do it all efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-estonia",
   "metadata": {},
   "source": [
    "# V1 - everything by cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "concerned-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.linalg import solve_triangular\n",
    "from jax.numpy.linalg import cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "rental-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posterior(gram, XT_y, alpha, beta):\n",
    "    L = cholesky(jnp.diag(alpha) + beta * gram)\n",
    "    R = solve_triangular(L, jnp.eye(alpha.shape[0]), check_finite=False, lower=True)\n",
    "    sigma = jnp.dot(R.T, R)\n",
    "    mean = beta * jnp.dot(sigma, XT_y)\n",
    "    \n",
    "    return mean, sigma\n",
    "\n",
    "def update(prior, X, y, gram, XT_y, alpha_prior, beta_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    alpha, beta = prior[:-1], prior[-1]\n",
    "    coeffs, sigma = update_posterior(gram, XT_y, alpha, beta)\n",
    "\n",
    "    # Update alpha and lambda\n",
    "    rmse_ = jnp.sum((y - jnp.dot(X, coeffs)) ** 2)\n",
    "    gamma_ = 1.0 - alpha * jnp.diag(sigma)\n",
    "\n",
    "    # TODO: Cap alpha with some threshold.\n",
    "    alpha = (gamma_ + 2.0 * alpha_prior[0]) / (\n",
    "        (coeffs.squeeze() ** 2 + 2.0 * alpha_prior[1])\n",
    "    )\n",
    "    beta = (n_samples - gamma_.sum() + 2.0 * beta_prior[0]) / (\n",
    "        rmse_ + 2.0 * beta_prior[1]\n",
    "    )\n",
    "\n",
    "    return jnp.concatenate([alpha, beta[jnp.newaxis]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "inner-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once for jit\n",
    "prior_params, metrics = fixed_point_solver(\n",
    "    update,\n",
    "    (theta_normed, dt, gram, XT_y, alpha_prior, beta_prior),\n",
    "    prior_init,\n",
    "    norm_weight,\n",
    "    tol=tol,\n",
    "    max_iter=max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "close-correspondence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.5 ms, sys: 185 µs, total: 58.7 ms\n",
      "Wall time: 58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prior_params, metrics = fixed_point_solver(\n",
    "    update,\n",
    "    (theta_normed, dt, gram, XT_y, alpha_prior, beta_prior),\n",
    "    prior_init,\n",
    "    norm_weight,\n",
    "    tol=tol,\n",
    "    max_iter=max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "russian-muscle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([6.4805281e-06, 3.1433667e-03, 4.4934018e-07, 3.0143492e-04,\n",
       "             1.5215245e-06, 5.6858559e-04, 2.7151845e-04, 1.2637673e-05,\n",
       "             4.5822901e-04, 8.2238701e-05, 2.4153102e-04, 4.4877052e-05,\n",
       "             0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.abs(prior_params_baseline - prior_params) / prior_params_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "front-scoop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([3.8261841e+01, 1.4261522e+03, 1.5917919e+00, 1.4651965e+03,\n",
       "             3.5256809e-01, 1.4059528e+03, 1.0432008e+03, 1.2460576e+03,\n",
       "             8.3046985e+02, 1.3493773e+03, 8.2133289e+02, 1.3355118e+03,\n",
       "             4.2957764e+00], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-atmosphere",
   "metadata": {},
   "source": [
    "# V2 - Putting alpha and beta in separate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cleared-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.linalg import solve_triangular\n",
    "from jax.numpy.linalg import cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "younger-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posterior(gram, XT_y, prior):\n",
    "    alpha, beta = prior\n",
    "    L = cholesky(jnp.diag(alpha) + beta * gram)\n",
    "    R = solve_triangular(L, jnp.eye(alpha.shape[0]), check_finite=False, lower=True)\n",
    "    sigma = jnp.dot(R.T, R)\n",
    "    mean = beta * jnp.dot(sigma, XT_y)\n",
    "    \n",
    "    return mean, sigma\n",
    "\n",
    "def update_prior(X, y, posterior, prior, hyper_prior):\n",
    "    mean, covariance = posterior\n",
    "    alpha, beta = prior\n",
    "    alpha_prior, beta_prior = hyper_prior\n",
    "    \n",
    "  \n",
    "    rmse = jnp.sum((y - jnp.dot(X, mean)) ** 2)\n",
    "    gamma = 1.0 - alpha * jnp.diag(covariance)\n",
    "\n",
    "    # Update alpha and beta\n",
    "    alpha = (gamma + 2.0 * alpha_prior[0]) / (\n",
    "        (mean.squeeze() ** 2 + 2.0 * alpha_prior[1])\n",
    "    )\n",
    "    beta = (n_samples - jnp.sum(gamma) + 2.0 * beta_prior[0]) / (\n",
    "        rmse + 2.0 * beta_prior[1]\n",
    "    )\n",
    "\n",
    "    return jnp.minimum(1e7, alpha), jnp.minimum(1e7, beta)\n",
    "\n",
    "\n",
    "def update(prior, X, y, gram, XT_y, hyper_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    prior = (prior[:-1], prior[-1])\n",
    "    posterior = update_posterior(gram, XT_y, prior)\n",
    "    alpha, beta = update_prior(X, y, posterior, prior, hyper_prior)\n",
    "  \n",
    "    return jnp.concatenate([alpha, beta[jnp.newaxis]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "guided-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once for jit\n",
    "prior_params, metrics = fixed_point_solver(\n",
    "    update,\n",
    "    (theta_normed, dt, gram, XT_y, (alpha_prior, beta_prior)),\n",
    "    prior_init,\n",
    "    norm_weight,\n",
    "    tol=tol,\n",
    "    max_iter=max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "pharmaceutical-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 ms, sys: 74 µs, total: 57.5 ms\n",
      "Wall time: 56.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prior_params, metrics = fixed_point_solver(\n",
    "    update,\n",
    "    (theta_normed, dt, gram, XT_y, (alpha_prior, beta_prior)),\n",
    "    prior_init,\n",
    "    norm_weight,\n",
    "    tol=tol,\n",
    "    max_iter=max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "partial-fields",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([6.4805281e-06, 3.1433667e-03, 4.4934018e-07, 3.0143492e-04,\n",
       "             1.5215245e-06, 5.6858559e-04, 2.7151845e-04, 1.2637673e-05,\n",
       "             4.5822901e-04, 8.2238701e-05, 2.4153102e-04, 4.4877052e-05,\n",
       "             0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.abs(prior_params_baseline - prior_params) / prior_params_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ahead-compilation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([3.8261841e+01, 1.4261522e+03, 1.5917919e+00, 1.4651965e+03,\n",
       "             3.5256809e-01, 1.4059528e+03, 1.0432008e+03, 1.2460576e+03,\n",
       "             8.3046985e+02, 1.3493773e+03, 8.2133289e+02, 1.3355118e+03,\n",
       "             4.2957764e+00], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "intermediate-temperature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(1000, dtype=int32), DeviceArray(75.63577, dtype=float32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-stand",
   "metadata": {},
   "source": [
    "# v3 putting in evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-soldier",
   "metadata": {},
   "source": [
    "As a convergence criterium I want to use the mean gradient of the loss - luckily, there's an analytical expression in tipping. Let's implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "familiar-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posterior(gram, XT_y, prior):\n",
    "    alpha, beta = prior\n",
    "    L = cholesky(jnp.diag(alpha) + beta * gram)\n",
    "    R = solve_triangular(L, jnp.eye(alpha.shape[0]), check_finite=False, lower=True)\n",
    "    sigma = jnp.dot(R.T, R)\n",
    "    mean = beta * jnp.dot(sigma, XT_y)\n",
    "    \n",
    "    return mean, sigma\n",
    "\n",
    "def update_prior(X, y, posterior, prior, hyper_prior):\n",
    "    mean, covariance = posterior\n",
    "    alpha, beta = prior\n",
    "    alpha_prior, beta_prior = hyper_prior\n",
    "    \n",
    "  \n",
    "    rmse = jnp.sum((y - jnp.dot(X, mean)) ** 2)\n",
    "    gamma = 1.0 - alpha * jnp.diag(covariance)\n",
    "\n",
    "    # Update alpha and beta\n",
    "    alpha = (gamma + 2.0 * alpha_prior[0]) / (\n",
    "        (mean.squeeze() ** 2 + 2.0 * alpha_prior[1])\n",
    "    )\n",
    "    beta = (n_samples - jnp.sum(gamma) + 2.0 * beta_prior[0]) / (\n",
    "        rmse + 2.0 * beta_prior[1]\n",
    "    )\n",
    "    \n",
    "    # Calculating dL/da\n",
    "    dLda = 1/2 * (1 / alpha - (mean.squeeze()**2 + jnp.diag(covariance))) + alpha_prior[0] / alpha - alpha_prior[1]\n",
    "    return jnp.minimum(1e7, alpha), jnp.minimum(1e7, beta), jnp.mean(jnp.abs(dLda))\n",
    "\n",
    "\n",
    "def update(prior, X, y, gram, XT_y, hyper_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    alpha, beta = prior[:-2], prior[-2]\n",
    "    posterior = update_posterior(gram, XT_y, (alpha, beta))\n",
    "    alpha, beta, loss_grad = update_prior(X, y, posterior, (alpha, beta), hyper_prior)\n",
    "  \n",
    "    return jnp.concatenate([alpha, beta[jnp.newaxis], loss_grad[jnp.newaxis]], axis=0)\n",
    "\n",
    "def evidence(X, y, gram, XT_y, prior, hyper_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    alpha, beta = prior[:-1], prior[-1]\n",
    "    alpha_prior, beta_prior = hyper_prior\n",
    "    \n",
    "    mean, covariance = update_posterior(gram, XT_y, (alpha, beta))\n",
    "    rmse = jnp.sum((y - jnp.dot(X, mean)) ** 2)\n",
    "\n",
    "    score = jnp.sum(alpha_prior[0] * jnp.log(alpha) - alpha_prior[1] * alpha)\n",
    "    score += beta_prior[0] * jnp.log(beta) - beta_prior[1] * beta\n",
    "    score += 0.5 * (\n",
    "        jnp.linalg.slogdet(covariance)[1]\n",
    "        + n_samples * jnp.log(beta)\n",
    "        + jnp.sum(jnp.log(alpha))\n",
    "    )\n",
    "    score -= 0.5 * (beta * rmse + jnp.sum(alpha * mean.squeeze() ** 2))\n",
    "\n",
    "    return score.squeeze(), coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "rubber-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_weight = jnp.concatenate((jnp.ones((n_features,)), jnp.zeros((2,))), axis=0)\n",
    "prior_init = jnp.concatenate([jnp.ones((n_features,)), (1.0 / (jnp.var(dt) + 1e-7))[jnp.newaxis], jnp.ones((1, ))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "computational-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once for jit\n",
    "prior_params, metrics = fixed_point_solver(\n",
    "    update,\n",
    "    (theta_normed, dt, gram, XT_y, (alpha_prior, beta_prior)),\n",
    "    prior_init,\n",
    "    norm_weight,\n",
    "    tol=tol,\n",
    "    max_iter=max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "challenging-average",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 ms, sys: 68 µs, total: 57.4 ms\n",
      "Wall time: 56.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prior_params, metrics = fixed_point_solver(\n",
    "    update,\n",
    "    (theta_normed, dt, gram, XT_y, (alpha_prior, beta_prior)),\n",
    "    prior_init,\n",
    "    norm_weight,\n",
    "    tol=tol,\n",
    "    max_iter=max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "surgical-liberty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(2.2866866e-06, dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_params[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-blend",
   "metadata": {},
   "source": [
    "That seems higher then what I observed when taking the gradient of the evidence - let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "beautiful-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "dprior = jax.grad(lambda prior: evidence(theta_normed, dt, gram, XT_y, prior, (alpha_prior, beta_prior))[0])(prior_params[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "unsigned-finish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(7.2724504e-08, dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.mean(jnp.abs(dprior[:-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-banner",
   "metadata": {},
   "source": [
    "It def is but with these numbers I think there's some numerical issue. Now let's build a function using a custom cond_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-omega",
   "metadata": {},
   "source": [
    "# Full SBL function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stuffed-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "from functools import partial\n",
    "from jax import lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "determined-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_posterior(gram, XT_y, prior):\n",
    "    alpha, beta = prior\n",
    "    L = cholesky(jnp.diag(alpha) + beta * gram)\n",
    "    R = solve_triangular(L, jnp.eye(alpha.shape[0]), check_finite=False, lower=True)\n",
    "    sigma = jnp.dot(R.T, R)\n",
    "    mean = beta * jnp.dot(sigma, XT_y)\n",
    "    \n",
    "    return mean, sigma\n",
    "\n",
    "def update_prior(X, y, posterior, prior, hyper_prior):\n",
    "    mean, covariance = posterior\n",
    "    alpha, beta = prior\n",
    "    alpha_prior, beta_prior = hyper_prior\n",
    "    \n",
    "    rmse = jnp.sum((y - jnp.dot(X, mean)) ** 2)\n",
    "    gamma = 1.0 - alpha * jnp.diag(covariance)\n",
    "\n",
    "    # Update alpha and beta\n",
    "    alpha = (gamma + 2.0 * alpha_prior[0]) / (\n",
    "        (mean.squeeze() ** 2 + 2.0 * alpha_prior[1])\n",
    "    )\n",
    "    beta = (n_samples - jnp.sum(gamma) + 2.0 * beta_prior[0]) / (\n",
    "        rmse + 2.0 * beta_prior[1]\n",
    "    )\n",
    "    \n",
    "    # Calculating dL/da\n",
    "    dLda = 1 / alpha * (1/2 * (1 - alpha * (mean.squeeze()**2 + jnp.diag(covariance))) + alpha_prior[0] - alpha * alpha_prior[1])\n",
    "    return jnp.minimum(1e7, alpha), jnp.minimum(1e7, beta), jnp.mean(jnp.abs(dLda))\n",
    "\n",
    "\n",
    "def update(prior, X, y, gram, XT_y, hyper_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    alpha, beta = prior[:-2], prior[-2]\n",
    "    posterior = update_posterior(gram, XT_y, (alpha, beta))\n",
    "    alpha, beta, loss_grad = update_prior(X, y, posterior, (alpha, beta), hyper_prior)\n",
    "  \n",
    "    return jnp.concatenate([alpha, beta[jnp.newaxis], loss_grad[jnp.newaxis]], axis=0)\n",
    "\n",
    "def evidence(X, y, gram, XT_y, prior, hyper_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    alpha, beta = prior[:-1], prior[-1]\n",
    "    alpha_prior, beta_prior = hyper_prior\n",
    "    \n",
    "    mean, covariance = update_posterior(gram, XT_y, (alpha, beta))\n",
    "    rmse = jnp.sum((y - jnp.dot(X, mean)) ** 2)\n",
    "\n",
    "    score = jnp.sum(alpha_prior[0] * jnp.log(alpha) - alpha_prior[1] * alpha)\n",
    "    score += beta_prior[0] * jnp.log(beta) - beta_prior[1] * beta\n",
    "    score += 0.5 * (\n",
    "        jnp.linalg.slogdet(covariance)[1]\n",
    "        + n_samples * jnp.log(beta)\n",
    "        + jnp.sum(jnp.log(alpha))\n",
    "    )\n",
    "    score -= 0.5 * (beta * rmse + jnp.sum(alpha * mean.squeeze() ** 2))\n",
    "\n",
    "    return score.squeeze(), mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "surprising-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=(0, 2))\n",
    "def fwd_solver_2(f, z_init, cond_fun, max_iter=300):\n",
    "    # n_features calculates the norm over the first n_features of z.\n",
    "    # Useful for when you're iterating over a but check your convergence on b\n",
    "    # such as with SBL.\n",
    "    def _cond_fun(carry):\n",
    "        z_prev, z, iteration = carry\n",
    "        return jax.lax.cond(iteration >= max_iter, \n",
    "                     lambda _: False, \n",
    "                     lambda args: cond_fun(*args), \n",
    "                     (z_prev, z))\n",
    "\n",
    "    def body_fun(carry):\n",
    "        _, z, iteration = carry\n",
    "        return z, f(z), iteration + 1\n",
    "\n",
    "    init_carry = (z_init, f(z_init), 0, )\n",
    "    z_star, _, metrics = lax.while_loop(_cond_fun, body_fun, init_carry)\n",
    "    return z_star, metrics\n",
    "\n",
    "@partial(jit, static_argnums=(0, 3))\n",
    "def fixed_point_solver_2(f, args, z_init, cond_fun, max_iter=300):\n",
    "    z_star, metrics = fwd_solver_2(\n",
    "        lambda z: f(z, *args), z_init, cond_fun, max_iter=max_iter,\n",
    "    )\n",
    "    return z_star, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "whole-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_init = jnp.concatenate([jnp.ones((n_features,)), (1.0 / (jnp.var(dt) + 1e-7))[jnp.newaxis], jnp.ones((1, ))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "boxed-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# once for jit\n",
    "prior_params, metrics = fixed_point_solver_2(\n",
    "    update,\n",
    "    (theta_normed, dt, gram, XT_y, (alpha_prior, beta_prior)),\n",
    "    prior_init,\n",
    "    lambda z_prev, z: z[-1] > 1e-5,\n",
    "    max_iter=max_iter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-lebanon",
   "metadata": {},
   "source": [
    "This works well - now let's wrap it in a full SBL function and update the fwd solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "tropical-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SBL(\n",
    "    X,\n",
    "    y,\n",
    "    prior_init=None,\n",
    "    hyper_prior=((1e-6, 1e-6), (1e-6, 1e-6)), \n",
    "    tol=1e-5,\n",
    "    max_iter=1000,\n",
    "    stop_prior_grad=True):\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    if prior_init is None:\n",
    "        prior_init = jnp.ones((n_features + 2, ))\n",
    "        prior_init = jax.ops.index_update(prior_init, n_features, 1 / (jnp.var(y) + 1e-6)) # setting initial noise value\n",
    " \n",
    "    gram = jnp.dot(X.T, X)\n",
    "    XT_y = jnp.dot(X.T, y)\n",
    "\n",
    "    prior_params, metrics = fixed_point_solver_2(\n",
    "        update,\n",
    "        (X, y, gram, XT_y, hyper_prior),\n",
    "        prior_init,\n",
    "        lambda z_prev, z: z[-1] > tol,\n",
    "        max_iter=max_iter,\n",
    "    )\n",
    "    \n",
    "    if stop_prior_grad:\n",
    "        prior = lax.stop_gradient(prior_params[:-1])\n",
    "    else:\n",
    "        prior = prior_params[:-1]\n",
    "        \n",
    "    loss, mn = evidence(X, y, gram, XT_y, prior, hyper_prior)\n",
    "    return loss, mn, prior, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "different-weapon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 628 ms, sys: 10.4 ms, total: 638 ms\n",
      "Wall time: 247 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray(455.7086, dtype=float32),\n",
       " DeviceArray([[ 4.8160609e-02],\n",
       "              [-1.4313304e-05],\n",
       "              [ 6.5489328e-01],\n",
       "              [-8.3149920e-05],\n",
       "              [ 1.6062721e+00],\n",
       "              [ 7.1582544e-05],\n",
       "              [-6.0125935e-04],\n",
       "              [ 7.5151841e-04],\n",
       "              [-3.0715278e-04],\n",
       "              [ 4.9016980e-04],\n",
       "              [ 6.5001904e-04],\n",
       "              [-4.6155893e-04]], dtype=float32),\n",
       " DeviceArray([3.8261742e+01, 1.6299744e+03, 1.5917979e+00, 1.4587059e+03,\n",
       "              3.5256785e-01, 1.3100173e+03, 1.0432620e+03, 1.2459415e+03,\n",
       "              8.3059930e+02, 1.3490701e+03, 8.2140729e+02, 1.3352468e+03,\n",
       "              4.2957759e+00], dtype=float32),\n",
       " DeviceArray(472, dtype=int32))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SBL(theta_normed, dt, None, (alpha_prior, beta_prior), tol=1e-5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "resident-seller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 628 ms, sys: 0 ns, total: 628 ms\n",
      "Wall time: 247 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(DeviceArray(455.7086, dtype=float32),\n",
       " DeviceArray([[ 4.8160609e-02],\n",
       "              [-1.4313304e-05],\n",
       "              [ 6.5489328e-01],\n",
       "              [-8.3149920e-05],\n",
       "              [ 1.6062721e+00],\n",
       "              [ 7.1582544e-05],\n",
       "              [-6.0125935e-04],\n",
       "              [ 7.5151841e-04],\n",
       "              [-3.0715278e-04],\n",
       "              [ 4.9016980e-04],\n",
       "              [ 6.5001904e-04],\n",
       "              [-4.6155893e-04]], dtype=float32),\n",
       " DeviceArray([3.8261742e+01, 1.6299744e+03, 1.5917979e+00, 1.4587059e+03,\n",
       "              3.5256785e-01, 1.3100173e+03, 1.0432620e+03, 1.2459415e+03,\n",
       "              8.3059930e+02, 1.3490701e+03, 8.2140729e+02, 1.3352468e+03,\n",
       "              4.2957759e+00], dtype=float32),\n",
       " DeviceArray(472, dtype=int32))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "SBL(theta_normed, dt, None, (alpha_prior, beta_prior), tol=1e-5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-blame",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
