{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "processed-neighborhood",
   "metadata": {},
   "source": [
    "I'm pretty sure the issue is with the custom back-prop. If we don't backpropagate through alpha we should still get a good result, let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stable-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "from jax import numpy as jnp, random\n",
    "import jax\n",
    "from modax.data.burgers import burgers\n",
    "from modax.data.kdv import doublesoliton\n",
    "from modax.models import Deepmod\n",
    "from modax.training.utils import create_update\n",
    "from flax import optim\n",
    "\n",
    "from modax.training import train_max_iter\n",
    "from modax.training.losses.utils import precision, normal_LL\n",
    "from modax.utils.forward_solver import fixed_point_solver\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "starting-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Making data\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "x = jnp.linspace(-3, 4, 50)\n",
    "t = jnp.linspace(0.5, 5.0, 20)\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "u = burgers(x_grid, t_grid, 0.1, 1.0)\n",
    "\n",
    "X = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y = u.reshape(-1, 1)\n",
    "y += 0.10 * jnp.std(y) * random.normal(key, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "going-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SBL(\n",
    "    X,\n",
    "    y,\n",
    "    prior_init=None,\n",
    "    alpha_prior=(1e-6, 1e-6),\n",
    "    beta_prior=(1e-6, 1e-6),\n",
    "    tol=1e-3,\n",
    "    max_iter=300,\n",
    "):\n",
    "    n_samples, n_features = X.shape\n",
    "    norm_weight = jnp.concatenate((jnp.ones((n_features,)), jnp.zeros((1,))), axis=0)\n",
    "    if prior_init is None:\n",
    "        prior_init = jnp.concatenate(\n",
    "            [jnp.ones((n_features,)), (1.0 / (jnp.var(y) + 1e-7))[jnp.newaxis]], axis=0\n",
    "        )\n",
    "    # adding zeros to z for coeffs\n",
    "    gram = jnp.dot(X.T, X)\n",
    "    XT_y = jnp.dot(X.T, y)\n",
    "\n",
    "    prior_params, metrics = fixed_point_solver(\n",
    "        update,\n",
    "        (X, y, gram, XT_y, alpha_prior, beta_prior),\n",
    "        prior_init,\n",
    "        norm_weight,\n",
    "        tol=tol,\n",
    "        max_iter=max_iter,\n",
    "    )\n",
    "    \n",
    "    prior = jax.lax.stop_gradient(prior_params) # no it doesnt backprop through the prior\n",
    "    loss, mn = evidence(X, y, prior, gram, XT_y, alpha_prior, beta_prior)\n",
    "\n",
    "    return loss, mn, prior, metrics\n",
    "\n",
    "\n",
    "def update_sigma(gram, alpha, beta):\n",
    "    sigma_inv = jnp.diag(alpha) + beta * gram\n",
    "    L_inv = jnp.linalg.pinv(jnp.linalg.cholesky(sigma_inv))\n",
    "    sigma_ = jnp.dot(L_inv.T, L_inv)\n",
    "    return sigma_\n",
    "\n",
    "\n",
    "def update_coeff(XT_y, beta, sigma_):\n",
    "    coef_ = beta * jnp.linalg.multi_dot([sigma_, XT_y])\n",
    "    return coef_\n",
    "\n",
    "\n",
    "def update(prior, X, y, gram, XT_y, alpha_prior, beta_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    alpha, beta = prior[:-1], prior[-1]\n",
    "    sigma = update_sigma(gram, alpha, beta)\n",
    "    coeffs = update_coeff(XT_y, beta, sigma)\n",
    "\n",
    "    # Update alpha and lambda\n",
    "    rmse_ = jnp.sum((y - jnp.dot(X, coeffs)) ** 2)\n",
    "    gamma_ = 1.0 - alpha * jnp.diag(sigma)\n",
    "\n",
    "    # TODO: Cap alpha with some threshold.\n",
    "    alpha = (gamma_ + 2.0 * alpha_prior[0]) / (\n",
    "        (coeffs.squeeze() ** 2 + 2.0 * alpha_prior[1])\n",
    "    )\n",
    "    beta = (n_samples - gamma_.sum() + 2.0 * beta_prior[0]) / (\n",
    "        rmse_ + 2.0 * beta_prior[1]\n",
    "    )\n",
    "\n",
    "    return jnp.concatenate([alpha, beta[jnp.newaxis]], axis=0)\n",
    "\n",
    "\n",
    "def evidence(X, y, prior, gram, XT_y, alpha_prior, beta_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    alpha, beta = prior[:-1], prior[-1]\n",
    "\n",
    "    sigma = update_sigma(gram, alpha, beta)\n",
    "    coeffs = update_coeff(XT_y, beta, sigma)\n",
    "    rmse_ = jnp.sum((y - jnp.dot(X, coeffs)) ** 2)\n",
    "\n",
    "    score = jnp.sum(alpha_prior[0] * jnp.log(alpha) - alpha_prior[1] * alpha)\n",
    "    score += beta_prior[0] * jnp.log(beta) - beta_prior[1] * beta\n",
    "    score += 0.5 * (\n",
    "        jnp.linalg.slogdet(sigma)[1]\n",
    "        + n_samples * jnp.log(beta)\n",
    "        + jnp.sum(jnp.log(alpha))\n",
    "    )\n",
    "    score -= 0.5 * (beta * rmse_ + jnp.sum(alpha * coeffs.squeeze() ** 2))\n",
    "\n",
    "    return score.squeeze(), coeffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vocational-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_SBL(params, state, model, X, y, warm_restart=True):\n",
    "    model_state, loss_state = state\n",
    "    variables = {\"params\": params, **model_state}\n",
    "    (prediction, dt, theta, coeffs), updated_model_state = model.apply(\n",
    "        variables, X, mutable=list(model_state.keys())\n",
    "    )\n",
    "\n",
    "    n_samples, n_features = theta.shape\n",
    "    prior_params_mse = (0.0, 0.0)\n",
    "\n",
    "    # MSE stuff\n",
    "    tau = precision(y, prediction, *prior_params_mse)\n",
    "    p_mse, MSE = normal_LL(prediction, y, tau)\n",
    "\n",
    "    # Regression stuff\n",
    "    # we dont want the gradient\n",
    "    hyper_prior_params = (\n",
    "        n_samples / 2,\n",
    "        n_samples / (2 * jax.lax.stop_gradient(tau)),\n",
    "    )\n",
    "    theta_normed = theta / jnp.linalg.norm(theta, axis=0)\n",
    "\n",
    "    if (loss_state[\"prior_init\"] is None) or (warm_restart is False):\n",
    "        prior_init = jnp.concatenate(\n",
    "            [jnp.ones((n_features,)), 1.0 / jnp.var(dt)[jnp.newaxis]]\n",
    "        )\n",
    "    else:\n",
    "        prior_init = loss_state[\"prior_init\"]\n",
    "\n",
    "    p_reg, mn, prior, fwd_metric = SBL(\n",
    "        theta_normed,\n",
    "        dt,\n",
    "        prior_init=prior_init,\n",
    "        beta_prior=hyper_prior_params,\n",
    "        tol=1e-3,\n",
    "        max_iter=300,\n",
    "    )\n",
    "\n",
    "    Reg = jnp.mean((dt - theta_normed @ mn) ** 2)\n",
    "\n",
    "    loss_state[\"prior_init\"] = prior\n",
    "    loss = -(p_mse + p_reg)\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"p_mse\": p_mse,\n",
    "        \"mse\": MSE,\n",
    "        \"p_reg\": p_reg,\n",
    "        \"reg\": Reg,\n",
    "        \"bayes_coeffs\": mn,\n",
    "        \"coeffs\": coeffs,\n",
    "        \"alpha\": prior[:-1],\n",
    "        \"beta\": prior[-1],\n",
    "        \"tau\": tau,\n",
    "        \"its\": fwd_metric[0],\n",
    "        \"gap\": fwd_metric[1],\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        loss,\n",
    "        ((updated_model_state, loss_state), metrics, (prediction, dt, theta, mn)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "early-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "\n",
    "state = (state, {\"prior_init\": None})  # adding prior to state\n",
    "update_fn = create_update(loss_fn_SBL, (model, X, y, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thick-atlantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: -2912.77294921875\n",
      "Loss step 100: -7139.28369140625\n",
      "Loss step 200: -8828.705078125\n",
      "Loss step 300: -9576.6103515625\n",
      "Loss step 400: -9888.5791015625\n",
      "Loss step 500: -9978.798828125\n",
      "Loss step 600: -10031.1171875\n",
      "Loss step 700: -10059.4697265625\n",
      "Loss step 800: -10077.68359375\n",
      "Loss step 900: -10092.5712890625\n",
      "Loss step 1000: -10095.96484375\n",
      "Loss step 1100: -10100.787109375\n",
      "Loss step 1200: -10103.408203125\n",
      "Loss step 1300: -10103.4306640625\n",
      "Loss step 1400: -10104.515625\n",
      "Loss step 1500: -10104.8994140625\n",
      "Loss step 1600: -10105.02734375\n",
      "Loss step 1700: -10104.728515625\n",
      "Loss step 1800: -10104.365234375\n",
      "Loss step 1900: -10105.572265625\n",
      "Loss step 2000: -10105.1181640625\n",
      "Loss step 2100: -10105.912109375\n",
      "Loss step 2200: -10105.462890625\n",
      "Loss step 2300: -10104.85546875\n",
      "Loss step 2400: -10106.134765625\n",
      "Loss step 2500: -10106.314453125\n",
      "Loss step 2600: -10106.23828125\n",
      "Loss step 2700: -10106.544921875\n",
      "Loss step 2800: -10105.14453125\n",
      "Loss step 2900: -10106.2958984375\n",
      "Loss step 3000: -10105.837890625\n",
      "Loss step 3100: -10106.8271484375\n",
      "Loss step 3200: -10105.8427734375\n",
      "Loss step 3300: -10107.3349609375\n",
      "Loss step 3400: -10106.9599609375\n",
      "Loss step 3500: -10106.08984375\n",
      "Loss step 3600: -10107.0908203125\n",
      "Loss step 3700: -10107.62109375\n",
      "Loss step 3800: -10106.642578125\n",
      "Loss step 3900: -10107.4453125\n",
      "Loss step 4000: -10107.349609375\n",
      "Loss step 4100: -10106.869140625\n",
      "Loss step 4200: -10106.9150390625\n",
      "Loss step 4300: -10107.4453125\n",
      "Loss step 4400: -10107.421875\n",
      "Loss step 4500: -10107.65234375\n",
      "Loss step 4600: -10107.404296875\n",
      "Loss step 4700: -10107.82421875\n",
      "Loss step 4800: -10108.001953125\n",
      "Loss step 4900: -10106.853515625\n"
     ]
    }
   ],
   "source": [
    "optimizer, state = train_max_iter(update_fn, optimizer, state, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-resident",
   "metadata": {},
   "source": [
    "Wow that worked amazing - let's try it with kdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spare-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "x = jnp.linspace(-10, 10, 100)\n",
    "t = jnp.linspace(0.1, 1.0, 10)\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "u = doublesoliton(x_grid, t_grid, c=[5.0, 2.0], x0=[0.0, -5.0])\n",
    "\n",
    "X = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y = u.reshape(-1, 1)\n",
    "y += 0.10 * jnp.std(y) * random.normal(key, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "august-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "\n",
    "state = (state, {\"prior_init\": None})  # adding prior to state\n",
    "update_fn = create_update(loss_fn_SBL, (model, X, y, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "liberal-picking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: 577.9468994140625\n",
      "Loss step 100: -463.724853515625\n",
      "Loss step 200: -1174.6539306640625\n",
      "Loss step 300: -2272.0556640625\n",
      "Loss step 400: -2773.500732421875\n",
      "Loss step 500: -3403.671142578125\n",
      "Loss step 600: -3751.59521484375\n",
      "Loss step 700: -4755.078125\n",
      "Loss step 800: -6396.3857421875\n",
      "Loss step 900: -6653.75439453125\n",
      "Loss step 1000: -6718.2197265625\n",
      "Loss step 1100: -6740.65380859375\n",
      "Loss step 1200: -6745.4296875\n",
      "Loss step 1300: -6745.13818359375\n",
      "Loss step 1400: -6755.9833984375\n",
      "Loss step 1500: -6752.78564453125\n",
      "Loss step 1600: -6723.474609375\n",
      "Loss step 1700: -6758.52392578125\n",
      "Loss step 1800: -6756.23291015625\n",
      "Loss step 1900: -6753.6591796875\n",
      "Loss step 2000: -6755.51025390625\n",
      "Loss step 2100: -6759.00390625\n",
      "Loss step 2200: -6763.03173828125\n",
      "Loss step 2300: -6755.2734375\n",
      "Loss step 2400: -6752.0966796875\n",
      "Loss step 2500: -6763.1015625\n",
      "Loss step 2600: -6758.123046875\n",
      "Loss step 2700: -6765.3212890625\n",
      "Loss step 2800: -6754.98291015625\n",
      "Loss step 2900: -6760.314453125\n",
      "Loss step 3000: -6762.98095703125\n",
      "Loss step 3100: -6761.611328125\n",
      "Loss step 3200: -6762.81005859375\n",
      "Loss step 3300: -6769.26708984375\n",
      "Loss step 3400: -6758.3154296875\n",
      "Loss step 3500: -6759.369140625\n",
      "Loss step 3600: -6737.357421875\n",
      "Loss step 3700: -6758.82373046875\n",
      "Loss step 3800: -6753.83349609375\n",
      "Loss step 3900: -6746.1806640625\n",
      "Loss step 4000: -6759.0556640625\n",
      "Loss step 4100: -6757.9833984375\n",
      "Loss step 4200: -6764.78759765625\n",
      "Loss step 4300: -6756.89013671875\n",
      "Loss step 4400: -6756.306640625\n",
      "Loss step 4500: -6766.4375\n",
      "Loss step 4600: -6753.4912109375\n",
      "Loss step 4700: -6761.505859375\n",
      "Loss step 4800: -6767.1884765625\n",
      "Loss step 4900: -6761.49853515625\n"
     ]
    }
   ],
   "source": [
    "optimizer, state = train_max_iter(update_fn, optimizer, state, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-master",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
