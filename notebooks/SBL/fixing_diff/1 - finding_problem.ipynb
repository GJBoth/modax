{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "south-hollywood",
   "metadata": {},
   "source": [
    "We know how to recreate the problem and ensure it's not the jit - now let's try and figure out why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alert-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "from jax import numpy as jnp, random\n",
    "import jax\n",
    "from modax.data.kdv import doublesoliton\n",
    "from modax.models import Deepmod\n",
    "from modax.training.utils import create_update\n",
    "from flax import optim\n",
    "\n",
    "from modax.training import train_max_iter\n",
    "from modax.training.losses.utils import precision, normal_LL\n",
    "from modax.utils.forward_solver import fixed_point_solver\n",
    "\n",
    "from flax.core import unfreeze\n",
    "from flax.traverse_util import flatten_dict\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "facial-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SBL(\n",
    "    X,\n",
    "    y,\n",
    "    prior_init=None,\n",
    "    alpha_prior=(1e-6, 1e-6),\n",
    "    beta_prior=(1e-6, 1e-6),\n",
    "    tol=1e-3,\n",
    "    max_iter=300,\n",
    "    non_diff=True\n",
    "):\n",
    "    n_samples, n_features = X.shape\n",
    "    norm_weight = jnp.concatenate((jnp.ones((n_features,)), jnp.zeros((1,))), axis=0)\n",
    "    if prior_init is None:\n",
    "        prior_init = jnp.concatenate(\n",
    "            [jnp.ones((n_features,)), (1.0 / (jnp.var(y) + 1e-7))[jnp.newaxis]], axis=0\n",
    "        )\n",
    "    # adding zeros to z for coeffs\n",
    "    gram = jnp.dot(X.T, X)\n",
    "    XT_y = jnp.dot(X.T, y)\n",
    "\n",
    "    prior_params, metrics = fixed_point_solver(\n",
    "        update,\n",
    "        (X, y, gram, XT_y, alpha_prior, beta_prior),\n",
    "        prior_init,\n",
    "        norm_weight,\n",
    "        tol=tol,\n",
    "        max_iter=max_iter,\n",
    "    )\n",
    "    \n",
    "    if non_diff:\n",
    "        prior = jax.lax.stop_gradient(prior_params) # no it doesnt backprop through the prior\n",
    "    else:\n",
    "        prior = prior_params\n",
    "    loss, mn = evidence(X, y, prior, gram, XT_y, alpha_prior, beta_prior)\n",
    "\n",
    "    return loss, mn, prior, metrics\n",
    "\n",
    "def update_sigma(gram, alpha, beta):\n",
    "    sigma_inv = jnp.diag(alpha) + beta * gram\n",
    "    L_inv = jnp.linalg.pinv(jnp.linalg.cholesky(sigma_inv))\n",
    "    sigma_ = jnp.dot(L_inv.T, L_inv)\n",
    "    return sigma_\n",
    "\n",
    "\n",
    "def update_coeff(XT_y, beta, sigma_):\n",
    "    coef_ = beta * jnp.linalg.multi_dot([sigma_, XT_y])\n",
    "    return coef_\n",
    "\n",
    "\n",
    "def update(prior, X, y, gram, XT_y, alpha_prior, beta_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    alpha, beta = prior[:-1], prior[-1]\n",
    "    sigma = update_sigma(gram, alpha, beta)\n",
    "    coeffs = update_coeff(XT_y, beta, sigma)\n",
    "\n",
    "    # Update alpha and lambda\n",
    "    rmse_ = jnp.sum((y - jnp.dot(X, coeffs)) ** 2)\n",
    "    gamma_ = 1.0 - alpha * jnp.diag(sigma)\n",
    "\n",
    "    # TODO: Cap alpha with some threshold.\n",
    "    alpha = (gamma_ + 2.0 * alpha_prior[0]) / (\n",
    "        (coeffs.squeeze() ** 2 + 2.0 * alpha_prior[1])\n",
    "    )\n",
    "    beta = (n_samples - gamma_.sum() + 2.0 * beta_prior[0]) / (\n",
    "        rmse_ + 2.0 * beta_prior[1]\n",
    "    )\n",
    "\n",
    "    return jnp.concatenate([alpha, beta[jnp.newaxis]], axis=0)\n",
    "\n",
    "\n",
    "def evidence(X, y, prior, gram, XT_y, alpha_prior, beta_prior):\n",
    "    n_samples, n_features = X.shape\n",
    "    alpha, beta = prior[:-1], prior[-1]\n",
    "\n",
    "    sigma = update_sigma(gram, alpha, beta)\n",
    "    coeffs = update_coeff(XT_y, beta, sigma)\n",
    "    rmse_ = jnp.sum((y - jnp.dot(X, coeffs)) ** 2)\n",
    "\n",
    "    score = jnp.sum(alpha_prior[0] * jnp.log(alpha) - alpha_prior[1] * alpha)\n",
    "    score += beta_prior[0] * jnp.log(beta) - beta_prior[1] * beta\n",
    "    score += 0.5 * (\n",
    "        jnp.linalg.slogdet(sigma)[1]\n",
    "        + n_samples * jnp.log(beta)\n",
    "        + jnp.sum(jnp.log(alpha))\n",
    "    )\n",
    "    score -= 0.5 * (beta * rmse_ + jnp.sum(alpha * coeffs.squeeze() ** 2))\n",
    "\n",
    "    return score.squeeze(), coeffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "steady-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_SBL(params, state, model, X, y, warm_restart=True, non_diff=True):\n",
    "    model_state, loss_state = state\n",
    "    variables = {\"params\": params, **model_state}\n",
    "    (prediction, dt, theta, coeffs), updated_model_state = model.apply(\n",
    "        variables, X, mutable=list(model_state.keys())\n",
    "    )\n",
    "\n",
    "    n_samples, n_features = theta.shape\n",
    "    prior_params_mse = (0.0, 0.0)\n",
    "\n",
    "    # MSE stuff\n",
    "    tau = precision(y, prediction, *prior_params_mse)\n",
    "    p_mse, MSE = normal_LL(prediction, y, tau)\n",
    "\n",
    "    # Regression stuff\n",
    "    # we dont want the gradient\n",
    "    hyper_prior_params = (\n",
    "        n_samples / 2,\n",
    "        n_samples / (2 * jax.lax.stop_gradient(tau)),\n",
    "    )\n",
    "    theta_normed = theta / jnp.linalg.norm(theta, axis=0)\n",
    "\n",
    "    if (loss_state[\"prior_init\"] is None) or (warm_restart is False):\n",
    "        prior_init = jnp.concatenate(\n",
    "            [jnp.ones((n_features,)), 1.0 / jnp.var(dt)[jnp.newaxis]]\n",
    "        )\n",
    "    else:\n",
    "        prior_init = loss_state[\"prior_init\"]\n",
    "\n",
    "    p_reg, mn, prior, fwd_metric = SBL(\n",
    "        theta_normed,\n",
    "        dt,\n",
    "        prior_init=prior_init,\n",
    "        beta_prior=hyper_prior_params,\n",
    "        tol=1e-3,\n",
    "        max_iter=300,\n",
    "        non_diff=non_diff\n",
    "    )\n",
    "\n",
    "    Reg = jnp.mean((dt - theta_normed @ mn) ** 2)\n",
    "\n",
    "    updated_loss_state = {\"prior_init\": prior}\n",
    "    loss = -(p_mse + p_reg)\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"p_mse\": p_mse,\n",
    "        \"mse\": MSE,\n",
    "        \"p_reg\": p_reg,\n",
    "        \"reg\": Reg,\n",
    "        \"bayes_coeffs\": mn,\n",
    "        \"coeffs\": coeffs,\n",
    "        \"alpha\": prior[:-1],\n",
    "        \"beta\": prior[-1],\n",
    "        \"tau\": tau,\n",
    "        \"its\": fwd_metric[0],\n",
    "        \"gap\": fwd_metric[1],\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        loss,\n",
    "        ((updated_model_state, updated_loss_state), metrics, (prediction, dt, theta, mn)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stainless-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "x = jnp.linspace(-10, 10, 100)\n",
    "t = jnp.linspace(0.1, 1.0, 10)\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "u = doublesoliton(x_grid, t_grid, c=[5.0, 2.0], x0=[0.0, -5.0])\n",
    "\n",
    "X = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y = u.reshape(-1, 1)\n",
    "y += 0.10 * jnp.std(y) * random.normal(key, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "architectural-place",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "\n",
    "state = (state, {\"prior_init\": None})  # adding prior to state\n",
    "update_fn = create_update(loss_fn_SBL, (model, X, y, True, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-fabric",
   "metadata": {},
   "source": [
    "To pinpoint the problem,let's look at the params after the first epoch (without jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "floating-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jax.disable_jit():\n",
    "    (new_optimizer, new_state), metrics, output = update_fn(optimizer, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "virgin-assets",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FrozenDict({\n",
       "     vars: {\n",
       "         LeastSquares_0: {\n",
       "             mask: DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                           True,  True,  True,  True], dtype=bool),\n",
       "         },\n",
       "     },\n",
       " }),\n",
       " {'prior_init': None})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "binary-exception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FrozenDict({\n",
       "     vars: {\n",
       "         LeastSquares_0: {\n",
       "             mask: DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                           True,  True,  True,  True], dtype=bool),\n",
       "         },\n",
       "     },\n",
       " }),\n",
       " {'prior_init': DeviceArray([3.8261806e+01, 1.2139067e+03, 1.5917789e+00, 1.3994301e+03,\n",
       "               3.5256889e-01, 1.6201604e+03, 1.0433588e+03, 1.2462245e+03,\n",
       "               8.3063586e+02, 1.3484807e+03, 8.2134668e+02, 1.3358071e+03,\n",
       "               4.2957764e+00], dtype=float32)})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-residence",
   "metadata": {},
   "source": [
    "Wait so I actually overwrote the loss_state variable; let's fix this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sudden-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "\n",
    "state = (state, {\"prior_init\": None})  # adding prior to state\n",
    "update_fn = create_update(loss_fn_SBL, (model, X, y, True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "emerging-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jax.disable_jit():\n",
    "    (new_optimizer, new_state), metrics, output = update_fn(optimizer, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "smoking-bronze",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FrozenDict({\n",
       "     vars: {\n",
       "         LeastSquares_0: {\n",
       "             mask: DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                           True,  True,  True,  True], dtype=bool),\n",
       "         },\n",
       "     },\n",
       " }),\n",
       " {'prior_init': None})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dated-exhibit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FrozenDict({\n",
       "     vars: {\n",
       "         LeastSquares_0: {\n",
       "             mask: DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                           True,  True,  True,  True], dtype=bool),\n",
       "         },\n",
       "     },\n",
       " }),\n",
       " {'prior_init': DeviceArray([3.8261806e+01, 1.2139067e+03, 1.5917789e+00, 1.3994301e+03,\n",
       "               3.5256889e-01, 1.6201604e+03, 1.0433588e+03, 1.2462245e+03,\n",
       "               8.3063586e+02, 1.3484807e+03, 8.2134668e+02, 1.3358071e+03,\n",
       "               4.2957764e+00], dtype=float32)})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-chamber",
   "metadata": {},
   "source": [
    "Now it's correct. The prior doesn't seem unnecessarily big; let's check the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adverse-south",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    MLP_0: {\n",
       "        Dense_0: {\n",
       "            bias: DeviceArray([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                         nan, nan, nan, nan, nan, nan], dtype=float32),\n",
       "            kernel: DeviceArray([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan]], dtype=float32),\n",
       "        },\n",
       "        Dense_1: {\n",
       "            bias: DeviceArray([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                         nan, nan, nan, nan, nan, nan], dtype=float32),\n",
       "            kernel: DeviceArray([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan]], dtype=float32),\n",
       "        },\n",
       "        Dense_2: {\n",
       "            bias: DeviceArray([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                         nan, nan, nan, nan, nan, nan], dtype=float32),\n",
       "            kernel: DeviceArray([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan],\n",
       "                         [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "                          nan, nan, nan, nan, nan, nan]], dtype=float32),\n",
       "        },\n",
       "        Dense_3: {\n",
       "            bias: DeviceArray([nan], dtype=float32),\n",
       "            kernel: DeviceArray([[nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan],\n",
       "                         [nan]], dtype=float32),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_optimizer.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-basement",
   "metadata": {},
   "source": [
    "Well that explains it - everything is nan - weird. How does the output look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cloudy-script",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nans: [DeviceArray(False, dtype=bool), DeviceArray(False, dtype=bool), DeviceArray(False, dtype=bool), DeviceArray(False, dtype=bool)]\n",
      "Infs: [DeviceArray(False, dtype=bool), DeviceArray(False, dtype=bool), DeviceArray(False, dtype=bool), DeviceArray(False, dtype=bool)]\n",
      "min: [DeviceArray(-0.08321948, dtype=float32), DeviceArray(-0.10772296, dtype=float32), DeviceArray(-0.8179425, dtype=float32), DeviceArray(-0.00060113, dtype=float32)]\n",
      "max: [DeviceArray(0.1909909, dtype=float32), DeviceArray(0.16929603, dtype=float32), DeviceArray(2.387917, dtype=float32), DeviceArray(1.606274, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nans: {[jnp.any(jnp.isnan(i)) for i in output]}\")\n",
    "print(f\"Infs: {[jnp.any(jnp.isinf(i)) for i in output]}\")\n",
    "print(f\"min: {[jnp.min(i) for i in output]}\")\n",
    "print(f\"max: {[jnp.max(i) for i in output]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-organization",
   "metadata": {},
   "source": [
    "That seems fine, so the nans have to be caused by the SBL backprop - let's verify by running the non-diff again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "practical-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "\n",
    "state = (state, {\"prior_init\": None})  # adding prior to state\n",
    "update_fn = create_update(loss_fn_SBL, (model, X, y, True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "requested-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jax.disable_jit():\n",
    "    (new_optimizer, new_state), metrics, output = update_fn(optimizer, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "transsexual-importance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FrozenDict({\n",
       "     vars: {\n",
       "         LeastSquares_0: {\n",
       "             mask: DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                           True,  True,  True,  True], dtype=bool),\n",
       "         },\n",
       "     },\n",
       " }),\n",
       " {'prior_init': None})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "likely-senator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(FrozenDict({\n",
       "     vars: {\n",
       "         LeastSquares_0: {\n",
       "             mask: DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                           True,  True,  True,  True], dtype=bool),\n",
       "         },\n",
       "     },\n",
       " }),\n",
       " {'prior_init': DeviceArray([3.8261806e+01, 1.2139067e+03, 1.5917789e+00, 1.3994301e+03,\n",
       "               3.5256889e-01, 1.6201604e+03, 1.0433588e+03, 1.2462245e+03,\n",
       "               8.3063586e+02, 1.3484807e+03, 8.2134668e+02, 1.3358071e+03,\n",
       "               4.2957764e+00], dtype=float32)})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-korean",
   "metadata": {},
   "source": [
    "Now it's correct. The prior doesn't seem unnecessarily big; let's check the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "coordinated-powell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    MLP_0: {\n",
       "        Dense_0: {\n",
       "            bias: DeviceArray([-0.00199999,  0.00199999,  0.002     , -0.00199999,\n",
       "                         -0.00199999,  0.00199999, -0.00199999, -0.00199999,\n",
       "                         -0.00199999,  0.00199999, -0.002     ,  0.00199999,\n",
       "                          0.002     , -0.002     , -0.00199999,  0.002     ,\n",
       "                          0.00199999,  0.00199999,  0.00199999,  0.002     ,\n",
       "                         -0.002     , -0.00199999, -0.00199999,  0.002     ,\n",
       "                          0.00199999, -0.00199999,  0.00199999,  0.00199999,\n",
       "                         -0.00199999,  0.00199999], dtype=float32),\n",
       "            kernel: DeviceArray([[-0.61155164, -1.1547233 , -0.33419877,  0.63251287,\n",
       "                           0.40870082,  0.9491058 ,  0.43981987,  0.1844259 ,\n",
       "                           0.02297306, -0.3432148 ,  0.1914811 , -0.9863748 ,\n",
       "                           0.5854419 , -0.65472823,  0.03556921, -1.392958  ,\n",
       "                           0.56658787,  0.28617826, -0.8788746 ,  0.44806272,\n",
       "                          -0.95187205, -0.77916795, -0.55202913,  0.12206326,\n",
       "                           0.92410743, -0.30023274, -0.76921934, -0.2013624 ,\n",
       "                          -0.30071196, -0.76189375],\n",
       "                         [ 0.7867578 , -0.18229297,  0.08236922, -0.74962664,\n",
       "                           0.9561876 ,  0.43199453,  0.11428934,  0.5417929 ,\n",
       "                          -1.4155974 ,  0.93846416,  0.39916366,  0.13555196,\n",
       "                          -0.01390334, -0.43225208, -0.17632873, -0.16544311,\n",
       "                          -0.9494949 , -0.33233544, -0.4183732 , -0.43419245,\n",
       "                           1.2259692 ,  0.23838176, -0.5451379 , -1.4901552 ,\n",
       "                          -0.12819542, -0.2010452 , -0.14794476, -0.37090954,\n",
       "                          -0.27141196, -0.9419416 ]], dtype=float32),\n",
       "        },\n",
       "        Dense_1: {\n",
       "            bias: DeviceArray([ 0.002     ,  0.002     ,  0.00199999,  0.002     ,\n",
       "                          0.00199999, -0.002     ,  0.002     ,  0.002     ,\n",
       "                          0.00199999,  0.00199999, -0.002     ,  0.00199999,\n",
       "                          0.00199999, -0.002     , -0.002     ,  0.002     ,\n",
       "                          0.00199999, -0.00199999,  0.00199999,  0.002     ,\n",
       "                          0.002     ,  0.00199999,  0.00199999, -0.002     ,\n",
       "                         -0.00199999,  0.00199999,  0.00199999,  0.00199999,\n",
       "                         -0.00199999,  0.002     ], dtype=float32),\n",
       "            kernel: DeviceArray([[-2.64917016e-01,  1.40024170e-01,  1.72233850e-01,\n",
       "                           4.12459165e-01,  1.93325877e-01, -6.35514110e-02,\n",
       "                           7.26488382e-02, -3.78570437e-01,  1.66854888e-01,\n",
       "                           1.79975599e-01,  3.58804837e-02, -2.28351522e-02,\n",
       "                          -7.44731426e-02, -1.45551026e-01, -2.50770360e-01,\n",
       "                           3.43430728e-01, -7.14953393e-02, -8.39632004e-02,\n",
       "                          -2.47536972e-01,  2.40308329e-01, -1.04136206e-01,\n",
       "                          -1.80376604e-01, -9.09454152e-02,  3.26713398e-02,\n",
       "                          -1.00353681e-01, -1.70465857e-01, -2.05613986e-01,\n",
       "                          -6.62238225e-02,  2.73176823e-02,  8.10887590e-02],\n",
       "                         [-9.11863986e-03,  1.84667557e-02, -4.53862436e-02,\n",
       "                          -2.85733137e-02,  7.20332004e-03,  9.15169716e-02,\n",
       "                          -1.44268543e-01, -1.62236452e-01, -1.49587560e-02,\n",
       "                           2.50803195e-02,  1.80377230e-01,  1.88912842e-02,\n",
       "                           1.00823678e-01, -1.33301646e-01,  4.24840786e-02,\n",
       "                          -2.23832339e-01, -1.06148399e-01, -1.22159153e-01,\n",
       "                          -3.01025175e-02, -3.28510940e-01, -1.08218260e-01,\n",
       "                           3.75796445e-02,  2.50834614e-01, -3.04186255e-01,\n",
       "                          -1.86980978e-01, -1.32843763e-01,  1.34693338e-02,\n",
       "                           6.80919886e-02,  1.25625610e-01, -1.13544554e-01],\n",
       "                         [ 5.08723445e-02, -9.00249705e-02,  1.11775607e-01,\n",
       "                           6.99145943e-02, -3.22502971e-01, -1.12282380e-01,\n",
       "                          -5.34070469e-03, -7.72487447e-02,  8.67809802e-02,\n",
       "                          -3.69085640e-01, -1.08457841e-01, -2.71907244e-02,\n",
       "                          -4.77940142e-02,  1.92146763e-01,  5.01881912e-02,\n",
       "                           1.65435538e-01, -7.37668350e-02,  4.15178649e-02,\n",
       "                          -2.49895304e-01, -2.30842188e-01,  6.45314232e-02,\n",
       "                           7.17240050e-02,  6.41762232e-03, -1.15739107e-01,\n",
       "                           1.28481030e-01,  2.77593844e-02,  6.91516846e-02,\n",
       "                           3.61885607e-01,  1.86692141e-02,  4.59858328e-02],\n",
       "                         [ 3.67164724e-02, -1.06417581e-01,  4.91149575e-02,\n",
       "                           6.44055158e-02, -1.32771567e-01,  2.19060510e-01,\n",
       "                           2.35456049e-01, -5.47070019e-02, -3.17691714e-01,\n",
       "                           2.86550373e-01,  3.60689372e-01, -2.21486300e-01,\n",
       "                          -2.64997065e-01,  1.29481375e-01, -1.02842897e-01,\n",
       "                           9.55973044e-02, -5.40789822e-03,  2.02896595e-01,\n",
       "                           1.95627168e-01, -2.86967814e-01,  6.75537363e-02,\n",
       "                           2.69110829e-01, -3.21564883e-01, -8.60654190e-02,\n",
       "                          -2.17477351e-01, -1.92414224e-01, -2.53066067e-02,\n",
       "                           1.86610684e-01,  1.29228398e-01, -2.79837966e-01],\n",
       "                         [-1.28407985e-01,  1.21749848e-01, -3.11180890e-01,\n",
       "                           5.13872057e-02,  7.74321109e-02, -4.68908530e-03,\n",
       "                          -2.75495678e-01,  1.69025213e-01, -8.17026868e-02,\n",
       "                          -8.22406262e-02, -8.74591842e-02, -2.80299842e-01,\n",
       "                          -1.77243546e-01, -1.45030007e-01,  3.87730077e-02,\n",
       "                          -2.59491384e-01, -9.19158757e-02, -2.97954679e-01,\n",
       "                           1.18027717e-01,  6.83153644e-02,  1.34637458e-02,\n",
       "                          -2.57405579e-01, -1.03143483e-01, -2.26544425e-01,\n",
       "                          -2.00716987e-01, -2.43898053e-02, -9.02053639e-02,\n",
       "                          -9.90915820e-02,  9.33100283e-02, -9.80421230e-02],\n",
       "                         [-2.46719886e-02,  2.06534788e-02,  4.53218296e-02,\n",
       "                          -1.90471023e-01, -3.67074870e-02,  8.50088149e-02,\n",
       "                           7.38654509e-02, -5.25991581e-02, -2.99584299e-01,\n",
       "                          -1.52215958e-01, -2.77345806e-01,  2.34519914e-01,\n",
       "                          -1.19304188e-01,  1.50627077e-01,  2.81765223e-01,\n",
       "                          -2.16840148e-01, -4.47541289e-02,  6.36012703e-02,\n",
       "                           2.62424797e-01,  1.76933005e-01, -2.86507875e-01,\n",
       "                           4.10635680e-01, -7.93560073e-02, -2.90452391e-01,\n",
       "                          -2.28604466e-01, -1.28277481e-01,  2.97204852e-01,\n",
       "                           2.36419916e-01,  5.78716993e-02, -1.00834839e-01],\n",
       "                         [ 1.05864674e-01,  5.08238561e-03, -1.86533611e-02,\n",
       "                           2.43360266e-01,  1.23732589e-01,  3.91215645e-03,\n",
       "                           3.84451151e-02, -4.71011549e-02, -2.52231471e-02,\n",
       "                           6.43908530e-02,  2.06391394e-01, -5.95822884e-03,\n",
       "                          -1.29591018e-01, -9.42563936e-02, -2.01195717e-01,\n",
       "                          -3.13784152e-01,  4.07809652e-02, -2.33860388e-01,\n",
       "                          -1.30026430e-01, -4.92973030e-02,  5.23902476e-02,\n",
       "                           6.54831678e-02, -2.75778115e-01, -3.45572531e-01,\n",
       "                          -3.03571373e-01,  2.32122660e-01, -2.65608907e-01,\n",
       "                          -1.62431166e-01, -1.19233973e-01, -1.22257441e-01],\n",
       "                         [-1.27108693e-02,  1.72062352e-01, -1.66942507e-01,\n",
       "                           1.39655769e-01, -3.89037102e-01,  7.57914856e-02,\n",
       "                          -4.04327735e-03, -6.54629543e-02, -1.57182559e-01,\n",
       "                          -1.33408561e-01,  5.46214916e-03, -1.42251030e-01,\n",
       "                           2.35340861e-03, -2.91856509e-02, -1.72088459e-01,\n",
       "                           1.67391673e-02, -4.52906638e-03,  8.50535408e-02,\n",
       "                          -2.76344329e-01,  7.92854577e-02, -1.21248551e-01,\n",
       "                           1.10749543e-01,  3.41876179e-01,  1.06906973e-01,\n",
       "                           2.34372005e-01,  5.67195788e-02,  1.32733434e-01,\n",
       "                          -5.09251766e-02,  3.43160421e-01, -3.75157923e-01],\n",
       "                         [ 1.24213360e-01, -1.16928473e-01, -1.83965400e-01,\n",
       "                           2.32889965e-01,  3.82282674e-01,  2.58175824e-02,\n",
       "                          -7.28048012e-02,  4.76814546e-02, -1.85925066e-01,\n",
       "                           1.39952525e-01, -3.40679809e-02, -1.65797576e-01,\n",
       "                          -2.03527212e-02,  7.71297980e-03,  2.06071157e-02,\n",
       "                          -5.63965105e-02,  1.08599663e-01, -3.85156423e-01,\n",
       "                          -1.72506906e-02, -2.03075215e-01, -1.44575462e-01,\n",
       "                           2.00331226e-01, -2.68198788e-01, -1.63036570e-01,\n",
       "                          -1.27445400e-01, -1.93630740e-01, -1.19929891e-02,\n",
       "                          -1.26434445e-01,  1.45010322e-01, -1.02745242e-01],\n",
       "                         [ 5.91915362e-02,  7.98109472e-02,  1.52121052e-01,\n",
       "                          -8.86435062e-02, -1.29134193e-01,  3.25790644e-02,\n",
       "                          -2.20217481e-01, -8.99898708e-02,  3.00541669e-01,\n",
       "                           2.44254678e-01,  2.94549495e-01,  8.29950497e-02,\n",
       "                           2.12081060e-01, -4.36870232e-02,  1.49904564e-01,\n",
       "                           1.27795666e-01, -4.26638462e-02, -1.91224173e-01,\n",
       "                          -1.09901264e-01,  1.92963034e-01,  4.53846119e-02,\n",
       "                           3.44547391e-01, -3.01243395e-01, -1.55665115e-01,\n",
       "                           1.91340670e-01, -8.40926841e-02,  2.40617141e-01,\n",
       "                           2.10897207e-01, -1.06964767e-01,  9.83640105e-02],\n",
       "                         [ 8.61535966e-02,  1.25780642e-01, -3.10003430e-01,\n",
       "                           9.52081531e-02,  3.47552508e-01,  1.66142538e-01,\n",
       "                           4.46591079e-02, -5.28244749e-02, -1.13443986e-01,\n",
       "                           1.10812292e-01,  1.10036068e-01, -1.30549893e-01,\n",
       "                          -1.26497492e-01,  2.36596406e-01,  7.03988448e-02,\n",
       "                           2.01223314e-01,  3.10098790e-02,  1.86984837e-01,\n",
       "                          -2.30689645e-01,  3.08927335e-02,  1.53885279e-02,\n",
       "                           2.21809223e-01,  3.49988788e-02,  3.35866958e-01,\n",
       "                          -1.27618447e-01,  1.87010780e-01, -5.26259420e-03,\n",
       "                           1.86842456e-01,  1.56126678e-01,  3.66102299e-03],\n",
       "                         [-2.70090908e-01,  1.70345470e-01, -2.39121825e-01,\n",
       "                          -1.78179041e-01, -1.21178217e-01, -2.36829534e-01,\n",
       "                          -8.17438662e-02,  9.23526287e-02,  3.20037574e-01,\n",
       "                          -1.09520487e-01, -8.11996013e-02,  8.47876295e-02,\n",
       "                          -1.26637861e-01, -3.84152502e-01,  2.71945328e-01,\n",
       "                           2.77071863e-01, -1.45060569e-01, -2.08945066e-01,\n",
       "                          -3.90004851e-02, -4.29366529e-02,  9.57604796e-02,\n",
       "                          -2.79601455e-01, -9.89647135e-02, -1.42040893e-01,\n",
       "                          -2.79361218e-01,  1.77806333e-01, -1.00833796e-01,\n",
       "                          -4.84459549e-02, -4.01911378e-01, -4.71933298e-02],\n",
       "                         [-2.80549228e-01,  3.92688811e-01, -2.84127831e-01,\n",
       "                          -5.62000424e-02, -9.55631025e-03, -1.55775189e-01,\n",
       "                           5.30254804e-02,  2.69370407e-01,  1.71231721e-02,\n",
       "                          -7.55588189e-02,  3.83504927e-02,  2.70565093e-01,\n",
       "                           1.25197202e-01, -8.35884139e-02,  1.80847108e-01,\n",
       "                           6.42493144e-02,  2.23601356e-01,  2.02543531e-02,\n",
       "                           4.05139118e-01,  3.14730555e-01,  4.09091860e-02,\n",
       "                          -1.55223325e-01, -2.26571575e-01, -3.06225479e-01,\n",
       "                           1.92001853e-02,  1.88133076e-01,  1.72679245e-01,\n",
       "                           2.54066020e-01,  3.01456992e-02, -2.39233807e-01],\n",
       "                         [-1.38142835e-02,  2.02823002e-02, -3.05592418e-01,\n",
       "                          -1.51518136e-01, -1.00309707e-01,  7.15351403e-02,\n",
       "                           9.70364138e-02,  2.45984107e-01, -3.02906603e-01,\n",
       "                           2.33705580e-01,  2.00495020e-01, -3.42612118e-02,\n",
       "                           4.68397029e-02, -1.45294797e-02,  6.59729242e-02,\n",
       "                          -4.09176126e-02,  1.03006147e-01, -2.24781170e-01,\n",
       "                           1.03383712e-01, -3.84416804e-02, -3.66610348e-01,\n",
       "                          -2.34503627e-01, -1.85707673e-01, -1.32328138e-01,\n",
       "                          -1.08339310e-01, -3.54284644e-02,  6.86629415e-02,\n",
       "                           2.70559862e-02, -1.69701323e-01,  1.78032685e-02],\n",
       "                         [ 2.87144482e-01,  2.04622954e-01, -2.02259764e-01,\n",
       "                           2.47743949e-01,  1.12127550e-01,  2.21722856e-01,\n",
       "                          -6.55159727e-02, -2.13762403e-01,  2.15996224e-02,\n",
       "                          -1.54411392e-02,  8.45778286e-02, -2.71661878e-01,\n",
       "                          -1.94371119e-01,  2.47344971e-01, -1.86376683e-02,\n",
       "                          -3.27985823e-01, -9.19533744e-02, -1.49537891e-01,\n",
       "                           4.79295067e-02, -2.32530739e-02, -3.70145887e-01,\n",
       "                           1.31343126e-01, -3.95400882e-01, -3.23632844e-02,\n",
       "                           9.90816429e-02, -7.95678049e-02, -4.26455364e-02,\n",
       "                          -1.39772534e-01,  3.31685394e-01,  1.15165442e-01],\n",
       "                         [ 3.36042702e-01, -2.62993183e-02, -1.84545770e-01,\n",
       "                          -1.72670558e-01,  6.73746765e-02,  2.14373752e-01,\n",
       "                           6.88024908e-02,  1.42382637e-01,  6.33210689e-02,\n",
       "                          -2.96192542e-02,  1.79288238e-01,  4.79100682e-02,\n",
       "                          -2.38160044e-01,  3.82111937e-01,  1.65601328e-01,\n",
       "                           1.11992873e-01,  5.55903502e-02,  2.27543905e-01,\n",
       "                          -1.05658829e-01,  1.28157243e-01,  7.98927695e-02,\n",
       "                           2.41815552e-01,  9.71664023e-03,  3.71117629e-02,\n",
       "                           3.01033892e-02,  8.35052580e-02,  3.45327944e-01,\n",
       "                          -1.32225573e-01, -2.37511292e-01, -1.33294314e-01],\n",
       "                         [-3.45039628e-02,  1.84970587e-01, -3.12166899e-01,\n",
       "                           1.57819360e-01, -2.76691969e-02, -5.92544861e-02,\n",
       "                           1.33035257e-01,  7.96007738e-02,  1.64651424e-01,\n",
       "                           5.85409068e-02, -3.08952630e-01, -3.02855782e-02,\n",
       "                          -2.42635775e-02, -1.54658720e-01,  1.19100921e-01,\n",
       "                           3.38787958e-02,  3.11370432e-01, -4.86065336e-02,\n",
       "                          -1.00144573e-01, -1.87452078e-01, -3.96195829e-01,\n",
       "                           1.12493910e-01, -7.83616975e-02,  3.24241489e-01,\n",
       "                          -2.09595904e-01,  2.90013283e-01,  8.82767513e-02,\n",
       "                          -1.45511687e-01, -3.29794616e-01, -2.44895026e-01],\n",
       "                         [ 2.60509998e-01,  3.61798815e-02, -2.13239357e-01,\n",
       "                           3.40713203e-01, -1.56706885e-01,  1.29643574e-01,\n",
       "                           1.05436720e-01,  1.00720823e-01,  3.92472208e-01,\n",
       "                          -6.71915859e-02,  2.82050818e-01, -9.79651511e-02,\n",
       "                           3.45928930e-02, -5.82870692e-02, -1.95703328e-01,\n",
       "                           3.33325356e-01, -1.68482780e-01, -2.89193571e-01,\n",
       "                          -7.93412700e-03,  1.33465692e-01, -2.58345336e-01,\n",
       "                           1.49886847e-01, -5.67486361e-02, -2.93599099e-01,\n",
       "                           6.84959963e-02, -2.77851313e-01,  2.33274862e-01,\n",
       "                           1.02973230e-01, -1.22836158e-01, -1.28320411e-01],\n",
       "                         [ 7.56847253e-03,  4.23599966e-03, -1.09436244e-01,\n",
       "                           3.30277443e-01, -1.97729349e-01, -1.05996348e-01,\n",
       "                           7.76706217e-03,  2.15629816e-01,  1.39886998e-02,\n",
       "                           3.87497549e-03,  6.68184683e-02,  1.78357270e-02,\n",
       "                          -2.23380238e-01, -8.14323351e-02, -1.79087192e-01,\n",
       "                           1.08863808e-01,  1.13426410e-01, -2.18652532e-01,\n",
       "                          -1.05164722e-01,  3.09564173e-01, -3.47460181e-01,\n",
       "                           1.59085467e-01, -1.14287242e-01, -5.03218099e-02,\n",
       "                           1.67707011e-01, -3.17928568e-02, -4.82327975e-02,\n",
       "                          -3.77994105e-02, -4.47515175e-02,  1.33482948e-01],\n",
       "                         [ 1.81251287e-01, -2.31891438e-01, -2.58534580e-01,\n",
       "                          -1.03230968e-01,  1.46111548e-01,  1.61479563e-01,\n",
       "                          -1.04995154e-01, -2.70593435e-01,  3.27374935e-01,\n",
       "                          -3.57761085e-02, -2.48693004e-01, -2.13351510e-02,\n",
       "                          -2.44196460e-01,  8.29126760e-02,  1.02629103e-01,\n",
       "                          -1.68042362e-01, -2.76304752e-01,  7.60510787e-02,\n",
       "                          -3.06492835e-01, -1.40360501e-02,  1.00315124e-01,\n",
       "                           1.13597766e-01,  2.67202202e-02, -1.58791184e-01,\n",
       "                          -3.29897463e-01,  5.37196584e-02,  2.10447744e-01,\n",
       "                           2.38402430e-02, -1.58976361e-01,  1.40483156e-01],\n",
       "                         [ 7.11295530e-02,  1.00084342e-01, -2.39434764e-01,\n",
       "                           5.84282093e-02, -1.53413080e-02,  6.03382709e-03,\n",
       "                           4.88975011e-02, -9.41727236e-02,  3.09531651e-02,\n",
       "                          -2.88955867e-01,  4.78384718e-02, -1.19066976e-01,\n",
       "                           2.30850875e-02, -3.69355053e-01, -6.18175566e-02,\n",
       "                           1.98603898e-01, -3.52295756e-01, -2.02984378e-01,\n",
       "                           4.47772592e-02, -1.78036377e-01, -2.85214046e-04,\n",
       "                          -9.10251886e-02, -1.00766174e-01, -1.21463701e-01,\n",
       "                           7.17414096e-02,  9.57378596e-02, -1.22320749e-01,\n",
       "                           2.63427764e-01,  2.76488930e-01, -1.55612841e-01],\n",
       "                         [-1.42240554e-01, -1.42137855e-02, -8.04738551e-02,\n",
       "                           4.92979176e-02,  8.60543624e-02,  2.32972465e-02,\n",
       "                           3.93758863e-01, -2.09286362e-01,  3.11708391e-01,\n",
       "                          -3.36518511e-02, -6.70158491e-02, -3.06509584e-01,\n",
       "                           1.11689024e-01,  1.71058387e-01,  2.64188796e-01,\n",
       "                          -2.70589978e-01, -1.45165101e-01,  3.12621355e-01,\n",
       "                           1.00182720e-01,  1.74580202e-01,  2.24736512e-01,\n",
       "                           2.03622282e-01, -3.40768725e-01,  2.32347965e-01,\n",
       "                          -3.66340369e-01,  2.72418499e-01, -1.05454929e-01,\n",
       "                           3.94459456e-01,  2.97998607e-01, -1.05990641e-01],\n",
       "                         [ 1.58804029e-01, -1.49433434e-01,  8.38021562e-03,\n",
       "                          -1.20710522e-01,  1.78018324e-02,  4.13678288e-01,\n",
       "                           7.77646303e-02,  1.45706549e-01, -1.53797165e-01,\n",
       "                          -1.45452157e-01,  1.67296410e-01,  1.32955104e-01,\n",
       "                           2.59682070e-02,  7.27114454e-02, -6.69568256e-02,\n",
       "                          -2.15889797e-01,  2.79828578e-01,  5.40830083e-02,\n",
       "                           5.81687912e-02, -2.56373614e-01,  4.57306113e-03,\n",
       "                          -2.07848996e-01, -1.22045033e-01, -2.33721867e-01,\n",
       "                           3.03557068e-01,  2.28358358e-01, -7.30365589e-02,\n",
       "                          -1.49078831e-01,  3.84734571e-01,  5.44947945e-02],\n",
       "                         [ 9.40346047e-02, -1.71815619e-01,  2.07607061e-01,\n",
       "                          -1.14156105e-01,  1.43315285e-01, -9.04071257e-02,\n",
       "                           3.80234361e-01, -2.73682065e-02,  3.86393517e-01,\n",
       "                           1.47250012e-01,  8.92506167e-02,  4.02126275e-02,\n",
       "                           2.80240357e-01,  1.62274003e-01,  1.49719819e-01,\n",
       "                          -1.55759916e-01,  1.70627505e-01, -4.53055650e-02,\n",
       "                           2.13585645e-02, -4.54562008e-02, -2.73525845e-02,\n",
       "                          -4.71638590e-02, -2.08529443e-01,  3.78917128e-01,\n",
       "                           8.28537196e-02,  2.26486906e-01,  3.10293529e-02,\n",
       "                           2.36116856e-01,  7.44190738e-02,  2.89629787e-01],\n",
       "                         [-3.33821848e-02, -1.91135243e-01, -1.23851886e-02,\n",
       "                           5.58807328e-02,  1.39079675e-01, -1.17040575e-01,\n",
       "                           2.42577329e-01, -7.44355097e-02,  6.33281916e-02,\n",
       "                          -8.75743926e-02, -2.20641773e-03,  7.00709820e-02,\n",
       "                           1.18626118e-01,  1.88050702e-01,  4.94274385e-02,\n",
       "                          -2.52747983e-01, -5.15539832e-02, -1.50423869e-01,\n",
       "                           1.67322345e-02, -3.75827312e-01,  1.93158597e-01,\n",
       "                           2.87290752e-01, -8.49111751e-02,  5.54282032e-02,\n",
       "                           8.98367539e-02,  9.14233085e-03, -1.80978894e-01,\n",
       "                           6.95664287e-02, -3.37120444e-01,  1.20804302e-01],\n",
       "                         [-1.03412755e-01,  8.02986845e-02, -4.27524820e-02,\n",
       "                          -2.06666030e-02,  3.06751132e-01,  2.03407988e-01,\n",
       "                           3.63121033e-02,  1.04408162e-02,  1.69581845e-01,\n",
       "                           1.34926662e-01,  5.91345914e-02,  1.79584935e-01,\n",
       "                          -1.37130106e-02,  1.20530799e-02,  1.97860405e-01,\n",
       "                           8.78791139e-02, -1.56548217e-01,  1.41202196e-01,\n",
       "                           5.77338263e-02,  1.31534189e-01,  1.02954805e-01,\n",
       "                          -8.86739194e-02, -7.44014084e-02, -9.97205600e-02,\n",
       "                           9.26235691e-02,  5.24972454e-02, -7.15969950e-02,\n",
       "                           1.04314111e-01,  1.11532852e-01,  1.26644135e-01],\n",
       "                         [-2.32921205e-02, -3.53416175e-01, -1.63810045e-01,\n",
       "                           1.98305145e-01, -9.08250958e-02,  1.49726450e-01,\n",
       "                          -1.91237837e-01, -1.55185983e-01,  1.74282521e-01,\n",
       "                           1.24743663e-01, -9.01642144e-02,  7.48873204e-02,\n",
       "                           8.08551908e-03,  6.38105795e-02, -1.73551440e-02,\n",
       "                           4.96030115e-02,  2.51054853e-01,  1.66912302e-01,\n",
       "                          -1.05140336e-01,  1.17753223e-01,  3.42140198e-02,\n",
       "                          -4.36161123e-02,  1.62278563e-01,  3.63798082e-01,\n",
       "                          -9.88119915e-02,  2.18017995e-02,  9.66420099e-02,\n",
       "                           2.30159342e-01,  1.13250300e-01,  5.70150279e-02],\n",
       "                         [-3.40866446e-01,  6.54983595e-02, -5.70208319e-02,\n",
       "                           1.23805158e-01, -1.78631753e-01, -1.35468589e-02,\n",
       "                          -4.18860018e-02,  2.03703925e-01,  1.56593341e-02,\n",
       "                           2.79136628e-01, -4.02903594e-02,  7.05345627e-03,\n",
       "                           2.41708323e-01,  1.00755163e-01,  1.35937646e-01,\n",
       "                           8.64479765e-02, -2.78802663e-02, -3.86815757e-01,\n",
       "                          -4.46702875e-02, -2.01381311e-01, -1.06237084e-01,\n",
       "                           1.39146611e-01,  6.29345998e-02, -3.79547358e-01,\n",
       "                          -1.63788885e-01, -6.41267672e-02,  1.41801924e-01,\n",
       "                          -1.19157925e-01,  2.79725194e-01,  1.13871351e-01],\n",
       "                         [ 1.76516864e-02, -3.33693296e-01, -3.35641176e-01,\n",
       "                          -7.67634138e-02, -1.62231326e-01,  2.46334448e-01,\n",
       "                          -2.55504996e-01,  1.03877075e-01, -6.97222501e-02,\n",
       "                          -2.00359866e-01, -1.18326366e-01,  2.51195222e-01,\n",
       "                          -4.91292030e-02,  2.74524033e-01, -1.58022866e-02,\n",
       "                          -1.71934947e-01, -9.41742491e-03,  1.82176545e-01,\n",
       "                          -3.76342803e-01, -5.46439551e-02,  1.58079803e-01,\n",
       "                          -1.55869439e-01,  9.89634395e-02,  3.39904517e-01,\n",
       "                          -2.52245307e-01,  9.07246321e-02, -8.80430564e-02,\n",
       "                          -2.49098390e-01,  3.03469896e-02, -4.65149246e-02],\n",
       "                         [ 3.58892232e-02,  1.29275188e-01, -4.53071669e-02,\n",
       "                           9.18866470e-02, -2.59267151e-01, -8.20449293e-02,\n",
       "                          -2.45081097e-01,  1.49893150e-01,  8.51572379e-02,\n",
       "                           2.92024285e-01, -1.33497687e-03, -2.96975672e-01,\n",
       "                           5.51314130e-02, -2.43302360e-01, -6.98184296e-02,\n",
       "                          -1.76972762e-01,  2.01576054e-01, -4.70177494e-02,\n",
       "                           2.10594520e-01, -2.41795033e-01,  1.24020271e-01,\n",
       "                           2.37225413e-01,  1.47412673e-01, -2.75587253e-02,\n",
       "                          -6.46517053e-02,  5.12159131e-02, -1.19852819e-01,\n",
       "                          -6.07495382e-03,  1.14693135e-01,  3.76260653e-02]],            dtype=float32),\n",
       "        },\n",
       "        Dense_2: {\n",
       "            bias: DeviceArray([-0.00199999, -0.00199999, -0.00199999,  0.00199999,\n",
       "                         -0.00199999, -0.00199999, -0.00199999,  0.00199999,\n",
       "                          0.002     , -0.002     , -0.00199999, -0.00199999,\n",
       "                          0.00199999, -0.00199999, -0.00199999, -0.00199999,\n",
       "                          0.002     , -0.00199999, -0.002     , -0.002     ,\n",
       "                          0.00199999, -0.00199999, -0.002     ,  0.00199999,\n",
       "                          0.00199999, -0.00199999, -0.002     , -0.002     ,\n",
       "                         -0.00199999, -0.00199999], dtype=float32),\n",
       "            kernel: DeviceArray([[ 1.92807727e-02,  3.42704684e-01, -6.20251298e-02,\n",
       "                           5.03625646e-02, -5.30796032e-03, -6.05095783e-03,\n",
       "                           6.96085095e-02,  1.33113638e-01,  3.72927427e-01,\n",
       "                           1.50752574e-01, -1.28873676e-01, -1.29234895e-01,\n",
       "                          -7.10460469e-02, -3.74451369e-01, -3.19324970e-01,\n",
       "                          -2.18583599e-01, -2.81523317e-01,  1.09571218e-01,\n",
       "                          -1.38069153e-01, -4.56466042e-02,  9.29329917e-03,\n",
       "                           1.64437011e-01, -1.91872623e-02,  6.51671514e-02,\n",
       "                          -4.52002771e-02,  4.27540019e-02,  9.34065729e-02,\n",
       "                          -9.51725394e-02, -3.17016728e-02,  3.25490803e-01],\n",
       "                         [ 4.53674346e-02,  3.48971993e-01, -2.51756050e-02,\n",
       "                           1.40681997e-01, -1.15199402e-01, -1.43166054e-02,\n",
       "                           1.59532830e-01,  3.29221755e-01,  6.04040585e-02,\n",
       "                           6.07860759e-02, -1.00375727e-01, -9.40214191e-03,\n",
       "                          -3.59538198e-01, -6.17045490e-03, -3.45070101e-02,\n",
       "                          -2.64592648e-01,  1.50009189e-02,  2.59183258e-01,\n",
       "                           6.40441710e-03, -2.14920461e-01,  2.16064706e-01,\n",
       "                          -1.52877405e-01, -3.76677126e-01,  9.64892805e-02,\n",
       "                           7.91823566e-02, -2.06178010e-01, -3.06123614e-01,\n",
       "                          -6.73722103e-02, -5.99947236e-02,  1.94439456e-01],\n",
       "                         [ 1.05434023e-01,  1.72615260e-01,  5.76317646e-02,\n",
       "                          -1.43960074e-01, -1.37216270e-01,  2.60303974e-01,\n",
       "                           3.27060610e-01,  1.57948554e-01,  1.04562901e-01,\n",
       "                          -1.45855650e-01,  2.58475095e-01, -2.63336331e-01,\n",
       "                           7.11994767e-02, -3.66345420e-02, -3.94799918e-01,\n",
       "                           1.05738312e-01, -2.34388545e-01, -8.13396797e-02,\n",
       "                          -1.80248767e-01,  1.28472522e-01, -1.42973945e-01,\n",
       "                          -1.58691138e-01, -3.22700739e-01, -2.48423629e-02,\n",
       "                          -9.11508203e-02,  6.94501773e-03, -2.33484685e-01,\n",
       "                           1.56052746e-02,  6.91624358e-02, -1.55897498e-01],\n",
       "                         [ 6.71525449e-02,  1.92797497e-01, -1.73739828e-02,\n",
       "                           2.88763702e-01, -3.04511756e-01, -1.44890651e-01,\n",
       "                          -6.68010768e-03, -1.43589899e-01,  1.22887582e-01,\n",
       "                           3.16322953e-01, -2.22347021e-01, -2.92046554e-02,\n",
       "                          -2.13917166e-01, -7.11810812e-02, -5.76640479e-02,\n",
       "                           1.69869661e-01,  9.74094048e-02, -2.32646260e-02,\n",
       "                          -8.03837031e-02,  1.49145722e-01,  2.55610086e-02,\n",
       "                          -1.33449957e-01, -2.79482514e-01,  1.91037312e-01,\n",
       "                          -2.75955677e-01, -2.93270528e-01, -2.46248737e-01,\n",
       "                          -2.98247695e-01, -2.13931367e-01, -2.10521892e-01],\n",
       "                         [ 1.79689139e-01, -1.32144839e-01,  3.96524295e-02,\n",
       "                           1.37860209e-01,  1.01204731e-01,  1.01285450e-01,\n",
       "                           3.38240504e-01, -9.76918563e-02, -1.30734399e-01,\n",
       "                           3.86371732e-01, -4.12988871e-01,  2.95840595e-02,\n",
       "                           4.68400456e-02, -1.68599188e-01, -8.15812349e-02,\n",
       "                           1.00913998e-02, -1.38434052e-01, -1.95077136e-02,\n",
       "                           1.33852988e-01, -2.09107071e-01,  1.21142544e-01,\n",
       "                          -3.88640046e-01,  1.46388188e-01,  2.39104211e-01,\n",
       "                           1.13495931e-01, -2.67366588e-01, -2.53811747e-01,\n",
       "                          -6.11680970e-02,  9.08868387e-02, -2.35301815e-02],\n",
       "                         [ 5.57477772e-02, -3.44564728e-02,  2.44624764e-01,\n",
       "                          -2.91560322e-01,  4.22800099e-03,  8.15637708e-02,\n",
       "                          -1.73548430e-01, -2.84227103e-01, -7.95566291e-02,\n",
       "                           3.90900038e-02,  3.25320810e-02, -1.99943483e-01,\n",
       "                          -3.25151473e-01,  2.64005095e-01, -5.78692704e-02,\n",
       "                          -8.37260857e-02,  1.76808089e-01,  1.74490407e-01,\n",
       "                          -2.16514692e-01, -1.31095797e-01, -2.18752161e-01,\n",
       "                           4.15048242e-01, -1.06172539e-01,  1.96842961e-02,\n",
       "                          -1.30857766e-01,  2.78516442e-01,  1.17482938e-01,\n",
       "                           1.21298678e-01, -1.28092557e-01,  8.59911516e-02],\n",
       "                         [ 5.88549450e-02,  1.89080946e-02, -2.04289824e-01,\n",
       "                          -1.53554782e-01,  1.21787917e-02,  1.33166574e-02,\n",
       "                          -2.14839444e-01,  2.29777992e-01,  2.56742351e-06,\n",
       "                          -1.28197387e-01,  1.73675753e-02, -8.63402244e-03,\n",
       "                          -4.68340442e-02, -9.05633867e-02,  2.87460769e-03,\n",
       "                           1.49381325e-01, -2.28584662e-01,  1.34732813e-01,\n",
       "                           2.93999482e-02,  1.77039262e-02,  1.54075637e-01,\n",
       "                          -2.22208321e-01, -4.13658097e-02, -6.94824308e-02,\n",
       "                           9.93238837e-02, -1.29900783e-01, -1.75801530e-01,\n",
       "                           2.25252043e-02,  3.21077891e-02, -1.04245335e-01],\n",
       "                         [-8.21700990e-02, -2.57053703e-01, -2.15964392e-01,\n",
       "                          -1.68890432e-01, -1.82056367e-01,  3.15096322e-03,\n",
       "                          -5.00794090e-02, -1.63046643e-01,  7.38370791e-02,\n",
       "                          -1.18223466e-01, -3.29862349e-02,  4.34495844e-02,\n",
       "                           1.54557839e-01, -1.39979228e-01,  4.45820205e-02,\n",
       "                           3.73238295e-01,  3.49733472e-01, -8.10002610e-02,\n",
       "                          -7.77942836e-02, -1.52750432e-01, -2.18425527e-01,\n",
       "                          -1.78019479e-01,  1.01089165e-01, -1.18613765e-01,\n",
       "                          -2.60458678e-01, -2.28830799e-01, -5.49714938e-02,\n",
       "                           2.11852074e-01, -1.47661909e-01, -8.72732699e-02],\n",
       "                         [ 1.47010788e-01,  1.70068219e-01,  8.65664682e-04,\n",
       "                           3.37406307e-01, -2.24256694e-01,  1.51356727e-01,\n",
       "                           7.64770955e-02, -2.29575232e-01,  2.74722040e-01,\n",
       "                          -1.35044515e-01, -2.91110098e-01,  2.08422840e-01,\n",
       "                          -2.41743904e-02,  1.27164081e-01,  2.31204350e-02,\n",
       "                           1.60463735e-01,  5.07847108e-02, -6.11632839e-02,\n",
       "                          -2.22946741e-02,  5.83485402e-02,  3.77713382e-01,\n",
       "                           2.07743391e-01,  1.03113025e-01,  1.88403979e-01,\n",
       "                          -3.20321210e-02, -1.67665124e-01, -5.37300967e-02,\n",
       "                           2.23349649e-02,  1.32575417e-02,  1.83829479e-02],\n",
       "                         [-2.92703897e-01,  7.84801096e-02,  2.27904797e-01,\n",
       "                          -1.79417729e-02,  3.13083380e-01, -1.91605598e-01,\n",
       "                           1.07867502e-01, -2.59146184e-01, -1.92540139e-01,\n",
       "                           8.11558515e-02, -2.07833111e-01,  4.18736488e-02,\n",
       "                           3.27020809e-02,  3.18856955e-01, -2.28297319e-02,\n",
       "                          -9.33052376e-02,  1.20958574e-01,  8.12832639e-02,\n",
       "                          -6.34620562e-02, -5.13224751e-02,  3.25828552e-01,\n",
       "                          -1.61094308e-01,  1.38889939e-01,  2.49491289e-01,\n",
       "                           1.10875450e-01,  1.14413574e-02, -3.96475233e-02,\n",
       "                           1.35658041e-01,  2.31149465e-01,  6.62398338e-02],\n",
       "                         [-2.72263080e-01, -3.27500105e-01, -3.71802479e-01,\n",
       "                          -1.37001649e-01,  1.24845784e-02, -1.50645509e-01,\n",
       "                           2.29115978e-01, -4.07699496e-02,  4.96296249e-02,\n",
       "                           1.42217100e-01,  2.71281242e-01,  1.40385211e-01,\n",
       "                           6.91997334e-02,  1.87853843e-01,  8.86212140e-02,\n",
       "                          -2.07043961e-01,  2.35011652e-02,  2.79407024e-01,\n",
       "                           2.35711336e-02,  5.96446507e-02,  3.28468651e-01,\n",
       "                           3.32715325e-02,  1.17960729e-01,  2.87295491e-01,\n",
       "                          -5.33404462e-02,  1.15120418e-01, -1.43638596e-01,\n",
       "                          -1.56714886e-01,  2.51672536e-01,  1.45209029e-01],\n",
       "                         [-8.16138536e-02, -4.07223195e-01,  1.94539800e-01,\n",
       "                          -1.65527537e-01,  1.18443869e-01,  2.19569132e-02,\n",
       "                          -1.94577530e-01, -1.18698023e-01, -2.06555873e-01,\n",
       "                          -2.25570425e-01, -1.39317065e-01, -2.69654006e-01,\n",
       "                          -2.85487145e-01, -1.97857022e-02,  4.36974410e-03,\n",
       "                           2.14362875e-01,  2.89076447e-01, -1.04888581e-01,\n",
       "                          -2.36138225e-01, -2.91314006e-01, -1.09547086e-01,\n",
       "                           1.42760515e-01, -1.29375309e-01, -5.81131876e-03,\n",
       "                          -3.88060629e-01, -6.42637461e-02,  8.88045318e-03,\n",
       "                          -3.70448440e-01, -2.39257142e-01, -6.78407699e-02],\n",
       "                         [ 8.91144946e-02, -2.66694009e-01, -4.90316488e-02,\n",
       "                          -3.92412364e-01,  1.89842835e-01, -1.72312170e-01,\n",
       "                           1.53837223e-02,  3.15152496e-01,  5.82221411e-02,\n",
       "                          -8.07905346e-02, -5.50577678e-02,  1.85981974e-01,\n",
       "                           2.93813199e-01, -2.28349909e-01,  1.50871024e-01,\n",
       "                          -7.36842528e-02, -5.81977330e-02, -4.54266109e-02,\n",
       "                          -8.58840495e-02,  1.08033977e-01, -6.28304780e-02,\n",
       "                          -2.77267635e-01,  7.21222386e-02,  1.85749978e-01,\n",
       "                           1.50736406e-01, -2.40594178e-01,  1.01397559e-01,\n",
       "                          -2.98631608e-01,  3.41037149e-03,  5.95627446e-03],\n",
       "                         [ 2.28563830e-01,  3.00227135e-01,  5.93819655e-04,\n",
       "                          -1.63809016e-01, -6.46532560e-03,  4.31765504e-02,\n",
       "                          -1.42691523e-01,  1.78581253e-01,  1.50843590e-01,\n",
       "                          -2.21636340e-01,  6.16961680e-02,  1.27390295e-01,\n",
       "                           4.00961675e-02,  4.10123467e-01, -7.55444467e-02,\n",
       "                           1.84550092e-01,  1.38322875e-01,  4.56340611e-02,\n",
       "                          -5.27892150e-02,  1.65305272e-01, -5.31064570e-02,\n",
       "                          -3.47005755e-01,  9.55335721e-02, -2.90889144e-01,\n",
       "                           1.93398297e-01,  1.33005843e-01, -8.67054760e-02,\n",
       "                           8.82324874e-02,  3.10317636e-01, -1.41409576e-01],\n",
       "                         [-7.51401186e-02, -6.45995885e-02, -5.15386723e-02,\n",
       "                           3.55872035e-01,  6.72996975e-03,  1.19711578e-01,\n",
       "                          -2.89239511e-02, -1.16841741e-01,  2.53696620e-01,\n",
       "                           2.17369616e-01,  1.62205249e-02,  3.56446564e-01,\n",
       "                          -3.90140891e-01,  5.47949225e-02, -3.53627801e-01,\n",
       "                          -2.88669258e-01, -2.73787200e-01, -2.31345773e-01,\n",
       "                           3.84478182e-01, -3.65688987e-02, -1.68107316e-01,\n",
       "                          -2.61042893e-01,  4.22119200e-02,  1.45300314e-01,\n",
       "                           2.42103770e-01,  7.49241188e-02,  1.70578256e-01,\n",
       "                          -5.16334400e-02,  1.18826039e-01, -1.00614972e-01],\n",
       "                         [-1.05212621e-01,  3.90395015e-01, -5.85346110e-02,\n",
       "                           1.00951530e-01,  1.50905088e-01,  2.58105010e-01,\n",
       "                           9.35508311e-03,  1.39323831e-01,  8.95347148e-02,\n",
       "                           9.36493799e-02,  1.85379878e-01,  1.77607670e-01,\n",
       "                          -1.15194380e-01, -1.34112448e-01, -1.33980751e-01,\n",
       "                          -8.25649276e-02,  1.06301997e-02,  2.34024689e-01,\n",
       "                          -2.09575370e-01,  9.15939286e-02,  1.27128944e-01,\n",
       "                           1.61960840e-01, -6.21457361e-02, -1.03788255e-02,\n",
       "                           1.80615246e-01,  2.11603239e-01, -3.80942896e-02,\n",
       "                          -1.35713264e-01, -2.11235628e-01, -1.30339965e-01],\n",
       "                         [ 1.48824587e-01, -9.64998901e-02,  1.31857857e-01,\n",
       "                          -4.34890836e-02,  2.81706005e-02, -2.53003612e-02,\n",
       "                           2.17329830e-01,  2.55652755e-01,  1.33802161e-01,\n",
       "                          -1.53935403e-01, -3.20543319e-01,  1.34272695e-01,\n",
       "                           9.53012407e-02, -2.67629802e-01, -1.19078811e-02,\n",
       "                           1.25598013e-01,  1.43560514e-01, -3.03403020e-01,\n",
       "                          -1.88586980e-01,  5.26654767e-05,  4.38482016e-02,\n",
       "                          -1.18191458e-01, -1.66239545e-01, -1.96716979e-01,\n",
       "                           3.88605744e-01,  9.46325958e-02, -2.84669828e-02,\n",
       "                           1.05871849e-01,  5.09148799e-02,  1.64134830e-01],\n",
       "                         [-1.70983076e-01, -4.64218259e-02,  2.56032646e-01,\n",
       "                          -3.36754262e-01,  1.14413328e-01, -5.40561192e-02,\n",
       "                          -4.31713462e-02,  2.02621654e-01,  3.68611738e-02,\n",
       "                           9.03082937e-02, -1.55163243e-01, -5.28558753e-02,\n",
       "                           1.17577918e-01,  2.74976101e-02,  5.22431917e-02,\n",
       "                           9.45266709e-02, -6.40915707e-02, -3.41569513e-01,\n",
       "                          -1.24440417e-02, -3.78696680e-01, -9.68426242e-02,\n",
       "                           6.97233975e-02,  4.07467425e-01, -9.32717398e-02,\n",
       "                           4.06978458e-01,  1.34313911e-01, -1.15681343e-01,\n",
       "                          -1.41402736e-01,  2.93472260e-01,  1.35891214e-01],\n",
       "                         [ 4.92539182e-02, -3.84003878e-01, -1.38385417e-02,\n",
       "                          -2.17407897e-01, -2.27551058e-01,  1.22411788e-01,\n",
       "                           2.42480770e-01,  2.26476684e-01,  4.15094048e-01,\n",
       "                           2.70140469e-01,  3.66796345e-01, -2.09505707e-01,\n",
       "                           1.17731817e-01, -5.68392985e-02,  2.34463185e-01,\n",
       "                          -1.76236108e-01,  6.39564320e-02, -3.38484764e-01,\n",
       "                          -1.51277035e-01,  1.70556486e-01,  1.56147644e-01,\n",
       "                           8.72618556e-02, -1.29259840e-01,  4.42208238e-02,\n",
       "                           1.26151294e-01, -8.72233436e-02, -1.27821593e-02,\n",
       "                           3.46571594e-01, -5.72833233e-03, -3.52694802e-02],\n",
       "                         [-2.40802124e-01, -5.08709885e-02, -3.01369667e-01,\n",
       "                          -2.07815766e-01, -1.75061643e-01,  7.84979761e-03,\n",
       "                          -1.38375804e-01,  7.20941797e-02,  1.37719139e-01,\n",
       "                           2.73174345e-02, -1.28424659e-01,  2.61229336e-01,\n",
       "                          -3.02099705e-01,  2.63382137e-01, -2.82579124e-01,\n",
       "                           3.89510930e-01, -4.95093688e-02, -6.81620017e-02,\n",
       "                          -9.98189747e-02, -1.99897036e-01,  3.88988316e-01,\n",
       "                           7.38441851e-03, -3.70886810e-02,  3.90572041e-01,\n",
       "                           1.02245301e-01,  7.26366043e-02, -1.84608653e-01,\n",
       "                           1.54915810e-01, -2.54078686e-01,  1.00376770e-01],\n",
       "                         [ 2.45440111e-01, -8.09170380e-02, -3.17947537e-01,\n",
       "                           2.80143946e-01,  7.48801157e-02,  1.69096932e-01,\n",
       "                          -4.59705554e-02, -2.36122355e-01,  3.91472071e-01,\n",
       "                           1.21913850e-01,  4.56044916e-03,  1.42811716e-01,\n",
       "                           4.23130989e-02,  5.54179847e-02,  5.08152954e-02,\n",
       "                           4.11379844e-01,  1.76448226e-01, -1.71726555e-01,\n",
       "                          -1.51759103e-01,  7.44486824e-02,  2.06096873e-01,\n",
       "                          -6.89691603e-02,  1.86874092e-01, -1.18870862e-01,\n",
       "                           7.18445256e-02,  4.30048779e-02, -4.05949920e-01,\n",
       "                           1.60308164e-02,  6.66316152e-02, -2.98260957e-01],\n",
       "                         [-1.13599002e-01, -2.26134658e-01, -1.74289271e-02,\n",
       "                           1.85668454e-01,  1.21344045e-01, -1.54036373e-01,\n",
       "                          -6.77943602e-02,  8.92230645e-02,  3.54956120e-01,\n",
       "                           4.76500839e-02,  6.33863434e-02,  2.74720967e-01,\n",
       "                          -8.27807263e-02, -1.87392995e-01,  2.82183349e-01,\n",
       "                          -1.94487348e-01,  9.84097458e-03,  1.74456537e-01,\n",
       "                           7.86425248e-02, -1.69758931e-01, -2.44016305e-01,\n",
       "                           9.64466035e-02,  5.38683608e-02,  2.57032156e-01,\n",
       "                          -6.17531314e-02, -4.87292483e-02, -7.35116899e-02,\n",
       "                           1.99303299e-01,  1.43096462e-01, -2.55144566e-01],\n",
       "                         [ 2.04201922e-01, -1.68689311e-01, -1.76807985e-01,\n",
       "                          -3.59810703e-02,  7.26775527e-02, -2.24749908e-01,\n",
       "                           1.07728243e-01, -8.54379237e-02,  1.64868027e-01,\n",
       "                          -6.46524951e-02, -2.10760847e-01,  4.04934645e-01,\n",
       "                          -1.89617530e-01, -1.15303020e-03,  2.11937949e-02,\n",
       "                          -2.54929155e-01, -3.39635871e-02, -2.07134917e-01,\n",
       "                          -1.20904125e-01, -1.84011087e-01, -4.97124866e-02,\n",
       "                          -7.48790875e-02,  9.59564820e-02, -2.14972854e-01,\n",
       "                           1.14943966e-01, -6.11666292e-02, -2.02468596e-03,\n",
       "                          -1.13371555e-02, -8.87843966e-02, -1.98063210e-01],\n",
       "                         [ 6.39229789e-02, -1.17795929e-01,  1.16339952e-01,\n",
       "                           1.34535972e-02, -3.61602157e-01, -3.32956016e-01,\n",
       "                          -2.81323423e-03, -1.12767600e-01, -4.82292697e-02,\n",
       "                           1.15069054e-01,  1.22741960e-01,  4.21018433e-03,\n",
       "                          -2.78380290e-02, -2.85823762e-01, -4.00604874e-01,\n",
       "                           2.04923168e-01, -2.07229611e-02,  2.49248147e-01,\n",
       "                          -7.90681019e-02, -8.38340223e-02,  3.13251436e-01,\n",
       "                          -2.17727989e-01,  3.05948835e-02, -1.70558736e-01,\n",
       "                          -2.56496035e-02,  1.16825670e-01,  2.89657533e-01,\n",
       "                          -3.11481301e-02,  2.35535204e-01, -1.69424757e-01],\n",
       "                         [-6.79113641e-02, -9.58683807e-03, -1.44058973e-01,\n",
       "                           9.60037485e-02, -8.30143690e-03, -3.45723748e-01,\n",
       "                           2.92610582e-02, -2.70120025e-01,  1.99088901e-01,\n",
       "                           5.97111732e-02,  5.97866699e-02, -1.07888274e-01,\n",
       "                           8.52735192e-02,  1.68604136e-01,  3.50588448e-02,\n",
       "                           1.71493813e-01, -8.40620250e-02, -5.52773736e-02,\n",
       "                          -3.61098126e-02, -7.77427778e-02,  1.34391442e-01,\n",
       "                           3.17459762e-01, -1.14351250e-01, -2.90563136e-01,\n",
       "                          -1.12157851e-01,  3.05213720e-01,  1.04049586e-01,\n",
       "                          -1.18822772e-02, -3.46817583e-01,  5.87020367e-02],\n",
       "                         [ 7.43771270e-02,  3.35129976e-01,  3.35162580e-02,\n",
       "                          -2.88763121e-02,  2.90107494e-03,  2.31208086e-01,\n",
       "                          -7.53174946e-02, -8.77948105e-02, -1.09807476e-01,\n",
       "                           1.80521995e-01,  6.74440265e-02, -2.06381664e-01,\n",
       "                          -3.15866433e-02, -2.51394778e-01, -1.60177737e-01,\n",
       "                          -1.39085442e-01, -2.84216762e-01, -2.03627512e-01,\n",
       "                          -1.87774166e-01, -2.98956215e-01, -1.49581507e-01,\n",
       "                          -4.41441983e-02, -1.48315147e-01, -1.02303280e-02,\n",
       "                           8.18828344e-02, -1.49348915e-01,  6.22448176e-02,\n",
       "                           4.08892065e-01, -2.65488505e-01, -8.26377124e-02],\n",
       "                         [ 1.28697991e-01,  1.08617194e-01,  4.28332947e-02,\n",
       "                          -1.86727598e-01,  3.38571608e-01, -1.64610147e-01,\n",
       "                          -3.94570902e-02,  3.25731546e-01,  3.01892608e-01,\n",
       "                           3.22222412e-02,  3.77213508e-02, -1.16106160e-01,\n",
       "                           3.04233104e-01, -1.67429343e-01, -7.17147291e-02,\n",
       "                          -8.90081897e-02, -1.21832281e-01,  3.77812795e-02,\n",
       "                           1.37379602e-01, -2.50002444e-01,  1.73734248e-01,\n",
       "                          -3.70061188e-03,  9.46438015e-02,  6.70162514e-02,\n",
       "                           1.42684564e-01, -8.64605010e-02, -2.44062051e-01,\n",
       "                          -1.57830045e-01, -3.67670923e-01,  4.70908871e-03],\n",
       "                         [ 1.76416889e-01, -4.78465371e-02, -4.19100374e-03,\n",
       "                           1.73625231e-01, -3.19230199e-01, -1.44581228e-01,\n",
       "                          -2.83403844e-01, -1.18684001e-01, -2.90009439e-01,\n",
       "                           4.15356010e-02, -1.79658050e-03,  1.84094861e-01,\n",
       "                          -2.27703415e-02, -1.47593737e-01,  1.34534724e-02,\n",
       "                          -1.05151996e-01, -1.58725217e-01, -4.02658254e-01,\n",
       "                          -1.88007072e-01, -3.35072458e-01, -1.44480006e-03,\n",
       "                           1.44176647e-01,  2.49316767e-01,  1.79588627e-02,\n",
       "                          -1.08158186e-01, -8.88574645e-02, -5.18043488e-02,\n",
       "                          -1.00605570e-01, -2.75747329e-01, -1.05166063e-01],\n",
       "                         [-3.26597631e-01,  1.03580639e-01, -6.65417314e-02,\n",
       "                          -2.25245297e-01, -3.63899283e-02,  6.76431656e-02,\n",
       "                           4.77703661e-02, -7.04983696e-02, -2.31112048e-01,\n",
       "                           1.24105074e-01,  1.31970555e-01, -2.63993479e-02,\n",
       "                          -2.79776663e-01, -6.10797592e-02, -6.79632947e-02,\n",
       "                           9.32224169e-02, -1.65476769e-01,  2.17328176e-01,\n",
       "                           6.81025609e-02, -2.47325927e-01, -1.40225068e-01,\n",
       "                          -1.69541195e-01, -1.00971274e-01, -1.36950001e-01,\n",
       "                          -1.28906325e-01, -2.33845115e-01, -9.77497101e-02,\n",
       "                          -3.24530691e-01,  1.42309695e-01,  1.01904690e-01],\n",
       "                         [-2.10095584e-01, -1.09997749e-01,  2.15816677e-01,\n",
       "                           2.95522422e-01, -1.33606642e-01,  2.75906354e-01,\n",
       "                          -9.14905872e-03, -1.86179802e-01,  9.89440233e-02,\n",
       "                           2.74788421e-02, -1.57466844e-01,  9.87847745e-02,\n",
       "                          -2.51839012e-01, -3.73615444e-01, -1.62748799e-01,\n",
       "                           2.18635052e-02,  3.43179256e-01, -5.67252599e-02,\n",
       "                           2.13860333e-01, -1.66032519e-02,  8.70723277e-02,\n",
       "                           1.65098011e-02,  3.02392878e-02, -1.12582870e-01,\n",
       "                          -2.81783659e-02,  1.05504408e-01, -3.67783755e-01,\n",
       "                          -9.36653912e-02, -4.28658631e-03, -1.56222939e-01]],            dtype=float32),\n",
       "        },\n",
       "        Dense_3: {\n",
       "            bias: DeviceArray([0.00199999], dtype=float32),\n",
       "            kernel: DeviceArray([[-0.14747444],\n",
       "                         [-0.0282362 ],\n",
       "                         [-0.18356532],\n",
       "                         [ 0.23110959],\n",
       "                         [-0.00265711],\n",
       "                         [-0.11905131],\n",
       "                         [-0.06853607],\n",
       "                         [ 0.19406171],\n",
       "                         [ 0.17415945],\n",
       "                         [-0.3449678 ],\n",
       "                         [-0.26777974],\n",
       "                         [-0.2065838 ],\n",
       "                         [ 0.10096797],\n",
       "                         [-0.13729453],\n",
       "                         [-0.15278429],\n",
       "                         [-0.3194161 ],\n",
       "                         [ 0.11603839],\n",
       "                         [-0.17872651],\n",
       "                         [-0.23900385],\n",
       "                         [-0.24160792],\n",
       "                         [ 0.17081125],\n",
       "                         [-0.07639075],\n",
       "                         [-0.09580401],\n",
       "                         [ 0.02402686],\n",
       "                         [ 0.01364373],\n",
       "                         [-0.18682781],\n",
       "                         [-0.14168286],\n",
       "                         [-0.10414975],\n",
       "                         [-0.08067863],\n",
       "                         [-0.19157986]], dtype=float32),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_optimizer.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-phoenix",
   "metadata": {},
   "source": [
    "Yeah that's all fine, as expected. Let's see if we can recreate this; it seems the forward pass is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acknowledged-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "state, params = variables.pop(\"params\")\n",
    "state = (state, {\"prior_init\": None})  # adding prior to state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "offshore-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "model_state, loss_state = state\n",
    "variables = {\"params\": params, **model_state}\n",
    "(prediction, dt, theta, coeffs), updated_model_state = model.apply(\n",
    "    variables, X, mutable=list(model_state.keys())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "unauthorized-straight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nans: [DeviceArray(False, dtype=bool), DeviceArray(False, dtype=bool), DeviceArray(False, dtype=bool), DeviceArray(False, dtype=bool)]\n",
      "Infs: [DeviceArray(False, dtype=bool), DeviceArray(False, dtype=bool), DeviceArray(False, dtype=bool), DeviceArray(False, dtype=bool)]\n",
      "min: [DeviceArray(-0.08321948, dtype=float32), DeviceArray(-0.10772296, dtype=float32), DeviceArray(-0.8179425, dtype=float32), DeviceArray(-20.45381, dtype=float32)]\n",
      "max: [DeviceArray(0.1909909, dtype=float32), DeviceArray(0.16929603, dtype=float32), DeviceArray(2.387917, dtype=float32), DeviceArray(2.1001604, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "output = (prediction, dt, theta, coeffs)\n",
    "print(f\"Nans: {[jnp.any(jnp.isnan(i)) for i in output]}\")\n",
    "print(f\"Infs: {[jnp.any(jnp.isinf(i)) for i in output]}\")\n",
    "print(f\"min: {[jnp.min(i) for i in output]}\")\n",
    "print(f\"max: {[jnp.max(i) for i in output]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-embassy",
   "metadata": {},
   "source": [
    "as expected, thats fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "macro-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE stuff\n",
    "prior_params_mse = (0.0, 0.0)\n",
    "tau = precision(y, prediction, *prior_params_mse)\n",
    "p_mse, MSE = normal_LL(prediction, y, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "given-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression stuff\n",
    "# we dont want the gradient\n",
    "\n",
    "n_samples, n_features = theta.shape\n",
    "hyper_prior_params = (\n",
    "    n_samples / 2,\n",
    "    n_samples / (2 * jax.lax.stop_gradient(tau)),\n",
    ")\n",
    "\n",
    "theta_normed = theta / jnp.linalg.norm(theta, axis=0)\n",
    "\n",
    "prior_init = jnp.concatenate(\n",
    "            [jnp.ones((n_features,)), 1.0 / jnp.var(dt)[jnp.newaxis]]\n",
    "        )\n",
    "    \n",
    "p_reg, mn, prior, fwd_metric = SBL(\n",
    "    theta_normed,\n",
    "    dt,\n",
    "    prior_init=prior_init,\n",
    "    beta_prior=hyper_prior_params,\n",
    "    tol=1e-3,\n",
    "    max_iter=300,\n",
    "    non_diff=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-separate",
   "metadata": {},
   "source": [
    "Let's look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hindu-inclusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455.7085 [[ 4.8160817e-02]\n",
      " [-1.9316360e-05]\n",
      " [ 6.5489900e-01]\n",
      " [-8.6727006e-05]\n",
      " [ 1.6062734e+00]\n",
      " [ 5.8015270e-05]\n",
      " [-6.0136005e-04]\n",
      " [ 7.5137132e-04]\n",
      " [-3.0721983e-04]\n",
      " [ 4.9039809e-04]\n",
      " [ 6.4977107e-04]\n",
      " [-4.6146798e-04]] [3.8261505e+01 1.2122681e+03 1.5917799e+00 1.3987809e+03 3.5256892e-01\n",
      " 1.6174141e+03 1.0430642e+03 1.2461332e+03 8.3042767e+02 1.3485659e+03\n",
      " 8.2173694e+02 1.3355662e+03 4.2957764e+00]\n"
     ]
    }
   ],
   "source": [
    "print(p_reg, mn, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hearing-superintendent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DeviceArray(300, dtype=int32), DeviceArray(704.2981, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print(fwd_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-phoenix",
   "metadata": {},
   "source": [
    "That a fairly big gap, but that shouldn't be the issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "beautiful-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reg = jnp.mean((dt - theta_normed @ mn) ** 2)\n",
    "loss = -(p_mse + p_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "false-compiler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577.9458\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-timeline",
   "metadata": {},
   "source": [
    "Now as the code works when we use the non-diff version of alpha; the problem should manifest itself if we try to calculate the grad of the prior. Lets try and calculate the grad of the evidence first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "proud-header",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([3.8261505e+01, 1.2122681e+03, 1.5917799e+00, 1.3987809e+03,\n",
       "             3.5256892e-01, 1.6174141e+03, 1.0430642e+03, 1.2461332e+03,\n",
       "             8.3042767e+02, 1.3485659e+03, 8.2173694e+02, 1.3355662e+03,\n",
       "             4.2957764e+00], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "liked-moore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[nan, nan, nan, ..., nan, nan, nan],\n",
       "             [nan, nan, nan, ..., nan, nan, nan],\n",
       "             [nan, nan, nan, ..., nan, nan, nan],\n",
       "             ...,\n",
       "             [nan, nan, nan, ..., nan, nan, nan],\n",
       "             [nan, nan, nan, ..., nan, nan, nan],\n",
       "             [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_evidence = jax.grad(lambda X: SBL(\n",
    "    X,\n",
    "    dt,\n",
    "    prior_init=prior_init,\n",
    "    beta_prior=hyper_prior_params,\n",
    "    tol=1e-3,\n",
    "    max_iter=300,\n",
    "    non_diff=False\n",
    ")[0])\n",
    "grad_evidence(theta_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-seattle",
   "metadata": {},
   "source": [
    "Right. Now let's do the same without the alpha grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "touched-pricing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-1.2440996e-03,  4.1718004e-06,  2.5634980e-02, ...,\n",
       "               1.9654544e-05,  1.8911585e-05, -2.0199899e-05],\n",
       "             [-1.3899140e-03,  4.3380123e-06,  2.3853820e-02, ...,\n",
       "               1.8198010e-05,  1.7685046e-05, -1.8724470e-05],\n",
       "             [-1.5445322e-03,  4.5053612e-06,  2.1956662e-02, ...,\n",
       "               1.6653010e-05,  1.6358415e-05, -1.7162565e-05],\n",
       "             ...,\n",
       "             [ 5.0728256e-04,  4.9672240e-06,  3.1806353e-02, ...,\n",
       "               3.5711520e-05, -2.1013140e-05, -4.3645468e-05],\n",
       "             [ 4.1561108e-04,  4.9216474e-06,  3.0796958e-02, ...,\n",
       "               3.4790366e-05, -2.1361724e-05, -4.2672349e-05],\n",
       "             [ 3.2548304e-04,  4.8696011e-06,  2.9805612e-02, ...,\n",
       "               3.3884531e-05, -2.1701526e-05, -4.1715357e-05]],            dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_evidence = jax.grad(lambda X: SBL(\n",
    "    X,\n",
    "    dt,\n",
    "    prior_init=prior_init,\n",
    "    beta_prior=hyper_prior_params,\n",
    "    tol=1e-3,\n",
    "    max_iter=300,\n",
    "    non_diff=True\n",
    ")[0])\n",
    "grad_evidence(theta_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-graphic",
   "metadata": {},
   "source": [
    "As expected, that works. Now let's check the alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "alpine-ethernet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 1.48259335e+01,  1.13631815e-01,  4.15236244e+01, ...,\n",
       "               2.46541947e-02,  3.99421901e-02, -2.55021565e-02],\n",
       "             [ 1.62474537e+01,  1.18221581e-01,  4.12807884e+01, ...,\n",
       "               2.53305510e-02,  4.05450463e-02, -2.55433004e-02],\n",
       "             [ 1.77543659e+01,  1.22841269e-01,  4.10161934e+01, ...,\n",
       "               2.60445736e-02,  4.11799699e-02, -2.55829357e-02],\n",
       "             ...,\n",
       "             [-3.03727722e+00,  1.36570469e-01,  3.10729713e+01, ...,\n",
       "               9.37876478e-03,  2.39835773e-02, -1.68121438e-02],\n",
       "             [-2.13769913e+00,  1.35279939e-01,  3.10193806e+01, ...,\n",
       "               9.86818224e-03,  2.44306866e-02, -1.69030670e-02],\n",
       "             [-1.25323868e+00,  1.33809865e-01,  3.09675369e+01, ...,\n",
       "               1.03502646e-02,  2.48709843e-02, -1.69931948e-02]],            dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_alpha = jax.grad(lambda X: SBL(\n",
    "    X,\n",
    "    dt,\n",
    "    prior_init=prior_init,\n",
    "    beta_prior=hyper_prior_params,\n",
    "    tol=1e-3,\n",
    "    max_iter=5000,\n",
    "    non_diff=False\n",
    ")[2][0])\n",
    "grad_alpha(theta_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "palestinian-spirit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[nan, nan, nan, ..., nan, nan, nan],\n",
       "             [nan, nan, nan, ..., nan, nan, nan],\n",
       "             [nan, nan, nan, ..., nan, nan, nan],\n",
       "             ...,\n",
       "             [nan, nan, nan, ..., nan, nan, nan],\n",
       "             [nan, nan, nan, ..., nan, nan, nan],\n",
       "             [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_alpha = jax.grad(lambda X: SBL(\n",
    "    X,\n",
    "    dt,\n",
    "    prior_init=prior_init,\n",
    "    beta_prior=hyper_prior_params,\n",
    "    tol=1e-3,\n",
    "    max_iter=300,\n",
    "    non_diff=False\n",
    ")[2][0])\n",
    "grad_alpha(theta_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-latter",
   "metadata": {},
   "source": [
    "Aha so I get Nan's when i have a to low number of iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "golden-programmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_alpha = jax.grad(lambda X: SBL(\n",
    "    X,\n",
    "    dt,\n",
    "    prior_init=prior_init,\n",
    "    beta_prior=hyper_prior_params,\n",
    "    tol=1e-3,\n",
    "    max_iter=300,\n",
    "    non_diff=True\n",
    ")[2][0])\n",
    "grad_alpha(theta_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-bridges",
   "metadata": {},
   "source": [
    "So we surely found the cause; is the backprop in the alpha. Let's do that in a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-chart",
   "metadata": {},
   "source": [
    "So if we use much more iterations for the SBL we dont get nans? let's check by running it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "preceding-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_SBL(params, state, model, X, y, warm_restart=True, non_diff=True):\n",
    "    model_state, loss_state = state\n",
    "    variables = {\"params\": params, **model_state}\n",
    "    (prediction, dt, theta, coeffs), updated_model_state = model.apply(\n",
    "        variables, X, mutable=list(model_state.keys())\n",
    "    )\n",
    "\n",
    "    n_samples, n_features = theta.shape\n",
    "    prior_params_mse = (0.0, 0.0)\n",
    "\n",
    "    # MSE stuff\n",
    "    tau = precision(y, prediction, *prior_params_mse)\n",
    "    p_mse, MSE = normal_LL(prediction, y, tau)\n",
    "\n",
    "    # Regression stuff\n",
    "    # we dont want the gradient\n",
    "    hyper_prior_params = (\n",
    "        n_samples / 2,\n",
    "        n_samples / (2 * jax.lax.stop_gradient(tau)),\n",
    "    )\n",
    "    theta_normed = theta / jnp.linalg.norm(theta, axis=0)\n",
    "\n",
    "    if (loss_state[\"prior_init\"] is None) or (warm_restart is False):\n",
    "        prior_init = jnp.concatenate(\n",
    "            [jnp.ones((n_features,)), 1.0 / jnp.var(dt)[jnp.newaxis]]\n",
    "        )\n",
    "    else:\n",
    "        prior_init = loss_state[\"prior_init\"]\n",
    "\n",
    "    p_reg, mn, prior, fwd_metric = SBL(\n",
    "        theta_normed,\n",
    "        dt,\n",
    "        prior_init=prior_init,\n",
    "        beta_prior=hyper_prior_params,\n",
    "        tol=1e-3,\n",
    "        max_iter=5000,\n",
    "        non_diff=non_diff\n",
    "    )\n",
    "\n",
    "    Reg = jnp.mean((dt - theta_normed @ mn) ** 2)\n",
    "\n",
    "    updated_loss_state = {\"prior_init\": prior}\n",
    "    loss = -(p_mse + p_reg)\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"p_mse\": p_mse,\n",
    "        \"mse\": MSE,\n",
    "        \"p_reg\": p_reg,\n",
    "        \"reg\": Reg,\n",
    "        \"bayes_coeffs\": mn,\n",
    "        \"coeffs\": coeffs,\n",
    "        \"alpha\": prior[:-1],\n",
    "        \"beta\": prior[-1],\n",
    "        \"tau\": tau,\n",
    "        \"its\": fwd_metric[0],\n",
    "        \"gap\": fwd_metric[1],\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        loss,\n",
    "        ((updated_model_state, updated_loss_state), metrics, (prediction, dt, theta, mn)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "referenced-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "\n",
    "state = (state, {\"prior_init\": None})  # adding prior to state\n",
    "update_fn = create_update(loss_fn_SBL, (model, X, y, True, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-screening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0, Nans: False, MSE: 0.4627499580383301\n",
      "Done with 100, Nans: False, MSE: 1.0874793529510498\n",
      "Done with 200, Nans: False, MSE: 3.0515739917755127\n"
     ]
    }
   ],
   "source": [
    "for i in jnp.arange(5000):\n",
    "    (optimizer, state), metrics, output = update_fn(optimizer, state)\n",
    "    if i % 100 == 0:\n",
    "        has_nan = jnp.any(jnp.array([jnp.any(jnp.isnan(item)) for _, item in flatten_dict(unfreeze(optimizer.target)).items()]))\n",
    "        mse = metrics[\"mse\"]\n",
    "        print(f\"Done with {i}, Nans: {has_nan}, MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "governmental-class",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Buffer([[-7.0177660e+00],\n",
       "        [ 3.8864780e+01],\n",
       "        [-1.8419664e+01],\n",
       "        [ 1.1504240e-03],\n",
       "        [-2.4564800e+00],\n",
       "        [ 4.5422382e+01],\n",
       "        [ 8.7160920e-04],\n",
       "        [-2.5117257e+00],\n",
       "        [-8.7517255e-04],\n",
       "        [ 4.3179214e-01],\n",
       "        [-1.4310470e-03],\n",
       "        [ 3.1985741e-03]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "serious-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "\n",
    "state = (state, {\"prior_init\": None})  # adding prior to state\n",
    "update_fn = create_update(loss_fn_SBL, (model, X, y, True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-design",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0, Nans: False, MSE: 0.4627499580383301\n",
      "Done with 100, Nans: False, MSE: 0.22842560708522797\n",
      "Done with 200, Nans: False, MSE: 0.14117945730686188\n",
      "Done with 300, Nans: False, MSE: 0.06718077510595322\n",
      "Done with 400, Nans: False, MSE: 0.048002939671278\n",
      "Done with 500, Nans: False, MSE: 0.031150447204709053\n",
      "Done with 600, Nans: False, MSE: 0.024588650092482567\n",
      "Done with 700, Nans: False, MSE: 0.011292420327663422\n",
      "Done with 800, Nans: False, MSE: 0.003878676798194647\n",
      "Done with 900, Nans: False, MSE: 0.0035111813340336084\n",
      "Done with 1000, Nans: False, MSE: 0.0034217783249914646\n",
      "Done with 1100, Nans: False, MSE: 0.003423253307119012\n",
      "Done with 1200, Nans: False, MSE: 0.0034096508752554655\n",
      "Done with 1300, Nans: False, MSE: 0.003411792917177081\n",
      "Done with 1400, Nans: False, MSE: 0.003428456373512745\n",
      "Done with 1500, Nans: False, MSE: 0.0034271518234163523\n",
      "Done with 1600, Nans: False, MSE: 0.0033987085334956646\n",
      "Done with 1700, Nans: False, MSE: 0.0034088718239217997\n",
      "Done with 1800, Nans: False, MSE: 0.0033934875391423702\n",
      "Done with 1900, Nans: False, MSE: 0.0034101016353815794\n",
      "Done with 2000, Nans: False, MSE: 0.0034024352207779884\n",
      "Done with 2100, Nans: False, MSE: 0.0034357754047960043\n",
      "Done with 2200, Nans: False, MSE: 0.003400336019694805\n",
      "Done with 2300, Nans: False, MSE: 0.0034360834397375584\n",
      "Done with 2400, Nans: False, MSE: 0.003409318160265684\n",
      "Done with 2500, Nans: False, MSE: 0.0034062324557453394\n",
      "Done with 2600, Nans: False, MSE: 0.0034053383860737085\n",
      "Done with 2700, Nans: False, MSE: 0.003412109101191163\n",
      "Done with 2800, Nans: False, MSE: 0.0034203773830085993\n",
      "Done with 2900, Nans: False, MSE: 0.0034380974248051643\n",
      "Done with 3000, Nans: False, MSE: 0.003405346069484949\n",
      "Done with 3100, Nans: False, MSE: 0.003422805340960622\n",
      "Done with 3200, Nans: False, MSE: 0.003404372138902545\n",
      "Done with 3300, Nans: False, MSE: 0.003435825929045677\n",
      "Done with 3400, Nans: False, MSE: 0.0034063009079545736\n",
      "Done with 3500, Nans: False, MSE: 0.0033997197169810534\n",
      "Done with 3600, Nans: False, MSE: 0.0034039467573165894\n",
      "Done with 3700, Nans: False, MSE: 0.0034127794206142426\n",
      "Done with 3800, Nans: False, MSE: 0.003396452171728015\n",
      "Done with 3900, Nans: False, MSE: 0.003412411781027913\n",
      "Done with 4000, Nans: False, MSE: 0.0034043327905237675\n"
     ]
    }
   ],
   "source": [
    "for i in jnp.arange(5000):\n",
    "    (optimizer, state), metrics, output = update_fn(optimizer, state)\n",
    "    if i % 100 == 0:\n",
    "        has_nan = jnp.any(jnp.array([jnp.any(jnp.isnan(item)) for _, item in flatten_dict(unfreeze(optimizer.target)).items()]))\n",
    "        mse = metrics[\"mse\"]\n",
    "        print(f\"Done with {i}, Nans: {has_nan}, MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-trial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-purple",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-medication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-intervention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-server",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-charm",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-graduate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-falls",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-empty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-bibliography",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-clark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-fleet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-length",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
