{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian regression works with the old code. Now let's make a nice new function a la sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import jax\n",
    "from jax import jit, numpy as jnp, lax, random\n",
    "from functools import partial\n",
    "from modax.utils.forward_solver import fixed_point_solver\n",
    "from modax.linear_model.bayesian_regression import bayesianregression, evidence\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jnp.load('test_data.npy', allow_pickle=True).item()\n",
    "y, X = data['y'], data['X']\n",
    "\n",
    "X_normed = X / jnp.linalg.norm(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.5665381e-01 2.7395762e+05]\n",
      "(DeviceArray(4, dtype=int32), DeviceArray(6.0796738e-06, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "prior, metric = bayesianregression(X_normed, y)\n",
    "print(prior)\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([1.8114342e+01, 2.7395122e+05], dtype=float32),\n",
       " (DeviceArray(300, dtype=int32), DeviceArray(0.00158882, dtype=float32)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesianregression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge(compute_score=True, fit_intercept=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = BayesianRidge(fit_intercept=False, compute_score=True)\n",
    "reg.fit(X_normed, y.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273806.14266750304"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3566588567031124"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02766591, -0.42259253,  3.77226037,  0.43094458, -0.1104252 ,\n",
       "       -4.15133131,  0.2873178 , -0.61194118,  0.10678613, -1.11750869,\n",
       "       -0.28417433, -0.04745628])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 761.94079555, 3578.52057138, 4772.05472899, 4772.05489891,\n",
       "       4772.05489891])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([3.566560e-01, 2.739576e+05], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(4772.328, dtype=float32),\n",
       " DeviceArray([[ 0.02767511],\n",
       "              [-0.4225819 ],\n",
       "              [ 3.7722263 ],\n",
       "              [ 0.43099666],\n",
       "              [-0.11044621],\n",
       "              [-4.151463  ],\n",
       "              [ 0.28747463],\n",
       "              [-0.6120944 ],\n",
       "              [ 0.10681057],\n",
       "              [-1.117393  ],\n",
       "              [-0.28420353],\n",
       "              [-0.04737961]], dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence(X_normed, y, prior, hyper_prior_params=(0.0, 0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_bayesian_ridge(params, state, model, X, y):\n",
    "    variables = {\"params\": params, **state}\n",
    "    (prediction, dt, theta, coeffs), updated_state = model.apply(\n",
    "        variables, X, mutable=list(state.keys())\n",
    "    )\n",
    "    \n",
    "    n_samples = theta.shape[0]\n",
    "    prior_params_mse = (0.0, 0.0)\n",
    "    \n",
    "    # MSE stuff\n",
    "    tau = precision(y, prediction, *prior_params_mse)\n",
    "    p_mse, MSE = normal_LL(prediction, y, tau)\n",
    "    \n",
    "    # Regression stuff\n",
    "    hyper_prior_params = (n_samples/2, n_samples / (2 * jax.lax.stop_gradient(tau))) # we dont want the gradient\n",
    "    theta_normed = theta / jnp.linalg.norm(theta, axis=0)\n",
    "    prior, fwd_metric = bayesianregression(theta_normed, dt, hyper_prior_params=hyper_prior_params)\n",
    "    p_reg, mn = evidence(theta_normed, dt, prior, hyper_prior_params=hyper_prior_params)\n",
    "    Reg = jnp.mean((dt - theta_normed @ mn)**2)\n",
    "    \n",
    "    \n",
    "    loss = -(p_mse + p_reg)\n",
    "    metrics = {\"loss\": loss, \n",
    "               \"p_mse\": p_mse,\n",
    "               \"mse\": MSE, \n",
    "               \"p_reg\": p_reg,\n",
    "               \"reg\": Reg, \n",
    "               \"bayes_coeffs\": mn, \n",
    "               \"coeffs\": coeffs, \n",
    "               \"alpha\": prior[:-1], \n",
    "               \"beta\": prior[-1],\n",
    "               \"tau\": tau,\n",
    "              \"its\": fwd_metric[0],\n",
    "              \"gap\": fwd_metric[1]}\n",
    "\n",
    "    return loss, (updated_state, metrics, (prediction, dt, theta, mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modax.data.burgers import burgers\n",
    "from modax.data.kdv import doublesoliton\n",
    "from modax.training import train_max_iter\n",
    "from modax.models import Deepmod\n",
    "from modax.training.utils import create_update\n",
    "from modax.training.losses.utils import precision, normal_LL\n",
    "from modax.training.losses import loss_fn_pinn\n",
    "\n",
    "from flax import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Making data\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "x = jnp.linspace(-3, 4, 50)\n",
    "t = jnp.linspace(0.5, 5.0, 20)\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "u = burgers(x_grid, t_grid, 0.1, 1.0)\n",
    "\n",
    "X = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y = u.reshape(-1, 1)\n",
    "y += 0.10 * jnp.std(y) * random.normal(key, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "update_fn = create_update(loss_fn_bayesian_ridge, (model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: -1985.470947265625\n",
      "Loss step 1000: -9142.236328125\n",
      "Loss step 2000: -9149.2734375\n",
      "Loss step 3000: -9151.642578125\n",
      "Loss step 4000: -9152.2529296875\n",
      "Loss step 5000: -9151.6845703125\n",
      "Loss step 6000: -9152.771484375\n",
      "Loss step 7000: -9152.865234375\n",
      "Loss step 8000: -9153.7734375\n",
      "Loss step 9000: -9154.736328125\n"
     ]
    }
   ],
   "source": [
    "optimizer, state = train_max_iter(update_fn, optimizer, state, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "update_fn = create_update(loss_fn_pinn, (model, X, y, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: 0.04526979848742485\n",
      "Loss step 1000: 0.00039324039244093\n",
      "Loss step 2000: 0.0003718193038366735\n",
      "Loss step 3000: 0.0003707118739839643\n",
      "Loss step 4000: 0.0003705784911289811\n",
      "Loss step 5000: 0.00037028806400485337\n",
      "Loss step 6000: 0.00037023628829047084\n",
      "Loss step 7000: 0.0003697912907227874\n",
      "Loss step 8000: 0.00036968590575270355\n",
      "Loss step 9000: 0.00036963008460588753\n"
     ]
    }
   ],
   "source": [
    "optimizer, state = train_max_iter(update_fn, optimizer, state, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Making data\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "x = jnp.linspace(-10, 10, 100)\n",
    "t = jnp.linspace(0.1, 1.0, 10)\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "\n",
    "u = doublesoliton(x_grid, t_grid, c=[5.0, 2.0], x0=[0.0, -5.0])\n",
    "\n",
    "X = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y = u.reshape(-1, 1)\n",
    "y += 0.1 * jnp.std(y) * random.normal(key, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "update_fn = create_update(loss_fn_pinn, (model, X, y, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: 0.4641355574131012\n",
      "Loss step 1000: 0.007112619932740927\n",
      "Loss step 2000: 0.003383260453119874\n",
      "Loss step 3000: 0.003378728637471795\n",
      "Loss step 4000: 0.0033790224697440863\n",
      "Loss step 5000: 0.0033775491174310446\n",
      "Loss step 6000: 0.0033782690297812223\n",
      "Loss step 7000: 0.0033780799712985754\n",
      "Loss step 8000: 0.0033773169852793217\n",
      "Loss step 9000: 0.0033768159337341785\n"
     ]
    }
   ],
   "source": [
    "optimizer, state = train_max_iter(update_fn, optimizer, state, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "update_fn = create_update(loss_fn_bayesian_ridge, (model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: 1499.5491943359375\n",
      "Loss step 1000: -5780.04150390625\n",
      "Loss step 2000: -5813.61083984375\n",
      "Loss step 3000: -5814.09814453125\n",
      "Loss step 4000: -5814.857421875\n",
      "Loss step 5000: -5815.029296875\n",
      "Loss step 6000: -5814.96337890625\n",
      "Loss step 7000: -5814.95068359375\n",
      "Loss step 8000: -5814.1875\n",
      "Loss step 9000: -5814.4736328125\n"
     ]
    }
   ],
   "source": [
    "optimizer, state = train_max_iter(update_fn, optimizer, state, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-33-d5d56a8db427>, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-d5d56a8db427>\"\u001b[0;36m, line \u001b[0;32m43\u001b[0m\n\u001b[0;31m    return loss, ((updated_state, network_state), metrics, (prediction, dt, theta, mn))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "class loss_fn_bayesian_ridge(params, state, model, X, y, warm_restart=True):\n",
    "    network_state, loss_state = state\n",
    "    variables = {\"params\": params, **network_state}\n",
    "    (prediction, dt, theta, coeffs), updated_state = model.apply(\n",
    "        variables, X, mutable=list(state.keys())\n",
    "    )\n",
    "\n",
    "    n_samples = theta.shape[0]\n",
    "    prior_params_mse = (0.0, 0.0)\n",
    "\n",
    "    # MSE stuff\n",
    "    tau = precision(y, prediction, *prior_params_mse)\n",
    "    p_mse, MSE = normal_LL(prediction, y, tau)\n",
    "\n",
    "    # Regression stuff\n",
    "    hyper_prior_params = (n_samples/2, n_samples / (2 * jax.lax.stop_gradient(tau))) # we dont want the gradient\n",
    "    theta_normed = theta / jnp.linalg.norm(theta, axis=0)\n",
    "    \n",
    "    if (network_state['prior_init'] is None) or (warm_restart is False):\n",
    "        prior_init = jnp.stack([1.0, 1.0 / jnp.var(dt)])\n",
    "    else:\n",
    "        prior_init = network_state['prior']\n",
    "\n",
    "    prior, fwd_metric = bayesianregression(theta_normed, dt, prior_params_init=prior_init, hyper_prior_params=hyper_prior_params)\n",
    "    p_reg, mn = evidence(theta_normed, dt, prior, hyper_prior_params=hyper_prior_params)\n",
    "    Reg = jnp.mean((dt - theta_normed @ mn)**2)\n",
    "    \n",
    "    network_state['prior_init'] = prior\n",
    "    loss = -(p_mse + p_reg)\n",
    "    metrics = {\"loss\": loss, \n",
    "               \"p_mse\": p_mse,\n",
    "               \"mse\": MSE, \n",
    "               \"p_reg\": p_reg,\n",
    "               \"reg\": Reg, \n",
    "               \"bayes_coeffs\": mn, \n",
    "               \"coeffs\": coeffs, \n",
    "               \"alpha\": prior[:-1], \n",
    "               \"beta\": prior[-1],\n",
    "               \"tau\": tau,\n",
    "              \"its\": fwd_metric[0],\n",
    "              \"gap\": fwd_metric[1]}\n",
    "\n",
    "    return loss, ((updated_state, network_state), metrics, (prediction, dt, theta, mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "# with warm restart\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "update_fn = create_update(loss_fn_bayesian_ridge(warm_restart=False), (model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: -1985.470947265625\n",
      "Loss step 1000: -9141.5234375\n",
      "Loss step 2000: -9149.9833984375\n",
      "Loss step 3000: -9150.4365234375\n",
      "Loss step 4000: -9151.61328125\n",
      "Loss step 5000: -9152.45703125\n",
      "Loss step 6000: -9151.58984375\n",
      "Loss step 7000: -9153.0888671875\n",
      "Loss step 8000: -9153.326171875\n",
      "Loss step 9000: -9154.28515625\n",
      "CPU times: user 59.9 s, sys: 970 ms, total: 1min\n",
      "Wall time: 59.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer, state = train_max_iter(update_fn, optimizer, state, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Building model and params\n",
    "# without warm restart\n",
    "model = Deepmod([30, 30, 30, 1])\n",
    "variables = model.init(key, X)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop(\"params\")\n",
    "optimizer = optimizer.create(params)\n",
    "update_fn = create_update(loss_fn_bayesian_ridge(warm_restart=True), (model, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: -1985.470947265625\n",
      "Loss step 1000: -9141.5400390625\n",
      "Loss step 2000: -9149.931640625\n",
      "Loss step 3000: -9150.6376953125\n",
      "Loss step 4000: -9151.5419921875\n",
      "Loss step 5000: -9152.056640625\n",
      "Loss step 6000: -9151.7451171875\n",
      "Loss step 7000: -9152.3037109375\n",
      "Loss step 8000: -9152.853515625\n",
      "Loss step 9000: -9154.81640625\n",
      "CPU times: user 1min, sys: 907 ms, total: 1min 1s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer, state = train_max_iter(update_fn, optimizer, state, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    vars: {\n",
       "        LeastSquares_0: {\n",
       "            mask: DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                          True,  True,  True,  True], dtype=bool),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.core import freeze, unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = freeze({**unfreeze(state), \"diff vars\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\"params\": params, **state}\n",
    "(prediction, dt, theta, coeffs), updated_state = model.apply(\n",
    "    variables, X, mutable=list(state.keys())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    vars: {\n",
       "        LeastSquares_0: {\n",
       "            mask: DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "                          True,  True,  True,  True], dtype=bool),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
