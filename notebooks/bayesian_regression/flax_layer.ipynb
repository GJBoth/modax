{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "from functools import partial\n",
    "from jax import lax\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax.core import freeze\n",
    "from typing import Tuple, Sequence\n",
    "\n",
    "from modax.feature_generators import library_backward\n",
    "from modax.networks import MLP, MultiTaskMLP\n",
    "from modax.data.burgers import burgers\n",
    "from modax.logging import Logger\n",
    "from flax import optim\n",
    "\n",
    "from code import fwd_solver, bayes_ridge_update, fixed_point_solver, evidence\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from jax.test_util import check_grads\n",
    "\n",
    "\n",
    "from modax.losses import neg_LL\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = jnp.load('test_data.npy', allow_pickle=True).item()\n",
    "y, X = data['y'], data['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_FLAGS=\"--xla_force_host_platform_device_count=4\"\n"
     ]
    }
   ],
   "source": [
    "%env XLA_FLAGS=\"--xla_force_host_platform_device_count=4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianRegression(nn.Module):\n",
    "    hyper_prior: Tuple\n",
    "    tol: float = 1e-3\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        z_init=jnp.stack([1, 1 / jnp.var(inputs[0])], axis=0)\n",
    "        f = lambda prior, y, X: bayes_ridge_update(prior, y, X, self.hyper_prior)\n",
    "        \n",
    "        z_star = fixed_point_solver(f, inputs, z_init, tol=self.tol)\n",
    "        return z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_terms = X.shape\n",
    "hyper_prior =  jnp.stack([n_samples / 2, 1 / (n_samples / 2 * 1e-4)],  axis=0)\n",
    "#inputs = (y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianRegression(hyper_prior)\n",
    "params = model.init(key, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 0.36742398, 49.686714  ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it works! Now let's move the construction of the update function to a separate initi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianRegression(nn.Module):\n",
    "    hyper_prior: Tuple\n",
    "    tol: float = 1e-3\n",
    "        \n",
    "    def setup(self):\n",
    "        self.update = lambda prior, y, X: bayes_ridge_update(prior, y, X, self.hyper_prior)\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        z_init=jnp.stack([1, 1 / jnp.var(inputs[0])], axis=0)\n",
    "        \n",
    "        z_star = fixed_point_solver(self.update, inputs, z_init, tol=self.tol)\n",
    "        return z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianRegression(hyper_prior)\n",
    "params = model.init(key, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 354 ms, sys: 6.35 ms, total: 360 ms\n",
      "Wall time: 357 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 0.36742398, 49.686714  ], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.apply(params, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 370 ms, sys: 7.26 ms, total: 377 ms\n",
      "Wall time: 375 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 0.36742398, 49.686714  ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.apply(params, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's try to work with flax variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianRegression(nn.Module):\n",
    "    hyper_prior: Tuple\n",
    "    tol: float = 1e-3\n",
    "        \n",
    "    def setup(self):\n",
    "        self.update = lambda prior, y, X: bayes_ridge_update(prior, y, X, self.hyper_prior)\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        is_initialized = self.has_variable('bayes', 'z')\n",
    "        z_init = self.variable('bayes', 'z', \n",
    "                               lambda y: jnp.stack([1, 1 / jnp.var(y)], axis=0), \n",
    "                               inputs[0])\n",
    "      \n",
    "        z_star = fixed_point_solver(self.update, inputs, z_init.value, tol=self.tol)\n",
    "        if is_initialized:\n",
    "            z_init.value = z_star\n",
    "        return z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianRegression(hyper_prior, tol=1e-5)\n",
    "variables = model.init(key, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    bayes: {\n",
      "        z: DeviceArray([ 1.    , 30.3171], dtype=float32),\n",
      "    },\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 373 ms, sys: 6.68 ms, total: 379 ms\n",
      "Wall time: 388 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y, updated_state = model.apply(variables, inputs, mutable=['bayes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    bayes: {\n",
       "        z: DeviceArray([ 0.3674249, 49.686718 ], dtype=float32),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    old_state, params = variables.pop('params') # if we don't have params\n",
    "    variables = freeze({'params': params, **updated_state})\n",
    "except:\n",
    "    variables = freeze(updated_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    bayes: {\n",
      "        z: DeviceArray([ 0.3674249, 49.686718 ], dtype=float32),\n",
      "    },\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 343 ms, sys: 5.91 ms, total: 348 ms\n",
      "Wall time: 345 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y, updated_state = model.apply(variables, inputs, mutable=['bayes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it works, let's put it in a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deepmod(nn.Module):\n",
    "    features: Sequence[int]  \n",
    "    hyper_prior: Tuple\n",
    "    tol: float = 1e-3\n",
    "        \n",
    "    @nn.compact \n",
    "    def __call__(self, inputs):\n",
    "        prediction, dt, theta = library_backward(MLP(self.features), inputs)\n",
    "        z = BayesianRegression(self.hyper_prior, self.tol)((dt, theta))\n",
    "        return prediction, dt, theta, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataset\n",
    "x = jnp.linspace(-3, 4, 50)\n",
    "t = jnp.linspace(0.5, 5.0, 20)\n",
    "\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "u = burgers(x_grid, t_grid, 0.1, 1.0)\n",
    "\n",
    "X_train = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y_train = u.reshape(-1, 1)\n",
    "y_train += 0.1 * jnp.std(y_train) * jax.random.normal(key, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deepmod([50, 50, 1], hyper_prior)\n",
    "variables = model.init(key, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ae3b3f0b5bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bayes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'variables' is not defined"
     ]
    }
   ],
   "source": [
    "model.apply(variables, X_train, mutable='bayes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "optimizer = optimizer.create(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    BayesianRegression_0: {\n",
       "        z: DeviceArray([ 1.    , 44.1788], dtype=float32),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.target['bayes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_pinn_bayes_regression(params, state, model, x, y):\n",
    "    \"\"\" first argument should always be params!\n",
    "    \"\"\"\n",
    "    variables = {'params': params, **state}\n",
    "    (prediction, dt, theta, z), updated_state = model.apply(variables, x, mutable=list(state.keys()))\n",
    "    \n",
    "    \n",
    "    # MSE\n",
    "    sigma_ml = jnp.mean((prediction - y) ** 2)\n",
    "    tau = 1 / sigma_ml\n",
    "    MSE = neg_LL(prediction, y, tau)\n",
    "    \n",
    "    # Reg\n",
    "    theta_normed = theta / jnp.linalg.norm(theta, axis=0, keepdims=True)\n",
    "    Reg, mn = evidence(z, dt, theta_normed, model.hyper_prior)\n",
    "    loss = MSE - Reg\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"mse\": MSE,\n",
    "        \"reg\": Reg,\n",
    "        \"coeff\": mn,\n",
    "        \"tau\": tau, \n",
    "        \"beta\": z[1],\n",
    "        \"alpha\": z[0]\n",
    "    }\n",
    "    return loss, (updated_state, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad, jit\n",
    "\n",
    "\n",
    "def create_update(loss_fn, *args, **kwargs):\n",
    "    def step(opt, state, loss_fn, *args, **kwargs):\n",
    "        grad_fn = value_and_grad(loss_fn, argnums=0, has_aux=True)\n",
    "        (loss, (updated_state, metrics)), grad = grad_fn(opt.target, state, *args, **kwargs)\n",
    "        opt = opt.apply_gradient(grad)  # Return the updated optimizer with parameters.\n",
    "        \n",
    "        return (opt, updated_state), metrics\n",
    "\n",
    "    return jit(lambda opt, state: step(opt, state, loss_fn, *args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataset\n",
    "x = jnp.linspace(-3, 4, 50)\n",
    "t = jnp.linspace(0.5, 5.0, 20)\n",
    "\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "u = burgers(x_grid, t_grid, 0.1, 1.0)\n",
    "\n",
    "X_train = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y_train = u.reshape(-1, 1)\n",
    "y_train += 0.01 * jnp.std(y_train) * jax.random.normal(key, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deepmod([50, 50, 1], hyper_prior, tol=1e-4)\n",
    "variables = model.init(key, X_train)\n",
    "\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "state, params = variables.pop('params')\n",
    "optimizer = optimizer.create(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling train step\n",
    "update = create_update(loss_fn_pinn_bayes_regression, model=model, x=X_train, y=y_train)\n",
    "_ = update(optimizer, state)  # triggering compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: -1360.2958984375\n",
      "Loss step 100: -3297.87841796875\n",
      "Loss step 200: -3842.685791015625\n",
      "Loss step 300: -4392.36376953125\n",
      "Loss step 400: -5141.0751953125\n",
      "Loss step 500: -5726.458984375\n",
      "Loss step 600: -5991.244140625\n",
      "Loss step 700: -6167.302734375\n",
      "Loss step 800: -6316.0205078125\n",
      "Loss step 900: -6411.0087890625\n",
      "Loss step 1000: -6495.7685546875\n",
      "Loss step 1100: -6547.9140625\n",
      "Loss step 1200: -6583.92626953125\n",
      "Loss step 1300: -6613.85498046875\n",
      "Loss step 1400: -6658.244140625\n",
      "Loss step 1500: -6674.17431640625\n",
      "Loss step 1600: -6695.69921875\n",
      "Loss step 1700: -6671.9560546875\n",
      "Loss step 1800: -6719.74609375\n",
      "Loss step 1900: -6761.48388671875\n",
      "Loss step 2000: -6744.74560546875\n",
      "Loss step 2100: -6731.6728515625\n",
      "Loss step 2200: -6751.22119140625\n",
      "Loss step 2300: -6743.8203125\n",
      "Loss step 2400: -6761.58203125\n",
      "Loss step 2500: -6753.8935546875\n",
      "Loss step 2600: -6755.419921875\n",
      "Loss step 2700: -6765.4755859375\n",
      "Loss step 2800: -6759.00048828125\n",
      "Loss step 2900: -6764.4111328125\n",
      "Loss step 3000: -6771.03125\n",
      "Loss step 3100: -6759.958984375\n",
      "Loss step 3200: -6782.22265625\n",
      "Loss step 3300: -6762.69775390625\n",
      "Loss step 3400: -6785.69140625\n",
      "Loss step 3500: -6772.2802734375\n",
      "Loss step 3600: -6782.298828125\n",
      "Loss step 3700: -6779.43359375\n",
      "Loss step 3800: -6774.52001953125\n",
      "Loss step 3900: -6784.69384765625\n",
      "Loss step 4000: -6766.4326171875\n",
      "Loss step 4100: -6785.7451171875\n",
      "Loss step 4200: -6774.68359375\n",
      "Loss step 4300: -6774.15234375\n",
      "Loss step 4400: -6792.9189453125\n",
      "Loss step 4500: -6765.400390625\n",
      "Loss step 4600: -6795.7119140625\n",
      "Loss step 4700: -6777.140625\n",
      "Loss step 4800: -6782.564453125\n",
      "Loss step 4900: -6804.775390625\n",
      "Loss step 5000: -6750.01513671875\n",
      "Loss step 5100: -6819.83154296875\n",
      "Loss step 5200: -6743.818359375\n",
      "Loss step 5300: -6809.2705078125\n",
      "Loss step 5400: -6775.41796875\n",
      "Loss step 5500: -6789.3017578125\n",
      "Loss step 5600: -6788.55078125\n",
      "Loss step 5700: -6787.091796875\n",
      "Loss step 5800: -6781.6064453125\n",
      "Loss step 5900: -6793.89013671875\n",
      "Loss step 6000: -6786.01171875\n",
      "Loss step 6100: -6784.84375\n",
      "Loss step 6200: -6810.1064453125\n",
      "Loss step 6300: -6752.3486328125\n",
      "Loss step 6400: -6826.7138671875\n",
      "Loss step 6500: -6758.24267578125\n",
      "Loss step 6600: -6807.431640625\n",
      "Loss step 6700: -6803.775390625\n",
      "Loss step 6800: -6767.3212890625\n",
      "Loss step 6900: -6820.2744140625\n",
      "Loss step 7000: -6766.6123046875\n",
      "Loss step 7100: -6802.4345703125\n",
      "Loss step 7200: -6788.9111328125\n",
      "Loss step 7300: -6795.2998046875\n",
      "Loss step 7400: -6783.7431640625\n",
      "Loss step 7500: -6809.01611328125\n",
      "Loss step 7600: -6780.71484375\n",
      "Loss step 7700: -6801.7412109375\n",
      "Loss step 7800: -6814.8173828125\n",
      "Loss step 7900: -6757.2861328125\n",
      "Loss step 8000: -6832.16259765625\n",
      "Loss step 8100: -6764.96142578125\n",
      "Loss step 8200: -6809.3291015625\n",
      "Loss step 8300: -6798.7333984375\n",
      "Loss step 8400: -6787.625\n",
      "Loss step 8500: -6804.72802734375\n",
      "Loss step 8600: -6803.7158203125\n",
      "Loss step 8700: -6789.8876953125\n",
      "Loss step 8800: -6803.71240234375\n",
      "Loss step 8900: -6808.462890625\n",
      "Loss step 9000: -6771.26806640625\n",
      "Loss step 9100: -6834.013671875\n",
      "Loss step 9200: -6762.87451171875\n",
      "Loss step 9300: -6815.6494140625\n",
      "Loss step 9400: -6811.76806640625\n",
      "Loss step 9500: -6777.263671875\n",
      "Loss step 9600: -6820.3115234375\n",
      "Loss step 9700: -6790.0009765625\n",
      "Loss step 9800: -6803.5185546875\n",
      "Loss step 9900: -6785.1708984375\n"
     ]
    }
   ],
   "source": [
    "# Running to convergence\n",
    "max_epochs = 10000\n",
    "logger = Logger()\n",
    "for epoch in jnp.arange(max_epochs):\n",
    "    (optimizer, state), metrics = update(optimizer, state)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Loss step {epoch}: {metrics['loss']}\")\n",
    "    if epoch % 100 == 0:\n",
    "        logger.write(metrics, epoch)\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': DeviceArray(0.5818718, dtype=float32),\n",
       " 'beta': DeviceArray(48.99622, dtype=float32),\n",
       " 'coeff': DeviceArray([[ 0.15310402],\n",
       "              [-1.4754432 ],\n",
       "              [ 2.3529584 ],\n",
       "              [ 0.71037406],\n",
       "              [ 0.4666279 ],\n",
       "              [-2.0328088 ],\n",
       "              [ 0.28171843],\n",
       "              [ 0.03225338],\n",
       "              [-1.1648126 ],\n",
       "              [-1.2099943 ],\n",
       "              [-0.49702704],\n",
       "              [-0.5595163 ]], dtype=float32),\n",
       " 'loss': DeviceArray(-4549.8765, dtype=float32),\n",
       " 'mse': DeviceArray(-2594.754, dtype=float32),\n",
       " 'reg': DeviceArray(1955.1224, dtype=float32),\n",
       " 'tau': DeviceArray(3063.719, dtype=float32)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: -4480.9990234375\n",
      "Loss step 100: -4482.5615234375\n",
      "Loss step 200: -4484.4423828125\n",
      "Loss step 300: -4485.9072265625\n",
      "Loss step 400: -4486.9345703125\n",
      "Loss step 500: -4488.0380859375\n",
      "Loss step 600: -4488.5107421875\n",
      "Loss step 700: -4489.59375\n",
      "Loss step 800: -4489.91943359375\n",
      "Loss step 900: -4490.802734375\n"
     ]
    }
   ],
   "source": [
    "# Running to convergence\n",
    "max_epochs = 1000\n",
    "logger = Logger()\n",
    "for epoch in jnp.arange(max_epochs):\n",
    "    (optimizer, state), metrics = update(optimizer, state)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Loss step {epoch}: {metrics['loss']}\")\n",
    "    if epoch % 100 == 0:\n",
    "        logger.write(metrics, epoch)\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': DeviceArray(0.4882597, dtype=float32),\n",
       " 'beta': DeviceArray(49.442085, dtype=float32),\n",
       " 'coeff': DeviceArray([[ 1.4626276e-03],\n",
       "              [-4.7906667e-02],\n",
       "              [ 9.0431087e-02],\n",
       "              [ 7.0760306e-04],\n",
       "              [ 5.4294147e-02],\n",
       "              [-8.2236814e-01],\n",
       "              [-2.4918601e-02],\n",
       "              [ 1.8872246e-03],\n",
       "              [-1.9642621e-01],\n",
       "              [-2.4825841e-02],\n",
       "              [ 8.3470047e-03],\n",
       "              [-3.2375995e-03]], dtype=float32),\n",
       " 'loss': DeviceArray(-4491.4624, dtype=float32),\n",
       " 'mse': DeviceArray(-2556.8352, dtype=float32),\n",
       " 'reg': DeviceArray(1934.6272, dtype=float32),\n",
       " 'tau': DeviceArray(2839.967, dtype=float32)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: -4491.01123046875\n",
      "Loss step 100: -4491.70458984375\n",
      "Loss step 200: -4492.50927734375\n",
      "Loss step 300: -4492.556640625\n",
      "Loss step 400: -4492.7998046875\n",
      "Loss step 500: -4493.5419921875\n",
      "Loss step 600: -4493.87353515625\n",
      "Loss step 700: -4494.416015625\n",
      "Loss step 800: -4495.08251953125\n",
      "Loss step 900: -4495.1376953125\n",
      "Loss step 1000: -4495.68017578125\n",
      "Loss step 1100: -4495.96630859375\n",
      "Loss step 1200: -4496.14990234375\n",
      "Loss step 1300: -4496.8369140625\n",
      "Loss step 1400: -4496.9521484375\n",
      "Loss step 1500: -4497.6611328125\n",
      "Loss step 1600: -4498.01611328125\n",
      "Loss step 1700: -4498.4619140625\n",
      "Loss step 1800: -4499.03564453125\n",
      "Loss step 1900: -4499.2626953125\n"
     ]
    }
   ],
   "source": [
    "# Running to convergence\n",
    "max_epochs = 2000\n",
    "logger = Logger()\n",
    "for epoch in jnp.arange(max_epochs):\n",
    "    (optimizer, state), metrics = update(optimizer, state)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Loss step {epoch}: {metrics['loss']}\")\n",
    "    if epoch % 100 == 0:\n",
    "        logger.write(metrics, epoch)\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': DeviceArray(0.5149161, dtype=float32),\n",
       " 'beta': DeviceArray(49.31387, dtype=float32),\n",
       " 'coeff': DeviceArray([[ 0.00158105],\n",
       "              [-0.05517259],\n",
       "              [ 0.09083098],\n",
       "              [ 0.00277134],\n",
       "              [ 0.04931932],\n",
       "              [-0.7894591 ],\n",
       "              [-0.03046691],\n",
       "              [-0.00440558],\n",
       "              [-0.18170032],\n",
       "              [-0.06253958],\n",
       "              [ 0.01805699],\n",
       "              [ 0.00101865]], dtype=float32),\n",
       " 'loss': DeviceArray(-4499.5713, dtype=float32),\n",
       " 'mse': DeviceArray(-2567.4487, dtype=float32),\n",
       " 'reg': DeviceArray(1932.1224, dtype=float32),\n",
       " 'tau': DeviceArray(2900.8936, dtype=float32)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SBL_update(prior_params, y, X, hyper_prior_params):\n",
    "    # Unpacking parameters\n",
    "    alpha_prev, beta_prev = prior_params[:-1], prior_params[-1]\n",
    "    a, b = hyper_prior_params\n",
    "\n",
    "    # Calculating intermediate matrices\n",
    "    n_samples, n_terms = X.shape\n",
    "    Sigma = jnp.linalg.inv(beta_prev * X.T @ X + jnp.diag(alpha_prev))\n",
    "    mu = beta_prev * Sigma @ X.T @ y\n",
    "    gamma = 1 - alpha_prev * jnp.diag(Sigma)\n",
    "\n",
    "    # Updating\n",
    "    cap = 1e6\n",
    "    alpha = jnp.minimum(gamma / (mu**2).squeeze(), cap)\n",
    "    beta = (n_samples - jnp.sum(gamma) + 2 * a) / (jnp.sum((y - X @ mu) ** 2) + 2 * b)\n",
    "   \n",
    "    return jnp.concatenate([alpha, beta[None]], axis=0), mu, gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
