{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import jax\n",
    "from jax import random, numpy as jnp\n",
    "from flax import optim\n",
    "from modax.models import Deepmod\n",
    "from modax.training import create_update\n",
    "from modax.losses import loss_fn_pinn\n",
    "from modax.logging import Logger\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from jax.scipy.stats import gamma\n",
    "from modax.data.burgers import burgers\n",
    "from time import time\n",
    "\n",
    "from functools import partial\n",
    "from jax import lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataset\n",
    "x = jnp.linspace(-3, 4, 5|0)\n",
    "t = jnp.linspace(0.5, 5.0, 20)\n",
    "\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "u = burgers(x_grid, t_grid, 0.1, 1.0)\n",
    "\n",
    "X_train = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y_train = u.reshape(-1, 1)\n",
    "y_train += 0.1 * jnp.std(y_train) * jax.random.normal(key, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating model and optimizers\n",
    "model = Deepmod(features=[50, 50, 1])\n",
    "key = random.PRNGKey(42)\n",
    "params = model.init(key, X_train)\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "optimizer = optimizer.create(params)\n",
    "\n",
    "# Compiling train step\n",
    "update = create_update(loss_fn_pinn, model=model, x=X_train, y=y_train)\n",
    "_ = update(optimizer)  # triggering compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: 0.07853322476148605\n",
      "Loss step 1000: 0.00017794837185647339\n",
      "Loss step 2000: 0.0001510621514171362\n",
      "Loss step 3000: 0.00012943477486260235\n",
      "Loss step 4000: 0.00012168406101409346\n",
      "Loss step 5000: 0.00011918714153580368\n",
      "Loss step 6000: 0.00011606322368606925\n",
      "Loss step 7000: 0.00011324064689688385\n",
      "Loss step 8000: 0.00011104914301540703\n",
      "Loss step 9000: 0.00010823373304447159\n",
      "Loss step 10000: 0.0001053838204825297\n",
      "Loss step 11000: 0.00010257066605845466\n",
      "Loss step 12000: 9.865898755379021e-05\n",
      "Loss step 13000: 9.564652282278985e-05\n",
      "Loss step 14000: 9.192790457746014e-05\n",
      "Loss step 15000: 8.85783665580675e-05\n",
      "Loss step 16000: 8.627793431514874e-05\n",
      "Loss step 17000: 8.401690138271078e-05\n",
      "Loss step 18000: 7.963005919009447e-05\n",
      "Loss step 19000: 7.57902380428277e-05\n",
      "Loss step 20000: 7.397824811050668e-05\n"
     ]
    }
   ],
   "source": [
    "# Running to convergence\n",
    "max_epochs = 20001\n",
    "logger = Logger()\n",
    "for epoch in jnp.arange(max_epochs):\n",
    "    optimizer, metrics = update(optimizer)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Loss step {epoch}: {metrics['loss']}\")\n",
    "    if epoch % 100 == 0:\n",
    "        logger.write(metrics, epoch)\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, dt, theta, coeffs = model.apply(optimizer.target, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_solver(f, z_init):\n",
    "    def cond_fun(carry):\n",
    "        z_prev, z = carry\n",
    "        return jnp.linalg.norm(z_prev - z) > 1e-4\n",
    "\n",
    "    def body_fun(carry):\n",
    "        _, z = carry\n",
    "        return z, f(z)\n",
    "\n",
    "    init_carry = (z_init, f(z_init))\n",
    "    _, z_star = lax.while_loop(cond_fun, body_fun, init_carry)\n",
    "    return z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.custom_vjp, nondiff_argnums=(0, ))\n",
    "def fixed_point_layer(f, params, x):\n",
    "    z_star = fwd_solver(lambda z: f(params, x, z), z_init=jnp.zeros_like(x))\n",
    "    return z_star\n",
    "\n",
    "def fixed_point_layer_fwd(f, params, x):\n",
    "    z_star = fixed_point_layer(f, params, x)\n",
    "    return z_star, (params, x, z_star)\n",
    "\n",
    "def fixed_point_layer_bwd(f, res, z_star_bar):\n",
    "    params, x, z_star = res\n",
    "    _, vjp_a = jax.vjp(lambda params, x: f(params, x, z_star), params, x)\n",
    "    _, vjp_z = jax.vjp(lambda z: f(params, x, z), z_star)\n",
    "    return vjp_a(fwd_solver(lambda u: vjp_z(u)[0] + z_star_bar,\n",
    "                      z_init=jnp.zeros_like(z_star)))\n",
    "\n",
    "fixed_point_layer.defvjp(fixed_point_layer_fwd, fixed_point_layer_bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda W, x, z: jnp.tanh(jnp.dot(W, z) + x)\n",
    "\n",
    "ndim = 10\n",
    "W = random.normal(random.PRNGKey(0), (ndim, ndim)) / jnp.sqrt(ndim)\n",
    "x = random.normal(random.PRNGKey(1), (ndim,))\n",
    "\n",
    "layer = jax.jit(fixed_point_layer, static_argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00632886 -0.70152855 -0.9847213  -0.0419194  -0.6151645  -0.48185453\n",
      "  0.5783277   0.9556748  -0.08354193  0.8447265 ]\n"
     ]
    }
   ],
   "source": [
    "z_star = layer(f, W, x)\n",
    "print(z_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00733157 -0.81267565 -1.1407362  -0.04856092 -0.7126285  -0.55819744\n",
      "  0.66995543  1.1070877  -0.09677795  0.9785612 ]\n"
     ]
    }
   ],
   "source": [
    "g = jax.grad(lambda W: layer(f, W, x).sum())(W)\n",
    "print(g[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1584356  0.38825384 0.06453395 1.792349   0.23890097 1.4024842\n",
      " 0.7339767  0.09179301 0.9173474  0.18508697]\n"
     ]
    }
   ],
   "source": [
    "g = jax.grad(lambda x: layer(f, W, x).sum())(x)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try this with bayesian ridge. To do so, we need to \n",
    "1. params -> dt\n",
    "2. x -> theta\n",
    "3. write a function taking in those and giving back alpha and beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_ridge_update(y, X, z):\n",
    "    # Unpacking parameters\n",
    "    alpha_prev, beta_prev = z\n",
    "    n_samples, n_terms  = X.shape\n",
    "    a, b = n_samples/2, 1 /(n_samples/2 * 1e-4) # prior params\n",
    "    \n",
    "    # Preparing some matrices\n",
    "    X_normed = X / jnp.linalg.norm(X, axis=0)\n",
    "    gram = X_normed.T @ X_normed\n",
    "    eigvals = jnp.linalg.eigvalsh(gram) \n",
    "    \n",
    "    # Calculating intermediat matrices\n",
    "    gamma_ = jnp.sum((beta_prev * eigvals) / (alpha_prev + beta_prev * eigvals))\n",
    "    S = jnp.linalg.inv(beta_prev * gram + alpha_prev * jnp.eye(n_terms))\n",
    "    mn = beta_prev * S @ X_normed.T @ y\n",
    "    \n",
    "    # Update estimate\n",
    "    alpha = gamma_ / jnp.sum(mn**2)\n",
    "    beta = (n_samples - gamma_ + 2 * (a - 1)) / (jnp.sum((y - X_normed @ mn)**2) + 2 * b)\n",
    "    \n",
    "    return jnp.stack([alpha, beta], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([  0.32389224, 144.18567   ], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_ridge_update(dt, theta, jnp.ones((2, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a single update works. Now to try with the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_init = jnp.stack([1, 1 / jnp.var(dt)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 ms ± 1.41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "fwd_solver(lambda z: bayes_ridge_update(dt, theta, z), z_init=z_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works as well... Now to try with the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_ridge_update(y, X, z):\n",
    "    # Unpacking parameters\n",
    "    alpha_prev, beta_prev = z\n",
    "    n_samples, n_terms  = X.shape\n",
    "    a, b = n_samples/2, 1 /(n_samples/2 * 1e-4) # prior params\n",
    "    \n",
    "    # Preparing some matrices\n",
    "    X_normed = X / jnp.linalg.norm(X, axis=0)\n",
    "    gram = X_normed.T @ X_normed\n",
    "    eigvals = jnp.linalg.eigvalsh(gram) \n",
    "    \n",
    "    # Calculating intermediate matrices\n",
    "    gamma_ = jnp.sum((beta_prev * eigvals) / (alpha_prev + beta_prev * eigvals))\n",
    "    S = jnp.linalg.inv(beta_prev * gram + alpha_prev * jnp.eye(n_terms))\n",
    "    mn = beta_prev * S @ X_normed.T @ y\n",
    "    \n",
    "    # Update estimate\n",
    "    alpha = gamma_ / jnp.sum(mn**2)\n",
    "    beta = (n_samples - gamma_ + 2 * (a - 1)) / (jnp.sum((y - X_normed @ mn)**2) + 2 * b)\n",
    "    \n",
    "    return jnp.stack([alpha, beta], axis=0)\n",
    "\n",
    "@partial(jax.custom_vjp, nondiff_argnums=(0, ))\n",
    "def fixed_point_layer(f, params, x):\n",
    "    z_star = fwd_solver(lambda z: f(params, x, z), z_init=jnp.stack([1, 1 / jnp.var(params)], axis=0))\n",
    "    return z_star\n",
    "\n",
    "def fixed_point_layer_fwd(f, params, x):\n",
    "    z_star = fixed_point_layer(f, params, x)\n",
    "    return z_star, (params, x, z_star)\n",
    "\n",
    "def fixed_point_layer_bwd(f, res, z_star_bar):\n",
    "    params, x, z_star = res\n",
    "    _, vjp_a = jax.vjp(lambda params, x: f(params, x, z_star), params, x)\n",
    "    _, vjp_z = jax.vjp(lambda z: f(params, x, z), z_star)\n",
    "    return vjp_a(fwd_solver(lambda u: vjp_z(u)[0] + z_star_bar,\n",
    "                      z_init=jnp.zeros_like(z_star)))\n",
    "\n",
    "fixed_point_layer.defvjp(fixed_point_layer_fwd, fixed_point_layer_bwd)\n",
    "layer = jax.jit(fixed_point_layer, static_argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_star = layer(bayes_ridge_update, dt, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1.6304933e-01, 1.9929942e+02], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if we can calculate the derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[ 0.00169743],\n",
       "              [ 0.00176666],\n",
       "              [ 0.00163769],\n",
       "              ...,\n",
       "              [-0.00159182],\n",
       "              [-0.00220567],\n",
       "              [-0.00181732]], dtype=float32),\n",
       " DeviceArray([[-1.7207964e-06, -9.6736410e-05, -1.5373771e-04, ...,\n",
       "               -4.2493947e-04,  8.8823639e-05, -6.7857636e-06],\n",
       "              [-1.7349913e-06, -9.6280361e-05, -1.6067649e-04, ...,\n",
       "               -4.2243715e-04,  8.8712732e-05, -6.7922888e-06],\n",
       "              [-1.7202834e-06, -9.6421179e-05, -1.4779282e-04, ...,\n",
       "               -4.2329155e-04,  8.8510293e-05, -6.7520523e-06],\n",
       "              ...,\n",
       "              [ 2.0118662e-06, -1.4835734e-04,  1.5040957e-04, ...,\n",
       "               -6.2490714e-04,  1.7513241e-05, -5.6543859e-06],\n",
       "              [ 1.3946650e-06, -1.4123453e-04,  2.1717543e-04, ...,\n",
       "               -6.0367957e-04,  3.2208281e-05, -5.5440623e-06],\n",
       "              [ 7.6851694e-07, -1.3259590e-04,  1.8263116e-04, ...,\n",
       "               -5.7059061e-04,  4.4317359e-05, -5.6692083e-06]],            dtype=float32))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vjp(lambda y, X: layer(bayes_ridge_update, y, X), dt, theta)[1](jnp.array([0., 1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([[0.00191845],\n",
       "              [0.00191423],\n",
       "              [0.00190841],\n",
       "              ...,\n",
       "              [0.00326243],\n",
       "              [0.00296539],\n",
       "              [0.00272752]], dtype=float32),\n",
       " DeviceArray([[-6.3239258e-07,  7.4459640e-05, -1.9322045e-04, ...,\n",
       "                3.4397098e-04, -2.8167053e-05,  3.3908968e-06],\n",
       "              [-6.3568882e-07,  7.4632335e-05, -1.9278172e-04, ...,\n",
       "                3.4469625e-04, -2.8151417e-05,  3.4021436e-06],\n",
       "              [-6.1973941e-07,  7.3841387e-05, -1.9226574e-04, ...,\n",
       "                3.4098790e-04, -2.8048093e-05,  3.3649455e-06],\n",
       "              ...,\n",
       "              [-1.8955561e-06,  9.7045580e-05, -3.2402133e-04, ...,\n",
       "                4.3808299e-04, -7.9861502e-06,  3.2982689e-06],\n",
       "              [-1.4947502e-06,  8.5814012e-05, -2.9566450e-04, ...,\n",
       "                3.8835127e-04, -1.1735467e-05,  2.9846397e-06],\n",
       "              [-1.2548866e-06,  8.0879137e-05, -2.7259887e-04, ...,\n",
       "                3.6721979e-04, -1.5101070e-05,  2.9460978e-06]],            dtype=float32))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vjp(lambda y, X: layer(bayes_ridge_update, y, X), dt, theta)[1](jnp.array([1., 0.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to put it in a nice flax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianRidge(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        dt, theta = inputs\n",
    "        z_star = layer(bayes_ridge_update, dt, theta)\n",
    "        return z_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianRidge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.init(key, (dt, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1.6304933e-01, 1.9929942e+02], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, (dt, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Callable\n",
    "from modax.feature_generators import library_backward, library_forward\n",
    "from modax.layers import LeastSquares, LeastSquaresMT\n",
    "from modax.networks import MLP, MultiTaskMLP\n",
    "from flax import linen as nn\n",
    "from modax.losses import neg_LL\n",
    "\n",
    "\n",
    "class Deepmod(nn.Module):\n",
    "    \"\"\"Simple feed-forward NN.\n",
    "    \"\"\"\n",
    "\n",
    "    features: Sequence[int]  # this is dataclass, so we dont use __init__\n",
    "\n",
    "    @nn.compact  # this function decorator lazily intializes the model, so it makes the layers the first time we call it\n",
    "    def __call__(self, inputs):\n",
    "        prediction, dt, theta = library_backward(MLP(self.features), inputs)\n",
    "        z = BayesianRidge()((dt, theta))\n",
    "        return prediction, dt, theta, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deepmod([30, 30, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.init(key, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([1.2429158e-02, 8.7499802e+01], dtype=float32)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, X_train)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evidence(y, X, z):\n",
    "    n_samples, n_terms  = X.shape\n",
    "    alpha, beta = z\n",
    "    a, b = n_samples/2, 1 /(n_samples/2 * 1e-4) # prior params\n",
    "   \n",
    "    A = alpha * jnp.eye(n_terms) + beta * X.T @ X \n",
    "    mn = beta * jnp.linalg.inv(A) @ X.T @ y\n",
    "    \n",
    "    E = beta / 2 * jnp.sum((y - X @ mn)**2) + alpha / 2 * jnp.sum(mn**2)\n",
    "    loss = n_terms / 2 * jnp.log(alpha) + n_samples / 2 * jnp.log(beta) - E - 1/2 * jnp.linalg.slogdet(A)[1] - n_samples / 2 * jnp.log(2 * jnp.pi)\n",
    "    loss += jnp.sum(gamma.logpdf(beta, a=a, scale=b))\n",
    "    return -loss, mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(1198.8901, dtype=float32),\n",
       " DeviceArray([[ 1.4184997e-04],\n",
       "              [-1.5999153e-03],\n",
       "              [ 9.9819690e-02],\n",
       "              [ 1.7427839e-05],\n",
       "              [ 7.0923567e-04],\n",
       "              [-9.8933864e-01],\n",
       "              [ 1.9797683e-04],\n",
       "              [-1.6318355e-04],\n",
       "              [-3.6606789e-03],\n",
       "              [-7.3438287e-03],\n",
       "              [-1.8105358e-03],\n",
       "              [ 4.2999390e-04]], dtype=float32))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = model.apply(params, X_train)[-1]\n",
    "evidence(dt, theta, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_pinn_bayes_regression(params, model, x, y):\n",
    "    \"\"\" first argument should always be params!\n",
    "    \"\"\"\n",
    "    prediction, dt, theta, z = model.apply(params, x)\n",
    "\n",
    "    # MSE\n",
    "    sigma_ml = jnp.mean((prediction - y) ** 2)\n",
    "    tau = 1 / sigma_ml\n",
    "    MSE = neg_LL(prediction, y, tau)\n",
    "    \n",
    "    # Reg\n",
    "    Reg, mn = evidence(dt, theta, z)\n",
    "    loss = MSE + Reg\n",
    "    metrics = {\n",
    "        \"loss\": loss,\n",
    "        \"mse\": MSE,\n",
    "        \"reg\": Reg,\n",
    "        \"coeff\": mn,\n",
    "        \"tau\": tau, \n",
    "        \"beta\": z[1],\n",
    "        \"alpha\": z[0]\n",
    "    }\n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(3920.4146, dtype=float32),\n",
       " {'loss': DeviceArray(3920.4146, dtype=float32),\n",
       "  'mse': DeviceArray(1625.2769, dtype=float32),\n",
       "  'reg': DeviceArray(2295.1377, dtype=float32),\n",
       "  'coeff': DeviceArray([[-0.08180069],\n",
       "               [-2.2042918 ],\n",
       "               [-0.39007962],\n",
       "               [-0.19036376],\n",
       "               [-0.4221705 ],\n",
       "               [-6.298418  ],\n",
       "               [ 2.7376833 ],\n",
       "               [-0.07010293],\n",
       "               [-0.8428339 ],\n",
       "               [-0.74586403],\n",
       "               [ 3.294509  ],\n",
       "               [ 1.1794865 ]], dtype=float32),\n",
       "  'tau': DeviceArray(3.362216, dtype=float32),\n",
       "  'beta': DeviceArray(87.4998, dtype=float32),\n",
       "  'alpha': DeviceArray(0.01242916, dtype=float32)})"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn_pinn_bayes_regression(params, model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so that works... maybe now let's try to do a full run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataset\n",
    "x = jnp.linspace(-3, 4, 50)\n",
    "t = jnp.linspace(0.5, 5.0, 20)\n",
    "\n",
    "t_grid, x_grid = jnp.meshgrid(t, x, indexing=\"ij\")\n",
    "u = burgers(x_grid, t_grid, 0.1, 1.0)\n",
    "\n",
    "X_train = jnp.concatenate([t_grid.reshape(-1, 1), x_grid.reshape(-1, 1)], axis=1)\n",
    "y_train = u.reshape(-1, 1)\n",
    "y_train += 0.1 * jnp.std(y_train) * jax.random.normal(key, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating model and optimizers\n",
    "model = Deepmod(features=[50, 50, 1])\n",
    "key = random.PRNGKey(42)\n",
    "params = model.init(key, X_train)\n",
    "optimizer = optim.Adam(learning_rate=2e-3, beta1=0.99, beta2=0.99)\n",
    "optimizer = optimizer.create(params)\n",
    "\n",
    "# Compiling train step\n",
    "update = create_update(loss_fn_pinn_bayes_regression, model=model, x=X_train, y=y_train)\n",
    "_ = update(optimizer)  # triggering compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss step 0: 2028.1748046875\n",
      "Loss step 1000: -1361.991943359375\n",
      "Loss step 2000: -1371.446533203125\n",
      "Loss step 3000: -1373.916748046875\n",
      "Loss step 4000: -1377.374755859375\n",
      "Loss step 5000: -1380.127685546875\n",
      "Loss step 6000: -1383.46728515625\n",
      "Loss step 7000: -1385.64208984375\n",
      "Loss step 8000: -1388.329833984375\n",
      "Loss step 9000: -1391.118896484375\n",
      "Loss step 10000: -1393.1318359375\n",
      "Loss step 11000: -1395.61376953125\n",
      "Loss step 12000: -1397.76318359375\n",
      "Loss step 13000: -1400.376220703125\n",
      "Loss step 14000: -1403.441162109375\n",
      "Loss step 15000: -1405.583984375\n",
      "Loss step 16000: -1407.463134765625\n",
      "Loss step 17000: -1409.633544921875\n",
      "Loss step 18000: -1411.95361328125\n",
      "Loss step 19000: -1413.703857421875\n",
      "Loss step 20000: -1414.93994140625\n"
     ]
    }
   ],
   "source": [
    "# Running to convergence\n",
    "max_epochs = 20001\n",
    "logger = Logger()\n",
    "for epoch in jnp.arange(max_epochs):\n",
    "    optimizer, metrics = update(optimizer)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Loss step {epoch}: {metrics['loss']}\")\n",
    "    if epoch % 100 == 0:\n",
    "        logger.write(metrics, epoch)\n",
    "logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
